# Comparing `tmp/brainpy-2.4.2-py3-none-any.whl.zip` & `tmp/brainpy-2.4.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,310 +1,335 @@
-Zip file size: 630975 bytes, number of entries: 308
--rw-r--r--  2.0 unx    10155 b- defN 23-Jun-27 09:01 brainpy/__init__.py
--rw-r--r--  2.0 unx      661 b- defN 23-Jun-27 09:01 brainpy/analysis.py
--rw-r--r--  2.0 unx     1308 b- defN 23-Jun-27 09:01 brainpy/channels.py
--rw-r--r--  2.0 unx    19357 b- defN 23-Jun-27 09:01 brainpy/check.py
--rw-r--r--  2.0 unx      407 b- defN 23-Jun-27 09:01 brainpy/checkpoints.py
--rw-r--r--  2.0 unx     1275 b- defN 23-Jun-27 09:01 brainpy/connect.py
--rw-r--r--  2.0 unx     2690 b- defN 23-Jun-27 09:01 brainpy/dnn.py
--rw-r--r--  2.0 unx      324 b- defN 23-Jun-27 09:01 brainpy/encoding.py
--rw-r--r--  2.0 unx     7520 b- defN 23-Jun-27 09:01 brainpy/errors.py
--rw-r--r--  2.0 unx      334 b- defN 23-Jun-27 09:01 brainpy/experimental.py
--rw-r--r--  2.0 unx     1123 b- defN 23-Jun-27 09:01 brainpy/initialize.py
--rw-r--r--  2.0 unx      335 b- defN 23-Jun-27 09:01 brainpy/inputs.py
--rw-r--r--  2.0 unx       20 b- defN 23-Jun-27 09:01 brainpy/layers.py
--rw-r--r--  2.0 unx     1086 b- defN 23-Jun-27 09:01 brainpy/losses.py
--rw-r--r--  2.0 unx      487 b- defN 23-Jun-27 09:01 brainpy/measure.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-27 09:01 brainpy/mixin.py
--rw-r--r--  2.0 unx     1013 b- defN 23-Jun-27 09:01 brainpy/neurons.py
--rw-r--r--  2.0 unx     1013 b- defN 23-Jun-27 09:01 brainpy/optim.py
--rw-r--r--  2.0 unx      299 b- defN 23-Jun-27 09:01 brainpy/rates.py
--rw-r--r--  2.0 unx      466 b- defN 23-Jun-27 09:01 brainpy/running.py
--rw-r--r--  2.0 unx       51 b- defN 23-Jun-27 09:01 brainpy/testing.py
--rw-r--r--  2.0 unx     1046 b- defN 23-Jun-27 09:01 brainpy/tools.py
--rw-r--r--  2.0 unx      265 b- defN 23-Jun-27 09:01 brainpy/types.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-27 09:01 brainpy/_src/__init__.py
--rw-r--r--  2.0 unx     9945 b- defN 23-Jun-27 09:01 brainpy/_src/_delay.py
--rw-r--r--  2.0 unx     2457 b- defN 23-Jun-27 09:01 brainpy/_src/checking.py
--rw-r--r--  2.0 unx     2631 b- defN 23-Jun-27 09:01 brainpy/_src/context.py
--rw-r--r--  2.0 unx    22860 b- defN 23-Jun-27 09:01 brainpy/_src/delay.py
--rw-r--r--  2.0 unx     1500 b- defN 23-Jun-27 09:01 brainpy/_src/deprecations.py
--rw-r--r--  2.0 unx    49179 b- defN 23-Jun-27 09:01 brainpy/_src/dynsys.py
--rw-r--r--  2.0 unx     2328 b- defN 23-Jun-27 09:01 brainpy/_src/mixin.py
--rw-r--r--  2.0 unx     1101 b- defN 23-Jun-27 09:01 brainpy/_src/modes.py
--rw-r--r--  2.0 unx    24973 b- defN 23-Jun-27 09:01 brainpy/_src/runners.py
--rw-r--r--  2.0 unx    10267 b- defN 23-Jun-27 09:01 brainpy/_src/transform.py
--rw-r--r--  2.0 unx     1065 b- defN 23-Jun-27 09:01 brainpy/_src/types.py
--rw-r--r--  2.0 unx      846 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/__init__.py
--rw-r--r--  2.0 unx      156 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/base.py
--rw-r--r--  2.0 unx     1721 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/constants.py
--rw-r--r--  2.0 unx     3924 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/plotstyle.py
--rw-r--r--  2.0 unx     5651 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/stability.py
--rw-r--r--  2.0 unx       52 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/highdim/__init__.py
--rw-r--r--  2.0 unx    30947 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/highdim/slow_points.py
--rw-r--r--  2.0 unx       93 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/lowdim/__init__.py
--rw-r--r--  2.0 unx    44743 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/lowdim/lowdim_analyzer.py
--rw-r--r--  2.0 unx    25000 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/lowdim/lowdim_bifurcation.py
--rw-r--r--  2.0 unx    20273 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/lowdim/lowdim_phase_plane.py
--rw-r--r--  2.0 unx      199 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/utils/__init__.py
--rw-r--r--  2.0 unx     2840 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/utils/function.py
--rw-r--r--  2.0 unx     3054 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/utils/measurement.py
--rw-r--r--  2.0 unx     5298 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/utils/model.py
--rw-r--r--  2.0 unx    19554 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/utils/optimization.py
--rw-r--r--  2.0 unx     5822 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/utils/others.py
--rw-r--r--  2.0 unx      158 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/utils/outputs.py
--rw-r--r--  2.0 unx      967 b- defN 23-Jun-27 09:01 brainpy/_src/analysis/utils/visualization.py
--rw-r--r--  2.0 unx      169 b- defN 23-Jun-27 09:01 brainpy/_src/base/__init__.py
--rw-r--r--  2.0 unx       24 b- defN 23-Jun-27 09:01 brainpy/_src/base/collector.py
--rw-r--r--  2.0 unx       25 b- defN 23-Jun-27 09:01 brainpy/_src/base/function.py
--rw-r--r--  2.0 unx     1029 b- defN 23-Jun-27 09:01 brainpy/_src/base/io.py
--rw-r--r--  2.0 unx      305 b- defN 23-Jun-27 09:01 brainpy/_src/base/naming.py
--rw-r--r--  2.0 unx       24 b- defN 23-Jun-27 09:01 brainpy/_src/checkpoints/__init__.py
--rw-r--r--  2.0 unx    12240 b- defN 23-Jun-27 09:01 brainpy/_src/checkpoints/io.py
--rw-r--r--  2.0 unx    57361 b- defN 23-Jun-27 09:01 brainpy/_src/checkpoints/serialization.py
--rw-r--r--  2.0 unx      268 b- defN 23-Jun-27 09:01 brainpy/_src/connect/__init__.py
--rw-r--r--  2.0 unx    24714 b- defN 23-Jun-27 09:01 brainpy/_src/connect/base.py
--rw-r--r--  2.0 unx     4210 b- defN 23-Jun-27 09:01 brainpy/_src/connect/custom_conn.py
--rw-r--r--  2.0 unx    40725 b- defN 23-Jun-27 09:01 brainpy/_src/connect/random_conn.py
--rw-r--r--  2.0 unx     9173 b- defN 23-Jun-27 09:01 brainpy/_src/connect/regular_conn.py
--rw-r--r--  2.0 unx      295 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/__init__.py
--rw-r--r--  2.0 unx    32514 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/activations.py
--rw-r--r--  2.0 unx      197 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/base.py
--rw-r--r--  2.0 unx    29730 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/conv.py
--rw-r--r--  2.0 unx     1302 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/dropout.py
--rw-r--r--  2.0 unx     1803 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/function.py
--rw-r--r--  2.0 unx     4106 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/interoperation_flax.py
--rw-r--r--  2.0 unx    35318 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/linear.py
--rw-r--r--  2.0 unx    25687 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/normalization.py
--rw-r--r--  2.0 unx     6732 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/nvar.py
--rw-r--r--  2.0 unx    34199 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/pooling.py
--rw-r--r--  2.0 unx     8875 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/reservoir.py
--rw-r--r--  2.0 unx    27026 b- defN 23-Jun-27 09:01 brainpy/_src/dnn/rnncells.py
--rw-r--r--  2.0 unx        1 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/__init__.py
--rw-r--r--  2.0 unx     1382 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/_docs.py
--rw-r--r--  2.0 unx     4882 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/base.py
--rw-r--r--  2.0 unx     5485 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/projections.py
--rw-r--r--  2.0 unx    40139 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/channels/Ca.py
--rw-r--r--  2.0 unx     9190 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/channels/IH.py
--rw-r--r--  2.0 unx    35645 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/channels/K.py
--rw-r--r--  2.0 unx     4491 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/channels/KCa.py
--rw-r--r--  2.0 unx    11936 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/channels/Na.py
--rw-r--r--  2.0 unx      423 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/channels/__init__.py
--rw-r--r--  2.0 unx     4039 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/channels/base.py
--rw-r--r--  2.0 unx     2102 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/channels/leaky.py
--rw-r--r--  2.0 unx       21 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/neurons/__init__.py
--rw-r--r--  2.0 unx    44733 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/neurons/hh.py
--rw-r--r--  2.0 unx     5860 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/neurons/input.py
--rw-r--r--  2.0 unx    84993 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/neurons/lif.py
--rw-r--r--  2.0 unx       28 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/others/__init__.py
--rw-r--r--  2.0 unx     4141 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/others/common.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/synapses/__init__.py
--rw-r--r--  2.0 unx    28351 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/synapses/dynamics.py
--rw-r--r--  2.0 unx     3147 b- defN 23-Jun-27 09:01 brainpy/_src/dyn/synapses/outputs.py
--rw-r--r--  2.0 unx      114 b- defN 23-Jun-27 09:01 brainpy/_src/encoding/__init__.py
--rw-r--r--  2.0 unx      364 b- defN 23-Jun-27 09:01 brainpy/_src/encoding/base.py
--rw-r--r--  2.0 unx     4769 b- defN 23-Jun-27 09:01 brainpy/_src/encoding/stateful_encoding.py
--rw-r--r--  2.0 unx     2156 b- defN 23-Jun-27 09:01 brainpy/_src/encoding/stateless_encoding.py
--rw-r--r--  2.0 unx      154 b- defN 23-Jun-27 09:01 brainpy/_src/initialize/__init__.py
--rw-r--r--  2.0 unx      614 b- defN 23-Jun-27 09:01 brainpy/_src/initialize/base.py
--rw-r--r--  2.0 unx    13428 b- defN 23-Jun-27 09:01 brainpy/_src/initialize/decay_inits.py
--rw-r--r--  2.0 unx    10608 b- defN 23-Jun-27 09:01 brainpy/_src/initialize/generic.py
--rw-r--r--  2.0 unx      554 b- defN 23-Jun-27 09:01 brainpy/_src/initialize/others.py
--rw-r--r--  2.0 unx    13534 b- defN 23-Jun-27 09:01 brainpy/_src/initialize/random_inits.py
--rw-r--r--  2.0 unx     2266 b- defN 23-Jun-27 09:01 brainpy/_src/initialize/regular_inits.py
--rw-r--r--  2.0 unx      173 b- defN 23-Jun-27 09:01 brainpy/_src/inputs/__init__.py
--rw-r--r--  2.0 unx    11432 b- defN 23-Jun-27 09:01 brainpy/_src/inputs/currents.py
--rw-r--r--  2.0 unx     1189 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/__init__.py
--rw-r--r--  2.0 unx     4154 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/base.py
--rw-r--r--  2.0 unx     2946 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/constants.py
--rw-r--r--  2.0 unx     8158 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/joint_eq.py
--rw-r--r--  2.0 unx    11802 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/runner.py
--rw-r--r--  2.0 unx     4455 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/utils.py
--rw-r--r--  2.0 unx    15040 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/fde/Caputo.py
--rw-r--r--  2.0 unx     7233 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/fde/GL.py
--rw-r--r--  2.0 unx      110 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/fde/__init__.py
--rw-r--r--  2.0 unx     2771 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/fde/base.py
--rw-r--r--  2.0 unx     2706 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/fde/generic.py
--rw-r--r--  2.0 unx      220 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/ode/__init__.py
--rw-r--r--  2.0 unx    17892 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/ode/adaptive_rk.py
--rw-r--r--  2.0 unx     4845 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/ode/base.py
--rw-r--r--  2.0 unx     1493 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/ode/common.py
--rw-r--r--  2.0 unx    25974 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/ode/explicit_rk.py
--rw-r--r--  2.0 unx    13756 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/ode/exponential.py
--rw-r--r--  2.0 unx     4139 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/ode/generic.py
--rw-r--r--  2.0 unx       24 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/pde/__init__.py
--rw-r--r--  2.0 unx       98 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/pde/base.py
--rw-r--r--  2.0 unx      184 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/sde/__init__.py
--rw-r--r--  2.0 unx     3321 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/sde/base.py
--rw-r--r--  2.0 unx     3877 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/sde/generic.py
--rw-r--r--  2.0 unx    24171 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/sde/normal.py
--rw-r--r--  2.0 unx    17060 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/sde/srk_scalar.py
--rw-r--r--  2.0 unx    17016 b- defN 23-Jun-27 09:01 brainpy/_src/integrators/sde/srk_strong.py
--rw-r--r--  2.0 unx      274 b- defN 23-Jun-27 09:01 brainpy/_src/losses/__init__.py
--rw-r--r--  2.0 unx      445 b- defN 23-Jun-27 09:01 brainpy/_src/losses/base.py
--rw-r--r--  2.0 unx    40982 b- defN 23-Jun-27 09:01 brainpy/_src/losses/comparison.py
--rw-r--r--  2.0 unx     2368 b- defN 23-Jun-27 09:01 brainpy/_src/losses/regularization.py
--rw-r--r--  2.0 unx      792 b- defN 23-Jun-27 09:01 brainpy/_src/losses/utils.py
--rw-r--r--  2.0 unx     1488 b- defN 23-Jun-27 09:01 brainpy/_src/math/__init__.py
--rw-r--r--  2.0 unx     1792 b- defN 23-Jun-27 09:01 brainpy/_src/math/_utils.py
--rw-r--r--  2.0 unx    18933 b- defN 23-Jun-27 09:01 brainpy/_src/math/activations.py
--rw-r--r--  2.0 unx    29884 b- defN 23-Jun-27 09:01 brainpy/_src/math/compat_numpy.py
--rw-r--r--  2.0 unx     6488 b- defN 23-Jun-27 09:01 brainpy/_src/math/compat_pytorch.py
--rw-r--r--  2.0 unx    17887 b- defN 23-Jun-27 09:01 brainpy/_src/math/compat_tensorflow.py
--rw-r--r--  2.0 unx      911 b- defN 23-Jun-27 09:01 brainpy/_src/math/datatypes.py
--rw-r--r--  2.0 unx    15861 b- defN 23-Jun-27 09:01 brainpy/_src/math/delayvars.py
--rw-r--r--  2.0 unx    16825 b- defN 23-Jun-27 09:01 brainpy/_src/math/environment.py
--rw-r--r--  2.0 unx     1498 b- defN 23-Jun-27 09:01 brainpy/_src/math/fft.py
--rw-r--r--  2.0 unx     8866 b- defN 23-Jun-27 09:01 brainpy/_src/math/index_tricks.py
--rw-r--r--  2.0 unx     2523 b- defN 23-Jun-27 09:01 brainpy/_src/math/interoperability.py
--rw-r--r--  2.0 unx     1792 b- defN 23-Jun-27 09:01 brainpy/_src/math/linalg.py
--rw-r--r--  2.0 unx     2319 b- defN 23-Jun-27 09:01 brainpy/_src/math/modes.py
--rw-r--r--  2.0 unx    47277 b- defN 23-Jun-27 09:01 brainpy/_src/math/ndarray.py
--rw-r--r--  2.0 unx     2255 b- defN 23-Jun-27 09:01 brainpy/_src/math/others.py
--rw-r--r--  2.0 unx    15894 b- defN 23-Jun-27 09:01 brainpy/_src/math/pre_syn_post.py
--rw-r--r--  2.0 unx    78579 b- defN 23-Jun-27 09:01 brainpy/_src/math/random.py
--rw-r--r--  2.0 unx     2088 b- defN 23-Jun-27 09:01 brainpy/_src/math/remove_vmap.py
--rw-r--r--  2.0 unx     3578 b- defN 23-Jun-27 09:01 brainpy/_src/math/sharding.py
--rw-r--r--  2.0 unx       61 b- defN 23-Jun-27 09:01 brainpy/_src/math/event/__init__.py
--rw-r--r--  2.0 unx    17787 b- defN 23-Jun-27 09:01 brainpy/_src/math/event/_csr_matvec.py
--rw-r--r--  2.0 unx     5062 b- defN 23-Jun-27 09:01 brainpy/_src/math/event/_info_collection.py
--rw-r--r--  2.0 unx       53 b- defN 23-Jun-27 09:01 brainpy/_src/math/jitconn/__init__.py
--rw-r--r--  2.0 unx    26368 b- defN 23-Jun-27 09:01 brainpy/_src/math/jitconn/_event_matvec.py
--rw-r--r--  2.0 unx    28793 b- defN 23-Jun-27 09:01 brainpy/_src/math/jitconn/_matvec.py
--rw-r--r--  2.0 unx      914 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/__init__.py
--rw-r--r--  2.0 unx     3615 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/_tools.py
--rw-r--r--  2.0 unx      758 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/_utils.py
--rw-r--r--  2.0 unx    40562 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/autograd.py
--rw-r--r--  2.0 unx    22773 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/base.py
--rw-r--r--  2.0 unx     5931 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/collectors.py
--rw-r--r--  2.0 unx    32211 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/controls.py
--rw-r--r--  2.0 unx     2999 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/function.py
--rw-r--r--  2.0 unx    16336 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/jit.py
--rw-r--r--  2.0 unx     1637 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/naming.py
--rw-r--r--  2.0 unx    17774 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/parallels.py
--rw-r--r--  2.0 unx    14573 b- defN 23-Jun-27 09:01 brainpy/_src/math/object_transform/variables.py
--rw-r--r--  2.0 unx      204 b- defN 23-Jun-27 09:01 brainpy/_src/math/op_registers/__init__.py
--rw-r--r--  2.0 unx     1051 b- defN 23-Jun-27 09:01 brainpy/_src/math/op_registers/utils.py
--rw-r--r--  2.0 unx     7129 b- defN 23-Jun-27 09:01 brainpy/_src/math/op_registers/numba_approach/__init__.py
--rw-r--r--  2.0 unx     5129 b- defN 23-Jun-27 09:01 brainpy/_src/math/op_registers/numba_approach/cpu_translation.py
--rw-r--r--  2.0 unx      142 b- defN 23-Jun-27 09:01 brainpy/_src/math/sparse/__init__.py
--rw-r--r--  2.0 unx    14576 b- defN 23-Jun-27 09:01 brainpy/_src/math/sparse/_bsr_mm.py
--rw-r--r--  2.0 unx     8134 b- defN 23-Jun-27 09:01 brainpy/_src/math/sparse/_bsr_mv.py
--rw-r--r--  2.0 unx     7127 b- defN 23-Jun-27 09:01 brainpy/_src/math/sparse/_coo_mv.py
--rw-r--r--  2.0 unx    16697 b- defN 23-Jun-27 09:01 brainpy/_src/math/sparse/_csr_mv.py
--rw-r--r--  2.0 unx     4439 b- defN 23-Jun-27 09:01 brainpy/_src/math/sparse/_jax_prim.py
--rw-r--r--  2.0 unx     5021 b- defN 23-Jun-27 09:01 brainpy/_src/math/sparse/_utils.py
--rw-r--r--  2.0 unx       99 b- defN 23-Jun-27 09:01 brainpy/_src/math/surrogate/__init__.py
--rw-r--r--  2.0 unx     7216 b- defN 23-Jun-27 09:01 brainpy/_src/math/surrogate/_compt.py
--rw-r--r--  2.0 unx    42623 b- defN 23-Jun-27 09:01 brainpy/_src/math/surrogate/_one_input.py
--rw-r--r--  2.0 unx     1492 b- defN 23-Jun-27 09:01 brainpy/_src/math/surrogate/_two_inputs.py
--rw-r--r--  2.0 unx     3665 b- defN 23-Jun-27 09:01 brainpy/_src/math/surrogate/_utils.py
--rw-r--r--  2.0 unx      243 b- defN 23-Jun-27 09:01 brainpy/_src/math/surrogate/base.py
--rw-r--r--  2.0 unx      287 b- defN 23-Jun-27 09:01 brainpy/_src/measure/__init__.py
--rw-r--r--  2.0 unx    10102 b- defN 23-Jun-27 09:01 brainpy/_src/measure/correlation.py
--rw-r--r--  2.0 unx     1770 b- defN 23-Jun-27 09:01 brainpy/_src/measure/firings.py
--rw-r--r--  2.0 unx     3896 b- defN 23-Jun-27 09:01 brainpy/_src/measure/lfp.py
--rw-r--r--  2.0 unx      177 b- defN 23-Jun-27 09:01 brainpy/_src/neurons/__init__.py
--rw-r--r--  2.0 unx    48772 b- defN 23-Jun-27 09:01 brainpy/_src/neurons/biological_models.py
--rw-r--r--  2.0 unx      595 b- defN 23-Jun-27 09:01 brainpy/_src/neurons/compat.py
--rw-r--r--  2.0 unx    13124 b- defN 23-Jun-27 09:01 brainpy/_src/neurons/fractional_models.py
--rw-r--r--  2.0 unx     5477 b- defN 23-Jun-27 09:01 brainpy/_src/neurons/input_groups.py
--rw-r--r--  2.0 unx     2301 b- defN 23-Jun-27 09:01 brainpy/_src/neurons/noise_groups.py
--rw-r--r--  2.0 unx    92062 b- defN 23-Jun-27 09:01 brainpy/_src/neurons/reduced_models.py
--rw-r--r--  2.0 unx       75 b- defN 23-Jun-27 09:01 brainpy/_src/optimizers/__init__.py
--rw-r--r--  2.0 unx    41331 b- defN 23-Jun-27 09:01 brainpy/_src/optimizers/optimizer.py
--rw-r--r--  2.0 unx    13240 b- defN 23-Jun-27 09:01 brainpy/_src/optimizers/scheduler.py
--rw-r--r--  2.0 unx       52 b- defN 23-Jun-27 09:01 brainpy/_src/rates/__init__.py
--rw-r--r--  2.0 unx    41216 b- defN 23-Jun-27 09:01 brainpy/_src/rates/populations.py
--rw-r--r--  2.0 unx      525 b- defN 23-Jun-27 09:01 brainpy/_src/running/__init__.py
--rw-r--r--  2.0 unx      269 b- defN 23-Jun-27 09:01 brainpy/_src/running/constants.py
--rw-r--r--  2.0 unx     4913 b- defN 23-Jun-27 09:01 brainpy/_src/running/jax_multiprocessing.py
--rw-r--r--  2.0 unx     2976 b- defN 23-Jun-27 09:01 brainpy/_src/running/native_multiprocessing.py
--rw-r--r--  2.0 unx     6995 b- defN 23-Jun-27 09:01 brainpy/_src/running/pathos_multiprocessing.py
--rw-r--r--  2.0 unx     9779 b- defN 23-Jun-27 09:01 brainpy/_src/running/runner.py
--rw-r--r--  2.0 unx      223 b- defN 23-Jun-27 09:01 brainpy/_src/synapses/__init__.py
--rw-r--r--  2.0 unx    35980 b- defN 23-Jun-27 09:01 brainpy/_src/synapses/abstract_models.py
--rw-r--r--  2.0 unx    21845 b- defN 23-Jun-27 09:01 brainpy/_src/synapses/biological_models.py
--rw-r--r--  2.0 unx    10246 b- defN 23-Jun-27 09:01 brainpy/_src/synapses/compat.py
--rw-r--r--  2.0 unx    11133 b- defN 23-Jun-27 09:01 brainpy/_src/synapses/delay_couplings.py
--rw-r--r--  2.0 unx     2023 b- defN 23-Jun-27 09:01 brainpy/_src/synapses/gap_junction.py
--rw-r--r--  2.0 unx     9212 b- defN 23-Jun-27 09:01 brainpy/_src/synapses/learning_rules.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-27 09:01 brainpy/_src/synapses_v2/__init__.py
--rw-r--r--  2.0 unx    13557 b- defN 23-Jun-27 09:01 brainpy/_src/synapses_v2/abstract_synapses.py
--rw-r--r--  2.0 unx     4649 b- defN 23-Jun-27 09:01 brainpy/_src/synapses_v2/base.py
--rw-r--r--  2.0 unx     2553 b- defN 23-Jun-27 09:01 brainpy/_src/synapses_v2/others.py
--rw-r--r--  2.0 unx     2648 b- defN 23-Jun-27 09:01 brainpy/_src/synapses_v2/syn_outs.py
--rw-r--r--  2.0 unx     4722 b- defN 23-Jun-27 09:01 brainpy/_src/synapses_v2/syn_plasticity.py
--rw-r--r--  2.0 unx       73 b- defN 23-Jun-27 09:01 brainpy/_src/synouts/__init__.py
--rw-r--r--  2.0 unx     2634 b- defN 23-Jun-27 09:01 brainpy/_src/synouts/conductances.py
--rw-r--r--  2.0 unx     3279 b- defN 23-Jun-27 09:01 brainpy/_src/synouts/ions.py
--rw-r--r--  2.0 unx       62 b- defN 23-Jun-27 09:01 brainpy/_src/synplast/__init__.py
--rw-r--r--  2.0 unx       24 b- defN 23-Jun-27 09:01 brainpy/_src/synplast/long_term_plasticity.py
--rw-r--r--  2.0 unx     5168 b- defN 23-Jun-27 09:01 brainpy/_src/synplast/short_term_plasticity.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-27 09:01 brainpy/_src/testing/__init__.py
--rw-r--r--  2.0 unx      422 b- defN 23-Jun-27 09:01 brainpy/_src/testing/base.py
--rw-r--r--  2.0 unx      182 b- defN 23-Jun-27 09:01 brainpy/_src/tools/__init__.py
--rw-r--r--  2.0 unx     7448 b- defN 23-Jun-27 09:01 brainpy/_src/tools/codes.py
--rw-r--r--  2.0 unx     5882 b- defN 23-Jun-27 09:01 brainpy/_src/tools/dicts.py
--rw-r--r--  2.0 unx     1015 b- defN 23-Jun-27 09:01 brainpy/_src/tools/install.py
--rw-r--r--  2.0 unx      223 b- defN 23-Jun-27 09:01 brainpy/_src/tools/math_util.py
--rw-r--r--  2.0 unx     4425 b- defN 23-Jun-27 09:01 brainpy/_src/tools/others.py
--rw-r--r--  2.0 unx     1407 b- defN 23-Jun-27 09:01 brainpy/_src/tools/package.py
--rw-r--r--  2.0 unx      562 b- defN 23-Jun-27 09:01 brainpy/_src/train/__init__.py
--rw-r--r--  2.0 unx     1970 b- defN 23-Jun-27 09:01 brainpy/_src/train/_utils.py
--rw-r--r--  2.0 unx    24067 b- defN 23-Jun-27 09:01 brainpy/_src/train/back_propagation.py
--rw-r--r--  2.0 unx     2974 b- defN 23-Jun-27 09:01 brainpy/_src/train/base.py
--rw-r--r--  2.0 unx    10034 b- defN 23-Jun-27 09:01 brainpy/_src/train/offline.py
--rw-r--r--  2.0 unx    10356 b- defN 23-Jun-27 09:01 brainpy/_src/train/online.py
--rw-r--r--  2.0 unx       77 b- defN 23-Jun-27 09:01 brainpy/_src/visualization/__init__.py
--rw-r--r--  2.0 unx     3544 b- defN 23-Jun-27 09:01 brainpy/_src/visualization/base.py
--rw-r--r--  2.0 unx      841 b- defN 23-Jun-27 09:01 brainpy/_src/visualization/figures.py
--rw-r--r--  2.0 unx    14603 b- defN 23-Jun-27 09:01 brainpy/_src/visualization/plots.py
--rw-r--r--  2.0 unx     1026 b- defN 23-Jun-27 09:01 brainpy/_src/visualization/styles.py
--rw-r--r--  2.0 unx       90 b- defN 23-Jun-27 09:01 brainpy/algorithms/__init__.py
--rw-r--r--  2.0 unx    17344 b- defN 23-Jun-27 09:01 brainpy/algorithms/offline.py
--rw-r--r--  2.0 unx     6284 b- defN 23-Jun-27 09:01 brainpy/algorithms/online.py
--rw-r--r--  2.0 unx     2656 b- defN 23-Jun-27 09:01 brainpy/algorithms/utils.py
--rw-r--r--  2.0 unx      121 b- defN 23-Jun-27 09:01 brainpy/dyn/__init__.py
--rw-r--r--  2.0 unx      820 b- defN 23-Jun-27 09:01 brainpy/dyn/channels.py
--rw-r--r--  2.0 unx      737 b- defN 23-Jun-27 09:01 brainpy/dyn/neurons.py
--rw-r--r--  2.0 unx       69 b- defN 23-Jun-27 09:01 brainpy/dyn/others.py
--rw-r--r--  2.0 unx       81 b- defN 23-Jun-27 09:01 brainpy/dyn/projections.py
--rw-r--r--  2.0 unx      271 b- defN 23-Jun-27 09:01 brainpy/dyn/synapses.py
--rw-r--r--  2.0 unx      128 b- defN 23-Jun-27 09:01 brainpy/integrators/__init__.py
--rw-r--r--  2.0 unx      576 b- defN 23-Jun-27 09:01 brainpy/integrators/fde.py
--rw-r--r--  2.0 unx     1061 b- defN 23-Jun-27 09:01 brainpy/integrators/ode.py
--rw-r--r--  2.0 unx      679 b- defN 23-Jun-27 09:01 brainpy/integrators/sde.py
--rw-r--r--  2.0 unx     4768 b- defN 23-Jun-27 09:01 brainpy/math/__init__.py
--rw-r--r--  2.0 unx      780 b- defN 23-Jun-27 09:01 brainpy/math/activations.py
--rw-r--r--  2.0 unx     8074 b- defN 23-Jun-27 09:01 brainpy/math/compat_numpy.py
--rw-r--r--  2.0 unx      469 b- defN 23-Jun-27 09:01 brainpy/math/compat_pytorch.py
--rw-r--r--  2.0 unx      932 b- defN 23-Jun-27 09:01 brainpy/math/compat_tensorflow.py
--rw-r--r--  2.0 unx      498 b- defN 23-Jun-27 09:01 brainpy/math/datatypes.py
--rw-r--r--  2.0 unx      255 b- defN 23-Jun-27 09:01 brainpy/math/delayvars.py
--rw-r--r--  2.0 unx      943 b- defN 23-Jun-27 09:01 brainpy/math/environment.py
--rw-r--r--  2.0 unx       75 b- defN 23-Jun-27 09:01 brainpy/math/event.py
--rw-r--r--  2.0 unx      402 b- defN 23-Jun-27 09:01 brainpy/math/fft.py
--rw-r--r--  2.0 unx      217 b- defN 23-Jun-27 09:01 brainpy/math/interoperability.py
--rw-r--r--  2.0 unx      292 b- defN 23-Jun-27 09:01 brainpy/math/jitconn.py
--rw-r--r--  2.0 unx      470 b- defN 23-Jun-27 09:01 brainpy/math/linalg.py
--rw-r--r--  2.0 unx      292 b- defN 23-Jun-27 09:01 brainpy/math/modes.py
--rw-r--r--  2.0 unx      150 b- defN 23-Jun-27 09:01 brainpy/math/ndarray.py
--rw-r--r--  2.0 unx     1225 b- defN 23-Jun-27 09:01 brainpy/math/object_base.py
--rw-r--r--  2.0 unx      651 b- defN 23-Jun-27 09:01 brainpy/math/object_transform.py
--rw-r--r--  2.0 unx      127 b- defN 23-Jun-27 09:01 brainpy/math/op_register.py
--rw-r--r--  2.0 unx      257 b- defN 23-Jun-27 09:01 brainpy/math/others.py
--rw-r--r--  2.0 unx      331 b- defN 23-Jun-27 09:01 brainpy/math/pre_syn_post.py
--rw-r--r--  2.0 unx     1770 b- defN 23-Jun-27 09:01 brainpy/math/random.py
--rw-r--r--  2.0 unx      213 b- defN 23-Jun-27 09:01 brainpy/math/sharding.py
--rw-r--r--  2.0 unx      164 b- defN 23-Jun-27 09:01 brainpy/math/sparse.py
--rw-r--r--  2.0 unx     1249 b- defN 23-Jun-27 09:01 brainpy/math/surrogate.py
--rw-r--r--  2.0 unx       73 b- defN 23-Jun-27 09:01 brainpy/synapses/__init__.py
--rw-r--r--  2.0 unx      596 b- defN 23-Jun-27 09:01 brainpy/synapses/dynamics.py
--rw-r--r--  2.0 unx      172 b- defN 23-Jun-27 09:01 brainpy/synapses/synouts.py
--rw-r--r--  2.0 unx      113 b- defN 23-Jun-27 09:01 brainpy/synapses/synplast.py
--rw-r--r--  2.0 unx    35100 b- defN 23-Jun-27 09:01 brainpy-2.4.2.dist-info/LICENSE
--rw-r--r--  2.0 unx     4131 b- defN 23-Jun-27 09:01 brainpy-2.4.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-27 09:01 brainpy-2.4.2.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 23-Jun-27 09:01 brainpy-2.4.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    27335 b- defN 23-Jun-27 09:01 brainpy-2.4.2.dist-info/RECORD
-308 files, 2583636 bytes uncompressed, 587735 bytes compressed:  77.3%
+Zip file size: 652421 bytes, number of entries: 333
+-rw-r--r--  2.0 unx     4574 b- defN 23-Jul-23 02:34 brainpy/__init__.py
+-rw-r--r--  2.0 unx     5966 b- defN 23-Jul-23 02:34 brainpy/_add_deprecations.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Jul-23 02:34 brainpy/analysis.py
+-rw-r--r--  2.0 unx       77 b- defN 23-Jul-23 02:34 brainpy/channels.py
+-rw-r--r--  2.0 unx    18984 b- defN 23-Jul-23 02:34 brainpy/check.py
+-rw-r--r--  2.0 unx      407 b- defN 23-Jul-23 02:34 brainpy/checkpoints.py
+-rw-r--r--  2.0 unx     1275 b- defN 23-Jul-23 02:34 brainpy/connect.py
+-rw-r--r--  2.0 unx      324 b- defN 23-Jul-23 02:34 brainpy/encoding.py
+-rw-r--r--  2.0 unx     7567 b- defN 23-Jul-23 02:34 brainpy/errors.py
+-rw-r--r--  2.0 unx      366 b- defN 23-Jul-23 02:34 brainpy/experimental.py
+-rw-r--r--  2.0 unx     1123 b- defN 23-Jul-23 02:34 brainpy/initialize.py
+-rw-r--r--  2.0 unx      335 b- defN 23-Jul-23 02:34 brainpy/inputs.py
+-rw-r--r--  2.0 unx       20 b- defN 23-Jul-23 02:34 brainpy/layers.py
+-rw-r--r--  2.0 unx     1086 b- defN 23-Jul-23 02:34 brainpy/losses.py
+-rw-r--r--  2.0 unx      487 b- defN 23-Jul-23 02:34 brainpy/measure.py
+-rw-r--r--  2.0 unx      266 b- defN 23-Jul-23 02:34 brainpy/mixin.py
+-rw-r--r--  2.0 unx      961 b- defN 23-Jul-23 02:34 brainpy/neurons.py
+-rw-r--r--  2.0 unx     1013 b- defN 23-Jul-23 02:34 brainpy/optim.py
+-rw-r--r--  2.0 unx       52 b- defN 23-Jul-23 02:34 brainpy/rates.py
+-rw-r--r--  2.0 unx      466 b- defN 23-Jul-23 02:34 brainpy/running.py
+-rw-r--r--  2.0 unx      931 b- defN 23-Jul-23 02:34 brainpy/synapses.py
+-rw-r--r--  2.0 unx      186 b- defN 23-Jul-23 02:34 brainpy/synouts.py
+-rw-r--r--  2.0 unx      120 b- defN 23-Jul-23 02:34 brainpy/synplast.py
+-rw-r--r--  2.0 unx     1046 b- defN 23-Jul-23 02:34 brainpy/tools.py
+-rw-r--r--  2.0 unx      265 b- defN 23-Jul-23 02:34 brainpy/types.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-23 02:34 brainpy/_src/__init__.py
+-rw-r--r--  2.0 unx     9941 b- defN 23-Jul-23 02:34 brainpy/_src/_delay.py
+-rw-r--r--  2.0 unx     2457 b- defN 23-Jul-23 02:34 brainpy/_src/checking.py
+-rw-r--r--  2.0 unx     2609 b- defN 23-Jul-23 02:34 brainpy/_src/context.py
+-rw-r--r--  2.0 unx    22973 b- defN 23-Jul-23 02:34 brainpy/_src/delay.py
+-rw-r--r--  2.0 unx     2031 b- defN 23-Jul-23 02:34 brainpy/_src/deprecations.py
+-rw-r--r--  2.0 unx    25214 b- defN 23-Jul-23 02:34 brainpy/_src/dynsys.py
+-rw-r--r--  2.0 unx    19533 b- defN 23-Jul-23 02:34 brainpy/_src/mixin.py
+-rw-r--r--  2.0 unx     1101 b- defN 23-Jul-23 02:34 brainpy/_src/modes.py
+-rw-r--r--  2.0 unx    23348 b- defN 23-Jul-23 02:34 brainpy/_src/runners.py
+-rw-r--r--  2.0 unx    10247 b- defN 23-Jul-23 02:34 brainpy/_src/transform.py
+-rw-r--r--  2.0 unx     1065 b- defN 23-Jul-23 02:34 brainpy/_src/types.py
+-rw-r--r--  2.0 unx      846 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/__init__.py
+-rw-r--r--  2.0 unx      156 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/base.py
+-rw-r--r--  2.0 unx     1721 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/constants.py
+-rw-r--r--  2.0 unx     3924 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/plotstyle.py
+-rw-r--r--  2.0 unx     5651 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/stability.py
+-rw-r--r--  2.0 unx       52 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/highdim/__init__.py
+-rw-r--r--  2.0 unx    31167 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/highdim/slow_points.py
+-rw-r--r--  2.0 unx       93 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/lowdim/__init__.py
+-rw-r--r--  2.0 unx    44554 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/lowdim/lowdim_analyzer.py
+-rw-r--r--  2.0 unx    24510 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/lowdim/lowdim_bifurcation.py
+-rw-r--r--  2.0 unx    20019 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/lowdim/lowdim_phase_plane.py
+-rw-r--r--  2.0 unx      199 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/utils/__init__.py
+-rw-r--r--  2.0 unx     2840 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/utils/function.py
+-rw-r--r--  2.0 unx     3054 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/utils/measurement.py
+-rw-r--r--  2.0 unx     5195 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/utils/model.py
+-rw-r--r--  2.0 unx    19554 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/utils/optimization.py
+-rw-r--r--  2.0 unx     5822 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/utils/others.py
+-rw-r--r--  2.0 unx      158 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/utils/outputs.py
+-rw-r--r--  2.0 unx      967 b- defN 23-Jul-23 02:34 brainpy/_src/analysis/utils/visualization.py
+-rw-r--r--  2.0 unx      169 b- defN 23-Jul-23 02:34 brainpy/_src/base/__init__.py
+-rw-r--r--  2.0 unx       24 b- defN 23-Jul-23 02:34 brainpy/_src/base/collector.py
+-rw-r--r--  2.0 unx       25 b- defN 23-Jul-23 02:34 brainpy/_src/base/function.py
+-rw-r--r--  2.0 unx     1029 b- defN 23-Jul-23 02:34 brainpy/_src/base/io.py
+-rw-r--r--  2.0 unx      305 b- defN 23-Jul-23 02:34 brainpy/_src/base/naming.py
+-rw-r--r--  2.0 unx       24 b- defN 23-Jul-23 02:34 brainpy/_src/checkpoints/__init__.py
+-rw-r--r--  2.0 unx    12240 b- defN 23-Jul-23 02:34 brainpy/_src/checkpoints/io.py
+-rw-r--r--  2.0 unx    57361 b- defN 23-Jul-23 02:34 brainpy/_src/checkpoints/serialization.py
+-rw-r--r--  2.0 unx      268 b- defN 23-Jul-23 02:34 brainpy/_src/connect/__init__.py
+-rw-r--r--  2.0 unx    24721 b- defN 23-Jul-23 02:34 brainpy/_src/connect/base.py
+-rw-r--r--  2.0 unx     4218 b- defN 23-Jul-23 02:34 brainpy/_src/connect/custom_conn.py
+-rw-r--r--  2.0 unx    51213 b- defN 23-Jul-23 02:34 brainpy/_src/connect/random_conn.py
+-rw-r--r--  2.0 unx     9173 b- defN 23-Jul-23 02:34 brainpy/_src/connect/regular_conn.py
+-rw-r--r--  2.0 unx      295 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/__init__.py
+-rw-r--r--  2.0 unx    32584 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/activations.py
+-rw-r--r--  2.0 unx      220 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/base.py
+-rw-r--r--  2.0 unx    29858 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/conv.py
+-rw-r--r--  2.0 unx     1413 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/dropout.py
+-rw-r--r--  2.0 unx     1819 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/function.py
+-rw-r--r--  2.0 unx     4115 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/interoperation_flax.py
+-rw-r--r--  2.0 unx    36039 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/linear.py
+-rw-r--r--  2.0 unx    25703 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/normalization.py
+-rw-r--r--  2.0 unx     6748 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/nvar.py
+-rw-r--r--  2.0 unx    34217 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/pooling.py
+-rw-r--r--  2.0 unx     8891 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/reservoir.py
+-rw-r--r--  2.0 unx    27042 b- defN 23-Jul-23 02:34 brainpy/_src/dnn/rnncells.py
+-rw-r--r--  2.0 unx        1 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/__init__.py
+-rw-r--r--  2.0 unx     1612 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/_docs.py
+-rw-r--r--  2.0 unx      396 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/base.py
+-rw-r--r--  2.0 unx      416 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/utils.py
+-rw-r--r--  2.0 unx      212 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/__init__.py
+-rw-r--r--  2.0 unx      774 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/base.py
+-rw-r--r--  2.0 unx    27906 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/calcium.py
+-rw-r--r--  2.0 unx     9192 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/hyperpolarization_activated.py
+-rw-r--r--  2.0 unx     1502 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/leaky.py
+-rw-r--r--  2.0 unx    70273 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/potassium.py
+-rw-r--r--  2.0 unx     4561 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/potassium_calcium.py
+-rw-r--r--  2.0 unx     4653 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/potassium_calcium_compatible.py
+-rw-r--r--  2.0 unx    36885 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/potassium_compatible.py
+-rw-r--r--  2.0 unx    11637 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/sodium.py
+-rw-r--r--  2.0 unx    12054 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/channels/sodium_compatible.py
+-rw-r--r--  2.0 unx       91 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/ions/__init__.py
+-rw-r--r--  2.0 unx     6867 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/ions/base.py
+-rw-r--r--  2.0 unx    11715 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/ions/calcium.py
+-rw-r--r--  2.0 unx     1469 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/ions/potassium.py
+-rw-r--r--  2.0 unx     1487 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/ions/sodium.py
+-rw-r--r--  2.0 unx       21 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/neurons/__init__.py
+-rw-r--r--  2.0 unx     1422 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/neurons/base.py
+-rw-r--r--  2.0 unx    49806 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/neurons/hh.py
+-rw-r--r--  2.0 unx    87391 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/neurons/lif.py
+-rw-r--r--  2.0 unx       28 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/others/__init__.py
+-rw-r--r--  2.0 unx     4131 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/others/common.py
+-rw-r--r--  2.0 unx     6748 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/others/input.py
+-rw-r--r--  2.0 unx     2195 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/others/noise.py
+-rw-r--r--  2.0 unx       43 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/outs/__init__.py
+-rw-r--r--  2.0 unx      755 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/outs/base.py
+-rw-r--r--  2.0 unx     3185 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/outs/outputs.py
+-rw-r--r--  2.0 unx       65 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/projections/__init__.py
+-rw-r--r--  2.0 unx    29373 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/projections/aligns.py
+-rw-r--r--  2.0 unx     3766 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/projections/conn.py
+-rw-r--r--  2.0 unx     2694 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/projections/others.py
+-rw-r--r--  2.0 unx       52 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/rates/__init__.py
+-rw-r--r--  2.0 unx    41198 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/rates/populations.py
+-rw-r--r--  2.0 unx       58 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/synapses/__init__.py
+-rw-r--r--  2.0 unx    21874 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/synapses/abstract_models.py
+-rw-r--r--  2.0 unx    11826 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/synapses/bio_models.py
+-rw-r--r--  2.0 unx    10834 b- defN 23-Jul-23 02:34 brainpy/_src/dyn/synapses/delay_couplings.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/experimental/__init__.py
+-rw-r--r--  2.0 unx    13565 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/experimental/abstract_synapses.py
+-rw-r--r--  2.0 unx     4641 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/experimental/base.py
+-rw-r--r--  2.0 unx     2549 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/experimental/others.py
+-rw-r--r--  2.0 unx     2656 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/experimental/syn_outs.py
+-rw-r--r--  2.0 unx     4730 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/experimental/syn_plasticity.py
+-rw-r--r--  2.0 unx      121 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/neurons/__init__.py
+-rw-r--r--  2.0 unx    37030 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/neurons/biological_models.py
+-rw-r--r--  2.0 unx    13087 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/neurons/fractional_models.py
+-rw-r--r--  2.0 unx    60694 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/neurons/reduced_models.py
+-rw-r--r--  2.0 unx      162 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synapses/__init__.py
+-rw-r--r--  2.0 unx    28740 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synapses/abstract_models.py
+-rw-r--r--  2.0 unx    10491 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synapses/base.py
+-rw-r--r--  2.0 unx    13419 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synapses/biological_models.py
+-rw-r--r--  2.0 unx     7569 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synapses/compat.py
+-rw-r--r--  2.0 unx     2054 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synapses/gap_junction.py
+-rw-r--r--  2.0 unx     8569 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synapses/learning_rules.py
+-rw-r--r--  2.0 unx       73 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synouts/__init__.py
+-rw-r--r--  2.0 unx     2620 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synouts/conductances.py
+-rw-r--r--  2.0 unx     3268 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synouts/ions.py
+-rw-r--r--  2.0 unx       62 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synplast/__init__.py
+-rw-r--r--  2.0 unx     5589 b- defN 23-Jul-23 02:34 brainpy/_src/dynold/synplast/short_term_plasticity.py
+-rw-r--r--  2.0 unx      114 b- defN 23-Jul-23 02:34 brainpy/_src/encoding/__init__.py
+-rw-r--r--  2.0 unx      364 b- defN 23-Jul-23 02:34 brainpy/_src/encoding/base.py
+-rw-r--r--  2.0 unx     4769 b- defN 23-Jul-23 02:34 brainpy/_src/encoding/stateful_encoding.py
+-rw-r--r--  2.0 unx     2156 b- defN 23-Jul-23 02:34 brainpy/_src/encoding/stateless_encoding.py
+-rw-r--r--  2.0 unx      154 b- defN 23-Jul-23 02:34 brainpy/_src/initialize/__init__.py
+-rw-r--r--  2.0 unx      614 b- defN 23-Jul-23 02:34 brainpy/_src/initialize/base.py
+-rw-r--r--  2.0 unx    13428 b- defN 23-Jul-23 02:34 brainpy/_src/initialize/decay_inits.py
+-rw-r--r--  2.0 unx    10608 b- defN 23-Jul-23 02:34 brainpy/_src/initialize/generic.py
+-rw-r--r--  2.0 unx      554 b- defN 23-Jul-23 02:34 brainpy/_src/initialize/others.py
+-rw-r--r--  2.0 unx    13534 b- defN 23-Jul-23 02:34 brainpy/_src/initialize/random_inits.py
+-rw-r--r--  2.0 unx     2266 b- defN 23-Jul-23 02:34 brainpy/_src/initialize/regular_inits.py
+-rw-r--r--  2.0 unx      173 b- defN 23-Jul-23 02:34 brainpy/_src/inputs/__init__.py
+-rw-r--r--  2.0 unx    11432 b- defN 23-Jul-23 02:34 brainpy/_src/inputs/currents.py
+-rw-r--r--  2.0 unx     1189 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/__init__.py
+-rw-r--r--  2.0 unx     4154 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/base.py
+-rw-r--r--  2.0 unx     2946 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/constants.py
+-rw-r--r--  2.0 unx     8158 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/joint_eq.py
+-rw-r--r--  2.0 unx    11802 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/runner.py
+-rw-r--r--  2.0 unx     4455 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/utils.py
+-rw-r--r--  2.0 unx    15040 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/fde/Caputo.py
+-rw-r--r--  2.0 unx     7233 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/fde/GL.py
+-rw-r--r--  2.0 unx      110 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/fde/__init__.py
+-rw-r--r--  2.0 unx     2771 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/fde/base.py
+-rw-r--r--  2.0 unx     2706 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/fde/generic.py
+-rw-r--r--  2.0 unx      220 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/ode/__init__.py
+-rw-r--r--  2.0 unx    17892 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/ode/adaptive_rk.py
+-rw-r--r--  2.0 unx     4845 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/ode/base.py
+-rw-r--r--  2.0 unx     1493 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/ode/common.py
+-rw-r--r--  2.0 unx    25974 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/ode/explicit_rk.py
+-rw-r--r--  2.0 unx    13760 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/ode/exponential.py
+-rw-r--r--  2.0 unx     4139 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/ode/generic.py
+-rw-r--r--  2.0 unx       24 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/pde/__init__.py
+-rw-r--r--  2.0 unx       98 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/pde/base.py
+-rw-r--r--  2.0 unx      184 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/sde/__init__.py
+-rw-r--r--  2.0 unx     3321 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/sde/base.py
+-rw-r--r--  2.0 unx     3877 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/sde/generic.py
+-rw-r--r--  2.0 unx    24171 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/sde/normal.py
+-rw-r--r--  2.0 unx    17060 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/sde/srk_scalar.py
+-rw-r--r--  2.0 unx    17016 b- defN 23-Jul-23 02:34 brainpy/_src/integrators/sde/srk_strong.py
+-rw-r--r--  2.0 unx      274 b- defN 23-Jul-23 02:34 brainpy/_src/losses/__init__.py
+-rw-r--r--  2.0 unx      445 b- defN 23-Jul-23 02:34 brainpy/_src/losses/base.py
+-rw-r--r--  2.0 unx    40982 b- defN 23-Jul-23 02:34 brainpy/_src/losses/comparison.py
+-rw-r--r--  2.0 unx     2368 b- defN 23-Jul-23 02:34 brainpy/_src/losses/regularization.py
+-rw-r--r--  2.0 unx      792 b- defN 23-Jul-23 02:34 brainpy/_src/losses/utils.py
+-rw-r--r--  2.0 unx     1488 b- defN 23-Jul-23 02:34 brainpy/_src/math/__init__.py
+-rw-r--r--  2.0 unx     1792 b- defN 23-Jul-23 02:34 brainpy/_src/math/_utils.py
+-rw-r--r--  2.0 unx    18933 b- defN 23-Jul-23 02:34 brainpy/_src/math/activations.py
+-rw-r--r--  2.0 unx    29885 b- defN 23-Jul-23 02:34 brainpy/_src/math/compat_numpy.py
+-rw-r--r--  2.0 unx     6487 b- defN 23-Jul-23 02:34 brainpy/_src/math/compat_pytorch.py
+-rw-r--r--  2.0 unx    17887 b- defN 23-Jul-23 02:34 brainpy/_src/math/compat_tensorflow.py
+-rw-r--r--  2.0 unx      911 b- defN 23-Jul-23 02:34 brainpy/_src/math/datatypes.py
+-rw-r--r--  2.0 unx    16139 b- defN 23-Jul-23 02:34 brainpy/_src/math/delayvars.py
+-rw-r--r--  2.0 unx    16825 b- defN 23-Jul-23 02:34 brainpy/_src/math/environment.py
+-rw-r--r--  2.0 unx     1498 b- defN 23-Jul-23 02:34 brainpy/_src/math/fft.py
+-rw-r--r--  2.0 unx     8866 b- defN 23-Jul-23 02:34 brainpy/_src/math/index_tricks.py
+-rw-r--r--  2.0 unx     2523 b- defN 23-Jul-23 02:34 brainpy/_src/math/interoperability.py
+-rw-r--r--  2.0 unx     1792 b- defN 23-Jul-23 02:34 brainpy/_src/math/linalg.py
+-rw-r--r--  2.0 unx     2319 b- defN 23-Jul-23 02:34 brainpy/_src/math/modes.py
+-rw-r--r--  2.0 unx    47487 b- defN 23-Jul-23 02:34 brainpy/_src/math/ndarray.py
+-rw-r--r--  2.0 unx     2255 b- defN 23-Jul-23 02:34 brainpy/_src/math/others.py
+-rw-r--r--  2.0 unx    15894 b- defN 23-Jul-23 02:34 brainpy/_src/math/pre_syn_post.py
+-rw-r--r--  2.0 unx    78579 b- defN 23-Jul-23 02:34 brainpy/_src/math/random.py
+-rw-r--r--  2.0 unx     2088 b- defN 23-Jul-23 02:34 brainpy/_src/math/remove_vmap.py
+-rw-r--r--  2.0 unx     3878 b- defN 23-Jul-23 02:34 brainpy/_src/math/sharding.py
+-rw-r--r--  2.0 unx       61 b- defN 23-Jul-23 02:34 brainpy/_src/math/event/__init__.py
+-rw-r--r--  2.0 unx    17787 b- defN 23-Jul-23 02:34 brainpy/_src/math/event/_csr_matvec.py
+-rw-r--r--  2.0 unx     5062 b- defN 23-Jul-23 02:34 brainpy/_src/math/event/_info_collection.py
+-rw-r--r--  2.0 unx       53 b- defN 23-Jul-23 02:34 brainpy/_src/math/jitconn/__init__.py
+-rw-r--r--  2.0 unx    25027 b- defN 23-Jul-23 02:34 brainpy/_src/math/jitconn/_event_matvec.py
+-rw-r--r--  2.0 unx    28793 b- defN 23-Jul-23 02:34 brainpy/_src/math/jitconn/_matvec.py
+-rw-r--r--  2.0 unx      914 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/__init__.py
+-rw-r--r--  2.0 unx     3615 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/_tools.py
+-rw-r--r--  2.0 unx      758 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/_utils.py
+-rw-r--r--  2.0 unx    40727 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/autograd.py
+-rw-r--r--  2.0 unx    22767 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/base.py
+-rw-r--r--  2.0 unx     5931 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/collectors.py
+-rw-r--r--  2.0 unx    32660 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/controls.py
+-rw-r--r--  2.0 unx     2999 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/function.py
+-rw-r--r--  2.0 unx    15803 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/jit.py
+-rw-r--r--  2.0 unx     1642 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/naming.py
+-rw-r--r--  2.0 unx    17774 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/parallels.py
+-rw-r--r--  2.0 unx    14573 b- defN 23-Jul-23 02:34 brainpy/_src/math/object_transform/variables.py
+-rw-r--r--  2.0 unx      204 b- defN 23-Jul-23 02:34 brainpy/_src/math/op_registers/__init__.py
+-rw-r--r--  2.0 unx     1051 b- defN 23-Jul-23 02:34 brainpy/_src/math/op_registers/utils.py
+-rw-r--r--  2.0 unx     7105 b- defN 23-Jul-23 02:34 brainpy/_src/math/op_registers/numba_approach/__init__.py
+-rw-r--r--  2.0 unx     5129 b- defN 23-Jul-23 02:34 brainpy/_src/math/op_registers/numba_approach/cpu_translation.py
+-rw-r--r--  2.0 unx      142 b- defN 23-Jul-23 02:34 brainpy/_src/math/sparse/__init__.py
+-rw-r--r--  2.0 unx    14576 b- defN 23-Jul-23 02:34 brainpy/_src/math/sparse/_bsr_mm.py
+-rw-r--r--  2.0 unx     8134 b- defN 23-Jul-23 02:34 brainpy/_src/math/sparse/_bsr_mv.py
+-rw-r--r--  2.0 unx     7127 b- defN 23-Jul-23 02:34 brainpy/_src/math/sparse/_coo_mv.py
+-rw-r--r--  2.0 unx    16697 b- defN 23-Jul-23 02:34 brainpy/_src/math/sparse/_csr_mv.py
+-rw-r--r--  2.0 unx     4439 b- defN 23-Jul-23 02:34 brainpy/_src/math/sparse/_jax_prim.py
+-rw-r--r--  2.0 unx     5021 b- defN 23-Jul-23 02:34 brainpy/_src/math/sparse/_utils.py
+-rw-r--r--  2.0 unx       99 b- defN 23-Jul-23 02:34 brainpy/_src/math/surrogate/__init__.py
+-rw-r--r--  2.0 unx     7216 b- defN 23-Jul-23 02:34 brainpy/_src/math/surrogate/_compt.py
+-rw-r--r--  2.0 unx    42623 b- defN 23-Jul-23 02:34 brainpy/_src/math/surrogate/_one_input.py
+-rw-r--r--  2.0 unx     1492 b- defN 23-Jul-23 02:34 brainpy/_src/math/surrogate/_two_inputs.py
+-rw-r--r--  2.0 unx     3665 b- defN 23-Jul-23 02:34 brainpy/_src/math/surrogate/_utils.py
+-rw-r--r--  2.0 unx      243 b- defN 23-Jul-23 02:34 brainpy/_src/math/surrogate/base.py
+-rw-r--r--  2.0 unx      287 b- defN 23-Jul-23 02:34 brainpy/_src/measure/__init__.py
+-rw-r--r--  2.0 unx     9953 b- defN 23-Jul-23 02:34 brainpy/_src/measure/correlation.py
+-rw-r--r--  2.0 unx     1770 b- defN 23-Jul-23 02:34 brainpy/_src/measure/firings.py
+-rw-r--r--  2.0 unx     3896 b- defN 23-Jul-23 02:34 brainpy/_src/measure/lfp.py
+-rw-r--r--  2.0 unx       75 b- defN 23-Jul-23 02:34 brainpy/_src/optimizers/__init__.py
+-rw-r--r--  2.0 unx    41331 b- defN 23-Jul-23 02:34 brainpy/_src/optimizers/optimizer.py
+-rw-r--r--  2.0 unx    13240 b- defN 23-Jul-23 02:34 brainpy/_src/optimizers/scheduler.py
+-rw-r--r--  2.0 unx      525 b- defN 23-Jul-23 02:34 brainpy/_src/running/__init__.py
+-rw-r--r--  2.0 unx      269 b- defN 23-Jul-23 02:34 brainpy/_src/running/constants.py
+-rw-r--r--  2.0 unx     4982 b- defN 23-Jul-23 02:34 brainpy/_src/running/jax_multiprocessing.py
+-rw-r--r--  2.0 unx     2976 b- defN 23-Jul-23 02:34 brainpy/_src/running/native_multiprocessing.py
+-rw-r--r--  2.0 unx     6995 b- defN 23-Jul-23 02:34 brainpy/_src/running/pathos_multiprocessing.py
+-rw-r--r--  2.0 unx     9674 b- defN 23-Jul-23 02:34 brainpy/_src/running/runner.py
+-rw-r--r--  2.0 unx      182 b- defN 23-Jul-23 02:34 brainpy/_src/tools/__init__.py
+-rw-r--r--  2.0 unx     7448 b- defN 23-Jul-23 02:34 brainpy/_src/tools/codes.py
+-rw-r--r--  2.0 unx     4565 b- defN 23-Jul-23 02:34 brainpy/_src/tools/dicts.py
+-rw-r--r--  2.0 unx     1015 b- defN 23-Jul-23 02:34 brainpy/_src/tools/install.py
+-rw-r--r--  2.0 unx      223 b- defN 23-Jul-23 02:34 brainpy/_src/tools/math_util.py
+-rw-r--r--  2.0 unx     4425 b- defN 23-Jul-23 02:34 brainpy/_src/tools/others.py
+-rw-r--r--  2.0 unx     1407 b- defN 23-Jul-23 02:34 brainpy/_src/tools/package.py
+-rw-r--r--  2.0 unx      614 b- defN 23-Jul-23 02:34 brainpy/_src/train/__init__.py
+-rw-r--r--  2.0 unx     1970 b- defN 23-Jul-23 02:34 brainpy/_src/train/_utils.py
+-rw-r--r--  2.0 unx    22357 b- defN 23-Jul-23 02:34 brainpy/_src/train/back_propagation.py
+-rw-r--r--  2.0 unx     2843 b- defN 23-Jul-23 02:34 brainpy/_src/train/base.py
+-rw-r--r--  2.0 unx     9346 b- defN 23-Jul-23 02:34 brainpy/_src/train/offline.py
+-rw-r--r--  2.0 unx     9705 b- defN 23-Jul-23 02:34 brainpy/_src/train/online.py
+-rw-r--r--  2.0 unx       77 b- defN 23-Jul-23 02:34 brainpy/_src/visualization/__init__.py
+-rw-r--r--  2.0 unx     3544 b- defN 23-Jul-23 02:34 brainpy/_src/visualization/base.py
+-rw-r--r--  2.0 unx      841 b- defN 23-Jul-23 02:34 brainpy/_src/visualization/figures.py
+-rw-r--r--  2.0 unx    14603 b- defN 23-Jul-23 02:34 brainpy/_src/visualization/plots.py
+-rw-r--r--  2.0 unx     1026 b- defN 23-Jul-23 02:34 brainpy/_src/visualization/styles.py
+-rw-r--r--  2.0 unx       90 b- defN 23-Jul-23 02:34 brainpy/algorithms/__init__.py
+-rw-r--r--  2.0 unx    17344 b- defN 23-Jul-23 02:34 brainpy/algorithms/offline.py
+-rw-r--r--  2.0 unx     6284 b- defN 23-Jul-23 02:34 brainpy/algorithms/online.py
+-rw-r--r--  2.0 unx     2656 b- defN 23-Jul-23 02:34 brainpy/algorithms/utils.py
+-rw-r--r--  2.0 unx      199 b- defN 23-Jul-23 02:34 brainpy/dnn/__init__.py
+-rw-r--r--  2.0 unx      471 b- defN 23-Jul-23 02:34 brainpy/dnn/activations.py
+-rw-r--r--  2.0 unx      273 b- defN 23-Jul-23 02:34 brainpy/dnn/conv.py
+-rw-r--r--  2.0 unx       91 b- defN 23-Jul-23 02:34 brainpy/dnn/interoperation.py
+-rw-r--r--  2.0 unx      523 b- defN 23-Jul-23 02:34 brainpy/dnn/linear.py
+-rw-r--r--  2.0 unx      313 b- defN 23-Jul-23 02:34 brainpy/dnn/normalization.py
+-rw-r--r--  2.0 unx      202 b- defN 23-Jul-23 02:34 brainpy/dnn/others.py
+-rw-r--r--  2.0 unx      519 b- defN 23-Jul-23 02:34 brainpy/dnn/pooling.py
+-rw-r--r--  2.0 unx      344 b- defN 23-Jul-23 02:34 brainpy/dnn/recurrent.py
+-rw-r--r--  2.0 unx      231 b- defN 23-Jul-23 02:34 brainpy/dyn/__init__.py
+-rw-r--r--  2.0 unx       72 b- defN 23-Jul-23 02:34 brainpy/dyn/base.py
+-rw-r--r--  2.0 unx     1519 b- defN 23-Jul-23 02:34 brainpy/dyn/channels.py
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-23 02:34 brainpy/dyn/compat.py
+-rw-r--r--  2.0 unx      566 b- defN 23-Jul-23 02:34 brainpy/dyn/ions.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-23 02:34 brainpy/dyn/neurons.py
+-rw-r--r--  2.0 unx      341 b- defN 23-Jul-23 02:34 brainpy/dyn/others.py
+-rw-r--r--  2.0 unx      126 b- defN 23-Jul-23 02:34 brainpy/dyn/outs.py
+-rw-r--r--  2.0 unx      342 b- defN 23-Jul-23 02:34 brainpy/dyn/projections.py
+-rw-r--r--  2.0 unx      138 b- defN 23-Jul-23 02:34 brainpy/dyn/rates.py
+-rw-r--r--  2.0 unx      310 b- defN 23-Jul-23 02:34 brainpy/dyn/synapses.py
+-rw-r--r--  2.0 unx      128 b- defN 23-Jul-23 02:34 brainpy/integrators/__init__.py
+-rw-r--r--  2.0 unx      576 b- defN 23-Jul-23 02:34 brainpy/integrators/fde.py
+-rw-r--r--  2.0 unx     1061 b- defN 23-Jul-23 02:34 brainpy/integrators/ode.py
+-rw-r--r--  2.0 unx      679 b- defN 23-Jul-23 02:34 brainpy/integrators/sde.py
+-rw-r--r--  2.0 unx     4768 b- defN 23-Jul-23 02:34 brainpy/math/__init__.py
+-rw-r--r--  2.0 unx      780 b- defN 23-Jul-23 02:34 brainpy/math/activations.py
+-rw-r--r--  2.0 unx     8074 b- defN 23-Jul-23 02:34 brainpy/math/compat_numpy.py
+-rw-r--r--  2.0 unx      469 b- defN 23-Jul-23 02:34 brainpy/math/compat_pytorch.py
+-rw-r--r--  2.0 unx      932 b- defN 23-Jul-23 02:34 brainpy/math/compat_tensorflow.py
+-rw-r--r--  2.0 unx      498 b- defN 23-Jul-23 02:34 brainpy/math/datatypes.py
+-rw-r--r--  2.0 unx      255 b- defN 23-Jul-23 02:34 brainpy/math/delayvars.py
+-rw-r--r--  2.0 unx      943 b- defN 23-Jul-23 02:34 brainpy/math/environment.py
+-rw-r--r--  2.0 unx       75 b- defN 23-Jul-23 02:34 brainpy/math/event.py
+-rw-r--r--  2.0 unx      402 b- defN 23-Jul-23 02:34 brainpy/math/fft.py
+-rw-r--r--  2.0 unx      217 b- defN 23-Jul-23 02:34 brainpy/math/interoperability.py
+-rw-r--r--  2.0 unx      292 b- defN 23-Jul-23 02:34 brainpy/math/jitconn.py
+-rw-r--r--  2.0 unx      470 b- defN 23-Jul-23 02:34 brainpy/math/linalg.py
+-rw-r--r--  2.0 unx      292 b- defN 23-Jul-23 02:34 brainpy/math/modes.py
+-rw-r--r--  2.0 unx      150 b- defN 23-Jul-23 02:34 brainpy/math/ndarray.py
+-rw-r--r--  2.0 unx     1225 b- defN 23-Jul-23 02:34 brainpy/math/object_base.py
+-rw-r--r--  2.0 unx      651 b- defN 23-Jul-23 02:34 brainpy/math/object_transform.py
+-rw-r--r--  2.0 unx      127 b- defN 23-Jul-23 02:34 brainpy/math/op_register.py
+-rw-r--r--  2.0 unx      257 b- defN 23-Jul-23 02:34 brainpy/math/others.py
+-rw-r--r--  2.0 unx      331 b- defN 23-Jul-23 02:34 brainpy/math/pre_syn_post.py
+-rw-r--r--  2.0 unx     1770 b- defN 23-Jul-23 02:34 brainpy/math/random.py
+-rw-r--r--  2.0 unx      213 b- defN 23-Jul-23 02:34 brainpy/math/sharding.py
+-rw-r--r--  2.0 unx      164 b- defN 23-Jul-23 02:34 brainpy/math/sparse.py
+-rw-r--r--  2.0 unx     1249 b- defN 23-Jul-23 02:34 brainpy/math/surrogate.py
+-rw-r--r--  2.0 unx    35100 b- defN 23-Jul-23 02:34 brainpy-2.4.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx     4369 b- defN 23-Jul-23 02:34 brainpy-2.4.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-23 02:34 brainpy-2.4.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 23-Jul-23 02:34 brainpy-2.4.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    29702 b- defN 23-Jul-23 02:34 brainpy-2.4.3.dist-info/RECORD
+333 files, 2663343 bytes uncompressed, 605367 bytes compressed:  77.3%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: brainpy/__init__.py
 Comment: 
 
+Filename: brainpy/_add_deprecations.py
+Comment: 
+
 Filename: brainpy/analysis.py
 Comment: 
 
 Filename: brainpy/channels.py
 Comment: 
 
 Filename: brainpy/check.py
@@ -12,17 +15,14 @@
 
 Filename: brainpy/checkpoints.py
 Comment: 
 
 Filename: brainpy/connect.py
 Comment: 
 
-Filename: brainpy/dnn.py
-Comment: 
-
 Filename: brainpy/encoding.py
 Comment: 
 
 Filename: brainpy/errors.py
 Comment: 
 
 Filename: brainpy/experimental.py
@@ -54,15 +54,21 @@
 
 Filename: brainpy/rates.py
 Comment: 
 
 Filename: brainpy/running.py
 Comment: 
 
-Filename: brainpy/testing.py
+Filename: brainpy/synapses.py
+Comment: 
+
+Filename: brainpy/synouts.py
+Comment: 
+
+Filename: brainpy/synplast.py
 Comment: 
 
 Filename: brainpy/tools.py
 Comment: 
 
 Filename: brainpy/types.py
 Comment: 
@@ -243,66 +249,195 @@
 
 Filename: brainpy/_src/dyn/_docs.py
 Comment: 
 
 Filename: brainpy/_src/dyn/base.py
 Comment: 
 
-Filename: brainpy/_src/dyn/projections.py
+Filename: brainpy/_src/dyn/utils.py
 Comment: 
 
-Filename: brainpy/_src/dyn/channels/Ca.py
+Filename: brainpy/_src/dyn/channels/__init__.py
 Comment: 
 
-Filename: brainpy/_src/dyn/channels/IH.py
+Filename: brainpy/_src/dyn/channels/base.py
 Comment: 
 
-Filename: brainpy/_src/dyn/channels/K.py
+Filename: brainpy/_src/dyn/channels/calcium.py
 Comment: 
 
-Filename: brainpy/_src/dyn/channels/KCa.py
+Filename: brainpy/_src/dyn/channels/hyperpolarization_activated.py
 Comment: 
 
-Filename: brainpy/_src/dyn/channels/Na.py
+Filename: brainpy/_src/dyn/channels/leaky.py
 Comment: 
 
-Filename: brainpy/_src/dyn/channels/__init__.py
+Filename: brainpy/_src/dyn/channels/potassium.py
 Comment: 
 
-Filename: brainpy/_src/dyn/channels/base.py
+Filename: brainpy/_src/dyn/channels/potassium_calcium.py
 Comment: 
 
-Filename: brainpy/_src/dyn/channels/leaky.py
+Filename: brainpy/_src/dyn/channels/potassium_calcium_compatible.py
+Comment: 
+
+Filename: brainpy/_src/dyn/channels/potassium_compatible.py
+Comment: 
+
+Filename: brainpy/_src/dyn/channels/sodium.py
+Comment: 
+
+Filename: brainpy/_src/dyn/channels/sodium_compatible.py
+Comment: 
+
+Filename: brainpy/_src/dyn/ions/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dyn/ions/base.py
+Comment: 
+
+Filename: brainpy/_src/dyn/ions/calcium.py
+Comment: 
+
+Filename: brainpy/_src/dyn/ions/potassium.py
+Comment: 
+
+Filename: brainpy/_src/dyn/ions/sodium.py
 Comment: 
 
 Filename: brainpy/_src/dyn/neurons/__init__.py
 Comment: 
 
-Filename: brainpy/_src/dyn/neurons/hh.py
+Filename: brainpy/_src/dyn/neurons/base.py
 Comment: 
 
-Filename: brainpy/_src/dyn/neurons/input.py
+Filename: brainpy/_src/dyn/neurons/hh.py
 Comment: 
 
 Filename: brainpy/_src/dyn/neurons/lif.py
 Comment: 
 
 Filename: brainpy/_src/dyn/others/__init__.py
 Comment: 
 
 Filename: brainpy/_src/dyn/others/common.py
 Comment: 
 
+Filename: brainpy/_src/dyn/others/input.py
+Comment: 
+
+Filename: brainpy/_src/dyn/others/noise.py
+Comment: 
+
+Filename: brainpy/_src/dyn/outs/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dyn/outs/base.py
+Comment: 
+
+Filename: brainpy/_src/dyn/outs/outputs.py
+Comment: 
+
+Filename: brainpy/_src/dyn/projections/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dyn/projections/aligns.py
+Comment: 
+
+Filename: brainpy/_src/dyn/projections/conn.py
+Comment: 
+
+Filename: brainpy/_src/dyn/projections/others.py
+Comment: 
+
+Filename: brainpy/_src/dyn/rates/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dyn/rates/populations.py
+Comment: 
+
 Filename: brainpy/_src/dyn/synapses/__init__.py
 Comment: 
 
-Filename: brainpy/_src/dyn/synapses/dynamics.py
+Filename: brainpy/_src/dyn/synapses/abstract_models.py
+Comment: 
+
+Filename: brainpy/_src/dyn/synapses/bio_models.py
+Comment: 
+
+Filename: brainpy/_src/dyn/synapses/delay_couplings.py
+Comment: 
+
+Filename: brainpy/_src/dynold/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dynold/experimental/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dynold/experimental/abstract_synapses.py
+Comment: 
+
+Filename: brainpy/_src/dynold/experimental/base.py
+Comment: 
+
+Filename: brainpy/_src/dynold/experimental/others.py
+Comment: 
+
+Filename: brainpy/_src/dynold/experimental/syn_outs.py
+Comment: 
+
+Filename: brainpy/_src/dynold/experimental/syn_plasticity.py
+Comment: 
+
+Filename: brainpy/_src/dynold/neurons/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dynold/neurons/biological_models.py
+Comment: 
+
+Filename: brainpy/_src/dynold/neurons/fractional_models.py
+Comment: 
+
+Filename: brainpy/_src/dynold/neurons/reduced_models.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synapses/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synapses/abstract_models.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synapses/base.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synapses/biological_models.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synapses/compat.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synapses/gap_junction.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synapses/learning_rules.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synouts/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synouts/conductances.py
 Comment: 
 
-Filename: brainpy/_src/dyn/synapses/outputs.py
+Filename: brainpy/_src/dynold/synouts/ions.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synplast/__init__.py
+Comment: 
+
+Filename: brainpy/_src/dynold/synplast/short_term_plasticity.py
 Comment: 
 
 Filename: brainpy/_src/encoding/__init__.py
 Comment: 
 
 Filename: brainpy/_src/encoding/base.py
 Comment: 
@@ -606,50 +741,23 @@
 
 Filename: brainpy/_src/measure/firings.py
 Comment: 
 
 Filename: brainpy/_src/measure/lfp.py
 Comment: 
 
-Filename: brainpy/_src/neurons/__init__.py
-Comment: 
-
-Filename: brainpy/_src/neurons/biological_models.py
-Comment: 
-
-Filename: brainpy/_src/neurons/compat.py
-Comment: 
-
-Filename: brainpy/_src/neurons/fractional_models.py
-Comment: 
-
-Filename: brainpy/_src/neurons/input_groups.py
-Comment: 
-
-Filename: brainpy/_src/neurons/noise_groups.py
-Comment: 
-
-Filename: brainpy/_src/neurons/reduced_models.py
-Comment: 
-
 Filename: brainpy/_src/optimizers/__init__.py
 Comment: 
 
 Filename: brainpy/_src/optimizers/optimizer.py
 Comment: 
 
 Filename: brainpy/_src/optimizers/scheduler.py
 Comment: 
 
-Filename: brainpy/_src/rates/__init__.py
-Comment: 
-
-Filename: brainpy/_src/rates/populations.py
-Comment: 
-
 Filename: brainpy/_src/running/__init__.py
 Comment: 
 
 Filename: brainpy/_src/running/constants.py
 Comment: 
 
 Filename: brainpy/_src/running/jax_multiprocessing.py
@@ -660,156 +768,135 @@
 
 Filename: brainpy/_src/running/pathos_multiprocessing.py
 Comment: 
 
 Filename: brainpy/_src/running/runner.py
 Comment: 
 
-Filename: brainpy/_src/synapses/__init__.py
-Comment: 
-
-Filename: brainpy/_src/synapses/abstract_models.py
-Comment: 
-
-Filename: brainpy/_src/synapses/biological_models.py
-Comment: 
-
-Filename: brainpy/_src/synapses/compat.py
-Comment: 
-
-Filename: brainpy/_src/synapses/delay_couplings.py
-Comment: 
-
-Filename: brainpy/_src/synapses/gap_junction.py
-Comment: 
-
-Filename: brainpy/_src/synapses/learning_rules.py
-Comment: 
-
-Filename: brainpy/_src/synapses_v2/__init__.py
+Filename: brainpy/_src/tools/__init__.py
 Comment: 
 
-Filename: brainpy/_src/synapses_v2/abstract_synapses.py
+Filename: brainpy/_src/tools/codes.py
 Comment: 
 
-Filename: brainpy/_src/synapses_v2/base.py
+Filename: brainpy/_src/tools/dicts.py
 Comment: 
 
-Filename: brainpy/_src/synapses_v2/others.py
+Filename: brainpy/_src/tools/install.py
 Comment: 
 
-Filename: brainpy/_src/synapses_v2/syn_outs.py
+Filename: brainpy/_src/tools/math_util.py
 Comment: 
 
-Filename: brainpy/_src/synapses_v2/syn_plasticity.py
+Filename: brainpy/_src/tools/others.py
 Comment: 
 
-Filename: brainpy/_src/synouts/__init__.py
+Filename: brainpy/_src/tools/package.py
 Comment: 
 
-Filename: brainpy/_src/synouts/conductances.py
+Filename: brainpy/_src/train/__init__.py
 Comment: 
 
-Filename: brainpy/_src/synouts/ions.py
+Filename: brainpy/_src/train/_utils.py
 Comment: 
 
-Filename: brainpy/_src/synplast/__init__.py
+Filename: brainpy/_src/train/back_propagation.py
 Comment: 
 
-Filename: brainpy/_src/synplast/long_term_plasticity.py
+Filename: brainpy/_src/train/base.py
 Comment: 
 
-Filename: brainpy/_src/synplast/short_term_plasticity.py
+Filename: brainpy/_src/train/offline.py
 Comment: 
 
-Filename: brainpy/_src/testing/__init__.py
+Filename: brainpy/_src/train/online.py
 Comment: 
 
-Filename: brainpy/_src/testing/base.py
+Filename: brainpy/_src/visualization/__init__.py
 Comment: 
 
-Filename: brainpy/_src/tools/__init__.py
+Filename: brainpy/_src/visualization/base.py
 Comment: 
 
-Filename: brainpy/_src/tools/codes.py
+Filename: brainpy/_src/visualization/figures.py
 Comment: 
 
-Filename: brainpy/_src/tools/dicts.py
+Filename: brainpy/_src/visualization/plots.py
 Comment: 
 
-Filename: brainpy/_src/tools/install.py
+Filename: brainpy/_src/visualization/styles.py
 Comment: 
 
-Filename: brainpy/_src/tools/math_util.py
+Filename: brainpy/algorithms/__init__.py
 Comment: 
 
-Filename: brainpy/_src/tools/others.py
+Filename: brainpy/algorithms/offline.py
 Comment: 
 
-Filename: brainpy/_src/tools/package.py
+Filename: brainpy/algorithms/online.py
 Comment: 
 
-Filename: brainpy/_src/train/__init__.py
+Filename: brainpy/algorithms/utils.py
 Comment: 
 
-Filename: brainpy/_src/train/_utils.py
+Filename: brainpy/dnn/__init__.py
 Comment: 
 
-Filename: brainpy/_src/train/back_propagation.py
+Filename: brainpy/dnn/activations.py
 Comment: 
 
-Filename: brainpy/_src/train/base.py
+Filename: brainpy/dnn/conv.py
 Comment: 
 
-Filename: brainpy/_src/train/offline.py
+Filename: brainpy/dnn/interoperation.py
 Comment: 
 
-Filename: brainpy/_src/train/online.py
+Filename: brainpy/dnn/linear.py
 Comment: 
 
-Filename: brainpy/_src/visualization/__init__.py
+Filename: brainpy/dnn/normalization.py
 Comment: 
 
-Filename: brainpy/_src/visualization/base.py
+Filename: brainpy/dnn/others.py
 Comment: 
 
-Filename: brainpy/_src/visualization/figures.py
+Filename: brainpy/dnn/pooling.py
 Comment: 
 
-Filename: brainpy/_src/visualization/plots.py
+Filename: brainpy/dnn/recurrent.py
 Comment: 
 
-Filename: brainpy/_src/visualization/styles.py
+Filename: brainpy/dyn/__init__.py
 Comment: 
 
-Filename: brainpy/algorithms/__init__.py
+Filename: brainpy/dyn/base.py
 Comment: 
 
-Filename: brainpy/algorithms/offline.py
+Filename: brainpy/dyn/channels.py
 Comment: 
 
-Filename: brainpy/algorithms/online.py
+Filename: brainpy/dyn/compat.py
 Comment: 
 
-Filename: brainpy/algorithms/utils.py
+Filename: brainpy/dyn/ions.py
 Comment: 
 
-Filename: brainpy/dyn/__init__.py
+Filename: brainpy/dyn/neurons.py
 Comment: 
 
-Filename: brainpy/dyn/channels.py
+Filename: brainpy/dyn/others.py
 Comment: 
 
-Filename: brainpy/dyn/neurons.py
+Filename: brainpy/dyn/outs.py
 Comment: 
 
-Filename: brainpy/dyn/others.py
+Filename: brainpy/dyn/projections.py
 Comment: 
 
-Filename: brainpy/dyn/projections.py
+Filename: brainpy/dyn/rates.py
 Comment: 
 
 Filename: brainpy/dyn/synapses.py
 Comment: 
 
 Filename: brainpy/integrators/__init__.py
 Comment: 
@@ -891,35 +978,23 @@
 
 Filename: brainpy/math/sparse.py
 Comment: 
 
 Filename: brainpy/math/surrogate.py
 Comment: 
 
-Filename: brainpy/synapses/__init__.py
-Comment: 
-
-Filename: brainpy/synapses/dynamics.py
-Comment: 
-
-Filename: brainpy/synapses/synouts.py
-Comment: 
-
-Filename: brainpy/synapses/synplast.py
-Comment: 
-
-Filename: brainpy-2.4.2.dist-info/LICENSE
+Filename: brainpy-2.4.3.dist-info/LICENSE
 Comment: 
 
-Filename: brainpy-2.4.2.dist-info/METADATA
+Filename: brainpy-2.4.3.dist-info/METADATA
 Comment: 
 
-Filename: brainpy-2.4.2.dist-info/WHEEL
+Filename: brainpy-2.4.3.dist-info/WHEEL
 Comment: 
 
-Filename: brainpy-2.4.2.dist-info/top_level.txt
+Filename: brainpy-2.4.3.dist-info/top_level.txt
 Comment: 
 
-Filename: brainpy-2.4.2.dist-info/RECORD
+Filename: brainpy-2.4.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## brainpy/__init__.py

```diff
@@ -1,44 +1,43 @@
 # -*- coding: utf-8 -*-
 
-__version__ = "2.4.2"
+__version__ = "2.4.3"
 
 # fundamental supporting modules
 from brainpy import errors, check, tools
 
 try:
   import jaxlib
-
   del jaxlib
 except ModuleNotFoundError:
   raise ModuleNotFoundError(tools.jaxlib_install_info) from None
 
-#  Part 1: Math Foundation  #
-# ------------------------- #
+#  Part: Math Foundation  #
+# ----------------------- #
 
 # math foundation
 from brainpy import math
 from .math import BrainPyObject
 
-#  Part 2: Toolbox  #
-# ----------------- #
-
+#  Part: Toolbox  #
+# --------------- #
 # modules of toolbox
 from brainpy import (
   connect,  # synaptic connection
   initialize,  # weight initialization
   optim,  # gradient descent optimizers
   losses,  # loss functions
   measure,  # methods for data analysis
   inputs,  # methods for generating input currents
   encoding,  # encoding schema
   checkpoints,  # checkpoints
   check,  # error checking
+  mixin,  # mixin classes
+  algorithms,  # online or offline training algorithms
 )
-from . import algorithms  # online or offline training algorithms
 
 # convenient alias
 conn = connect
 init = initialize
 
 # numerical integrators
 from brainpy import integrators
@@ -46,192 +45,103 @@
 from brainpy._src.integrators.base import (Integrator as Integrator)
 from brainpy._src.integrators.joint_eq import (JointEq as JointEq)
 from brainpy._src.integrators.runner import (IntegratorRunner as IntegratorRunner)
 from brainpy._src.integrators.ode.generic import (odeint as odeint)
 from brainpy._src.integrators.sde.generic import (sdeint as sdeint)
 from brainpy._src.integrators.fde.generic import (fdeint as fdeint)
 
-#  Part 3: Models  #
-# ---------------- #
 
-from brainpy import (
-  channels,  # channel models
-  neurons,  # neuron groups
-  synapses,  # synapses
-  rates,  # rate models
-  experimental,
-
-  dnn, layers,  # deep neural network module
-  dyn,  # dynamics module
-  # delay,  # delay module
-)
-
-from brainpy.synapses import (
-  synouts,  # synaptic output
-  synplast,  # synaptic plasticity
-)
+#  Part: Models  #
+# -------------- #
 
+# base classes
 from brainpy._src.dynsys import (
   DynamicalSystem as DynamicalSystem,
-  Container as Container,
+  DynSysGroup as DynSysGroup,  # collectors
   Sequential as Sequential,
-  Network as Network,
-  NeuGroup as NeuGroup,
-  SynConn as SynConn,
-  SynOut as SynOut,
-  SynSTP as SynSTP,
-  SynLTP as SynLTP,
-  TwoEndConn as TwoEndConn,
-  CondNeuGroup as CondNeuGroup,
-  Channel as Channel
+  Dynamic as Dynamic,  # category
+  Projection as Projection,
+)
+DynamicalSystemNS = DynamicalSystem
+Network = DynSysGroup
+# delays
+from brainpy._src.delay import (
+  VarDelay as VarDelay,
+)
+
+# building blocks
+from brainpy import (
+  dnn, layers,  # module for dnn layers
+  dyn,  # module for modeling dynamics
 )
+NeuGroup = NeuGroupNS = dyn.NeuDyn
 
 # shared parameters
-from brainpy._src.context import share
+from brainpy._src.context import (share as share)
 from brainpy._src.dynsys import not_pass_shared
 
-# running
+
+#  Part: Running  #
+# --------------- #
 from brainpy._src.runners import (DSRunner as DSRunner)
 from brainpy._src.transform import (LoopOverTime as LoopOverTime, )
+from brainpy import (running as running)
 
-# DynamicalSystem base classes
-from brainpy._src.dynsys import (
-  DynamicalSystemNS as DynamicalSystemNS,
-  NeuGroupNS as NeuGroupNS,
-  TwoEndConnNS as TwoEndConnNS,
-)
-from brainpy._src.synapses_v2.base import (SynOutNS as SynOutNS,
-                                           SynSTPNS as SynSTPNS,
-                                           SynConnNS as SynConnNS, )
-
-#  Part 4: Training  #
-# ------------------ #
 
+#  Part: Training  #
+# ---------------- #
 from brainpy._src.train.base import (DSTrainer as DSTrainer, )
 from brainpy._src.train.back_propagation import (BPTT as BPTT,
-                                                 BPFF as BPFF, )
+                                                 BPFF as BPFF,)
 from brainpy._src.train.online import (OnlineTrainer as OnlineTrainer,
                                        ForceTrainer as ForceTrainer, )
 from brainpy._src.train.offline import (OfflineTrainer as OfflineTrainer,
                                         RidgeTrainer as RidgeTrainer, )
 
-#  Part 6: Others    #
-# ------------------ #
 
-from brainpy import running, testing, analysis
+#  Part: Analysis  #
+# ---------------- #
+from brainpy import (analysis as analysis)
+
+
+#  Part: Others    #
+# ---------------- #
 from brainpy._src.visualization import (visualize as visualize)
-from brainpy._src import base, train
 
-#  Part 7: Deprecations  #
-# ---------------------- #
 
+#  Part: Deprecations  #
+# -------------------- #
+from brainpy._src import base, train
+from brainpy import (
+  channels,  # channel models
+  neurons,  # neuron groups
+  synapses,  # synapses
+  rates,  # rate models
+  experimental,
+  synouts,  # synaptic output
+  synplast,  # synaptic plasticity
+)
 from brainpy._src import modes
 from brainpy._src.math.object_transform.base import (Base as Base,
-                                                     ArrayCollector,
+                                                     ArrayCollector as ArrayCollector,
                                                      Collector as Collector, )
 
 # deprecated
-from brainpy._src import checking
-from brainpy._src.synapses import compat
-from brainpy._src.deprecations import deprecation_getattr2
+from brainpy._add_deprecations import deprecation_getattr2
 
 __deprecations = {
+  'Module': ('brainpy.Module', 'brainpy.DynamicalSystem', DynamicalSystem),
+  'Channel': ('brainpy.Channel', 'brainpy.dyn.IonChannel', dyn.IonChannel),
+  'SynConn': ('brainpy.SynConn', 'brainpy.dyn.SynConn', dyn.SynConn),
+  'Container': ('brainpy.Container', 'brainpy.DynSysGroup', DynSysGroup),
+
   'optimizers': ('brainpy.optimizers', 'brainpy.optim', optim),
   'TensorCollector': ('brainpy.TensorCollector', 'brainpy.ArrayCollector', ArrayCollector),
+  'SynSTP': ('brainpy.SynSTP', 'brainpy.synapses.SynSTP', synapses.SynSTP),
+  'SynOut': ('brainpy.SynOut', 'brainpy.synapses.SynOut', synapses.SynOut),
+  'TwoEndConn': ('brainpy.TwoEndConn', 'brainpy.synapses.TwoEndConn', synapses.TwoEndConn),
+  'CondNeuGroup': ('brainpy.CondNeuGroup', 'brainpy.syn.CondNeuGroup', dyn.CondNeuGroup),
 }
 __getattr__ = deprecation_getattr2('brainpy', __deprecations)
 
-tools.__deprecations = {
-  'clear_name_cache': ('brainpy.tools.clear_name_cache', 'brainpy.math.clear_name_cache', math.clear_name_cache),
-  'checking': ('brainpy.tools.checking', 'brainpy.checking', checking),
-}
-tools.__getattr__ = deprecation_getattr2('brainpy.tools', tools.__deprecations)
-
-integrators.__deprecations = {
-  'Integrator': ('brainpy.integrators.Integrator', 'brainpy.Integrator', Integrator),
-  'odeint': ('brainpy.integrators.odeint', 'brainpy.odeint', odeint),
-  'sdeint': ('brainpy.integrators.sdeint', 'brainpy.sdeint', sdeint),
-  'fdeint': ('brainpy.integrators.fdeint', 'brainpy.fdeint', fdeint),
-  'IntegratorRunner': ('brainpy.integrators.IntegratorRunner', 'brainpy.IntegratorRunner', IntegratorRunner),
-  'JointEq': ('brainpy.integrators.JointEq', 'brainpy.JointEq', JointEq),
-}
-integrators.__getattr__ = deprecation_getattr2('brainpy.integrators', integrators.__deprecations)
-
-train.__deprecations = {
-  'DSTrainer': ('brainpy.train.DSTrainer', 'brainpy.DSTrainer', DSTrainer),
-  'BPTT': ('brainpy.train.BPTT', 'brainpy.BPTT', BPTT),
-  'BPFF': ('brainpy.train.BPFF', 'brainpy.BPFF', BPFF),
-  'OnlineTrainer': ('brainpy.train.OnlineTrainer', 'brainpy.OnlineTrainer', OnlineTrainer),
-  'ForceTrainer': ('brainpy.train.ForceTrainer', 'brainpy.ForceTrainer', ForceTrainer),
-  'OfflineTrainer': ('brainpy.train.OfflineTrainer', 'brainpy.OfflineTrainer', OfflineTrainer),
-  'RidgeTrainer': ('brainpy.train.RidgeTrainer', 'brainpy.RidgeTrainer', RidgeTrainer),
-}
-train.__getattr__ = deprecation_getattr2('brainpy.train', train.__deprecations)
-
-ode.__deprecations = {'odeint': ('brainpy.ode.odeint', 'brainpy.odeint', odeint)}
-ode.__getattr__ = deprecation_getattr2('brainpy.ode', ode.__deprecations)
-
-sde.__deprecations = {'sdeint': ('brainpy.sde.sdeint', 'brainpy.sdeint', sdeint)}
-sde.__getattr__ = deprecation_getattr2('brainpy.sde', sde.__deprecations)
-
-fde.__deprecations = {'fdeint': ('brainpy.fde.fdeint', 'brainpy.fdeint', fdeint)}
-fde.__getattr__ = deprecation_getattr2('brainpy.fde', sde.__deprecations)
-
-dyn.__deprecations = {
-  # module
-  # 'channels': ('brainpy.dyn.channels', 'brainpy.channels', channels),
-  # 'neurons': ('brainpy.dyn.neurons', 'brainpy.neurons', neurons),
-  'rates': ('brainpy.dyn.rates', 'brainpy.rates', rates),
-  # 'synapses': ('brainpy.dyn.synapses', 'brainpy.synapses', synapses),
-  'synouts': ('brainpy.dyn.synouts', 'brainpy.synapses', synouts),
-  'synplast': ('brainpy.dyn.synplast', 'brainpy.synapses', synplast),
-
-  # models
-  'DynamicalSystem': ('brainpy.dyn.DynamicalSystem', 'brainpy.DynamicalSystem', DynamicalSystem),
-  'Container': ('brainpy.dyn.Container', 'brainpy.Container', Container),
-  'Sequential': ('brainpy.dyn.Sequential', 'brainpy.Sequential', Sequential),
-  'Network': ('brainpy.dyn.Network', 'brainpy.Network', Network),
-  'NeuGroup': ('brainpy.dyn.NeuGroup', 'brainpy.NeuGroup', NeuGroup),
-  'SynConn': ('brainpy.dyn.SynConn', 'brainpy.SynConn', SynConn),
-  # 'SynOut': ('brainpy.dyn.SynOut', 'brainpy.SynOut', SynOut),
-  'SynLTP': ('brainpy.dyn.SynLTP', 'brainpy.SynLTP', SynLTP),
-  'SynSTP': ('brainpy.dyn.SynSTP', 'brainpy.SynSTP', SynSTP),
-  'TwoEndConn': ('brainpy.dyn.TwoEndConn', 'brainpy.TwoEndConn', TwoEndConn),
-  'CondNeuGroup': ('brainpy.dyn.CondNeuGroup', 'brainpy.CondNeuGroup', CondNeuGroup),
-  'Channel': ('brainpy.dyn.Channel', 'brainpy.Channel', Channel),
-  'LoopOverTime': ('brainpy.dyn.LoopOverTime', 'brainpy.LoopOverTime', LoopOverTime),
-  'DSRunner': ('brainpy.dyn.DSRunner', 'brainpy.DSRunner', DSRunner),
-
-  # neurons
-  'HH': ('brainpy.dyn.HH', 'brainpy.neurons.HH', neurons.HH),
-  'MorrisLecar': ('brainpy.dyn.MorrisLecar', 'brainpy.neurons.MorrisLecar', neurons.MorrisLecar),
-  'PinskyRinzelModel': ('brainpy.dyn.PinskyRinzelModel', 'brainpy.neurons.PinskyRinzelModel',
-                        neurons.PinskyRinzelModel),
-  'FractionalFHR': ('brainpy.dyn.FractionalFHR', 'brainpy.neurons.FractionalFHR', neurons.FractionalFHR),
-  'FractionalIzhikevich': ('brainpy.dyn.FractionalIzhikevich', 'brainpy.neurons.FractionalIzhikevich',
-                           neurons.FractionalIzhikevich),
-  'LIF': ('brainpy.dyn.LIF', 'brainpy.neurons.LIF', neurons.LIF),
-  'ExpIF': ('brainpy.dyn.ExpIF', 'brainpy.neurons.ExpIF', neurons.ExpIF),
-  'AdExIF': ('brainpy.dyn.AdExIF', 'brainpy.neurons.AdExIF', neurons.AdExIF),
-  'QuaIF': ('brainpy.dyn.QuaIF', 'brainpy.neurons.QuaIF', neurons.QuaIF),
-  'AdQuaIF': ('brainpy.dyn.AdQuaIF', 'brainpy.neurons.AdQuaIF', neurons.AdQuaIF),
-  'GIF': ('brainpy.dyn.GIF', 'brainpy.neurons.GIF', neurons.GIF),
-  'Izhikevich': ('brainpy.dyn.Izhikevich', 'brainpy.neurons.Izhikevich', neurons.Izhikevich),
-  'HindmarshRose': ('brainpy.dyn.HindmarshRose', 'brainpy.neurons.HindmarshRose', neurons.HindmarshRose),
-  'FHN': ('brainpy.dyn.FHN', 'brainpy.neurons.FHN', neurons.FHN),
-  'SpikeTimeGroup': ('brainpy.dyn.SpikeTimeGroup', 'brainpy.neurons.SpikeTimeGroup', neurons.SpikeTimeGroup),
-  'PoissonGroup': ('brainpy.dyn.PoissonGroup', 'brainpy.neurons.PoissonGroup', neurons.PoissonGroup),
-  'OUProcess': ('brainpy.dyn.OUProcess', 'brainpy.neurons.OUProcess', neurons.OUProcess),
-
-  # synapses
-  'DeltaSynapse': ('brainpy.dyn.DeltaSynapse', 'brainpy.synapses.Delta', compat.DeltaSynapse),
-  'ExpCUBA': ('brainpy.dyn.ExpCUBA', 'brainpy.synapses.Exponential', compat.ExpCUBA),
-  'ExpCOBA': ('brainpy.dyn.ExpCOBA', 'brainpy.synapses.Exponential', compat.ExpCOBA),
-  'DualExpCUBA': ('brainpy.dyn.DualExpCUBA', 'brainpy.synapses.DualExponential', compat.DualExpCUBA),
-  'DualExpCOBA': ('brainpy.dyn.DualExpCOBA', 'brainpy.synapses.DualExponential', compat.DualExpCOBA),
-  'AlphaCUBA': ('brainpy.dyn.AlphaCUBA', 'brainpy.synapses.Alpha', compat.AlphaCUBA),
-  'AlphaCOBA': ('brainpy.dyn.AlphaCOBA', 'brainpy.synapses.Alpha', compat.AlphaCOBA),
-  # 'NMDA': ('brainpy.dyn.NMDA', 'brainpy.synapses.NMDA', compat.NMDA),
-}
-dyn.__getattr__ = deprecation_getattr2('brainpy.dyn', dyn.__deprecations)
+del deprecation_getattr2
 
-del deprecation_getattr2, checking, compat
```

## brainpy/channels.py

```diff
@@ -1,58 +1,4 @@
 # -*- coding: utf-8 -*-
 
-from brainpy._src.dyn.channels.base import (
-  Ion as Ion,
-  IonChannel as IonChannel,
-  Calcium as Calcium,
-  IhChannel as IhChannel,
-  CalciumChannel as CalciumChannel,
-  SodiumChannel as SodiumChannel,
-  PotassiumChannel as PotassiumChannel,
-  LeakyChannel as LeakyChannel,
-)
-
-from brainpy._src.dyn.channels.Ca import (
-  CalciumFixed as CalciumFixed,
-  CalciumDyna as CalciumDyna,
-  CalciumDetailed as CalciumDetailed,
-  CalciumFirstOrder as CalciumFirstOrder,
-  ICaN_IS2008 as ICaN_IS2008,
-  ICaT_HM1992 as ICaT_HM1992,
-  ICaT_HP1992 as ICaT_HP1992,
-  ICaHT_HM1992 as ICaHT_HM1992,
-  ICaL_IS2008 as ICaL_IS2008,
-)
-
-from brainpy._src.dyn.channels.IH import (
-  Ih_HM1992 as Ih_HM1992,
-  Ih_De1996 as Ih_De1996,
-)
-
-from brainpy._src.dyn.channels.K import (
-  IKDR_Ba2002 as IKDR_Ba2002,
-  IK_TM1991 as IK_TM1991,
-  IK_HH1952 as IK_HH1952,
-  IKA1_HM1992 as IKA1_HM1992,
-  IKA2_HM1992 as IKA2_HM1992,
-  IKK2A_HM1992 as IKK2A_HM1992,
-  IKK2B_HM1992 as IKK2B_HM1992,
-  IKNI_Ya1989 as IKNI_Ya1989,
-)
-
-from brainpy._src.dyn.channels.KCa import (
-  IAHP_De1994 as IAHP_De1994,
-)
-
-from brainpy._src.dyn.channels.leaky import (
-  IL as IL,
-  IKL as IKL,
-)
-
-from brainpy._src.dyn.channels.Na import (
-  INa_Ba2002 as INa_Ba2002,
-  INa_TM1991 as INa_TM1991,
-  INa_HH1952 as INa_HH1952,
-)
-
-
-
+from .dyn.channels import *
+from .dyn.ions import *
```

## brainpy/check.py

```diff
@@ -503,23 +503,19 @@
   instance: Any
     The instance in the inheritance hierarchy tree.
   supported_types: type, list of type, tuple of type
     All types that are supported.
   name: str
     The checking target name.
   """
-  if isinstance(supported_types, type):
-    supported_types = (supported_types,)
-  if not isinstance(supported_types, (tuple, list)):
-    raise TypeError(f'supported_types must be a tuple/list of type. But wwe got {type(supported_types)}')
-  for smode in supported_types:
-    assert isinstance(smode, type), f'supported_types must be a tuple/list of type. But wwe got {smode}'
+  if not name:
+    name = 'We'
   if not isinstance(instance, supported_types):
-    raise NotImplementedError(f"{name} does not support {instance}. We only support "
-                              f"{', '.join([mode.__name__ for mode in supported_types])}. ")
+    raise NotImplementedError(f"{name} expect to get an instance of {supported_types}."
+                              f"But we got {type(instance)}. ")
   return instance
 
 
 def is_elem_or_seq_or_dict(targets: Any,
                            elem_type: Union[type, Tuple[type, ...]],
                            out_as: str = 'tuple'):
   assert out_as in ['tuple', 'list', 'dict', None], 'Only support to output as tuple/list/dict/None'
```

## brainpy/errors.py

```diff
@@ -228,7 +228,13 @@
     super(GPUOperatorNotFound, self).__init__(f'''
 GPU operator for "{name}" does not found. 
 
 Please install brainpylib GPU operators with linux + CUDA environment.
     ''')
 
 
+
+
+class SharedArgError(BrainPyError):
+  pass
+
+
```

## brainpy/experimental.py

```diff
@@ -1,18 +1,18 @@
 
-from brainpy._src.synapses_v2.syn_plasticity import (
+from brainpy._src.dynold.experimental.syn_plasticity import (
   STD as STD,
   STP as STP,
 )
-from brainpy._src.synapses_v2.syn_outs import (
+from brainpy._src.dynold.experimental.syn_outs import (
   CUBA as CUBA,
   COBA as COBA,
 )
-from brainpy._src.synapses_v2.abstract_synapses import (
+from brainpy._src.dynold.experimental.abstract_synapses import (
   Exponential,
   DualExponential,
   Alpha,
 )
-from brainpy._src.synapses_v2.others import (
+from brainpy._src.dynold.experimental.others import (
   PoissonInput,
 )
```

## brainpy/mixin.py

```diff
@@ -0,0 +1,17 @@
+00000000: 0a66 726f 6d20 6272 6169 6e70 792e 5f73  .from brainpy._s
+00000010: 7263 2e6d 6978 696e 2069 6d70 6f72 7420  rc.mixin import 
+00000020: 280a 2020 4d69 7849 6e20 6173 204d 6978  (.  MixIn as Mix
+00000030: 496e 2c0a 2020 416c 6967 6e50 6f73 7420  In,.  AlignPost 
+00000040: 6173 2041 6c69 676e 506f 7374 2c0a 2020  as AlignPost,.  
+00000050: 4175 746f 4465 6c61 7953 7570 7020 6173  AutoDelaySupp as
+00000060: 2041 7574 6f44 656c 6179 5375 7070 2c0a   AutoDelaySupp,.
+00000070: 2020 5061 7261 6d44 6573 6320 6173 2050    ParamDesc as P
+00000080: 6172 616d 4465 7363 2c0a 2020 5061 7261  aramDesc,.  Para
+00000090: 6d44 6573 6349 6e69 7420 6173 2050 6172  mDescInit as Par
+000000a0: 616d 4465 7363 496e 6974 2c0a 2020 4e6f  amDescInit,.  No
+000000b0: 5348 2061 7320 4e6f 5348 2c0a 2020 436f  SH as NoSH,.  Co
+000000c0: 6e74 6169 6e65 7220 6173 2043 6f6e 7461  ntainer as Conta
+000000d0: 696e 6572 2c0a 2020 5472 6565 4e6f 6465  iner,.  TreeNode
+000000e0: 2061 7320 5472 6565 4e6f 6465 2c0a 2020   as TreeNode,.  
+000000f0: 4a6f 696e 7454 7970 6520 6173 204a 6f69  JointType as Joi
+00000100: 6e74 5479 7065 2c0a 290a                 ntType,.).
```

## brainpy/neurons.py

```diff
@@ -1,42 +1,38 @@
 # -*- coding: utf-8 -*-
 
-from brainpy._src.neurons.biological_models import (
+from brainpy._src.dynold.neurons.biological_models import (
   HH as HH,
   MorrisLecar as MorrisLecar,
   PinskyRinzelModel as PinskyRinzelModel,
   WangBuzsakiModel as WangBuzsakiModel,
 )
 
-from brainpy._src.neurons.fractional_models import (
+from brainpy._src.dynold.neurons.fractional_models import (
   FractionalNeuron as FractionalNeuron,
   FractionalFHR as FractionalFHR,
   FractionalIzhikevich as FractionalIzhikevich,
 )
 
-from brainpy._src.neurons.input_groups import (
-  InputGroup as InputGroup,
-  OutputGroup as OutputGroup,
-  SpikeTimeGroup as SpikeTimeGroup,
-  PoissonGroup as PoissonGroup,
-)
-
-from brainpy._src.neurons.noise_groups import (
-  OUProcess as OUProcess,
-)
-
-from brainpy._src.neurons.reduced_models import (
-  Leaky as Leaky,
-  Integrator as Integrator,
+from brainpy._src.dynold.neurons.reduced_models import (
   LeakyIntegrator as LeakyIntegrator,
   LIF as LIF,
   ExpIF as ExpIF,
   AdExIF as AdExIF,
   QuaIF as QuaIF,
   AdQuaIF as AdQuaIF,
   GIF as GIF,
   ALIFBellec2020 as ALIFBellec2020,
   Izhikevich as Izhikevich,
   HindmarshRose as HindmarshRose,
   FHN as FHN,
   LIF_SFA_Bellec2020,
 )
+from .dyn.others import (
+  InputGroup as InputGroup,
+  OutputGroup as OutputGroup,
+  SpikeTimeGroup as SpikeTimeGroup,
+  PoissonGroup as PoissonGroup,
+  Leaky as Leaky,
+  Integrator as Integrator,
+  OUProcess as OUProcess, 
+)
```

## brainpy/rates.py

```diff
@@ -1,14 +1,5 @@
 # -*- coding: utf-8 -*-
 
-
-from brainpy._src.rates.populations import (
-  RateModel as RateModel,
-  FHN as FHN,
-  FeedbackFHN as FeedbackFHN,
-  QIF as QIF,
-  StuartLandauOscillator as StuartLandauOscillator,
-  WilsonCowanModel as WilsonCowanModel,
-  ThresholdLinearModel as ThresholdLinearModel,
-)
+from .dyn.rates import *
```

## brainpy/_src/_delay.py

```diff
@@ -7,25 +7,25 @@
 import jax
 import jax.numpy as jnp
 import numpy as np
 from jax.lax import stop_gradient
 
 from brainpy import check
 from brainpy import math as bm
-from brainpy._src.dynsys import DynamicalSystemNS
+from brainpy._src.dynsys import DynamicalSystem
 from brainpy._src.math.delayvars import ROTATE_UPDATE, CONCAT_UPDATE
 from brainpy._src.context import share
 
 
 __all__ = [
   'Delay',
 ]
 
 
-class Delay(DynamicalSystemNS):
+class Delay(DynamicalSystem):
   """Delay variable which has a fixed delay length.
 
   The data in this delay variable is arranged as::
 
        delay = 0             [ data
        delay = 1               data
        delay = 2               data
```

## brainpy/_src/context.py

```diff
@@ -1,26 +1,25 @@
 """
 Context for brainpy computation.
 
 This context defines all shared data used in all modules in a computation.
 """
 
-from typing import Any
-from typing import Union
+from typing import Any, Union
 
-from brainpy._src.dynsys import DynamicalSystemNS
+from brainpy._src.dynsys import DynamicalSystem
 from brainpy._src.math.environment import get_dt
 from brainpy._src.tools.dicts import DotDict
 
 __all__ = [
   'share',
 ]
 
 
-class _ShareContext(DynamicalSystemNS):
+class _ShareContext(DynamicalSystem):
   def __init__(self):
     super().__init__()
 
     # Shared data across all nodes at current time step.
     # -------------
 
     self._arguments = DotDict()
```

## brainpy/_src/delay.py

```diff
@@ -1,37 +1,37 @@
 """
 Delay variable.
 """
+
 import math
 import numbers
-from typing import Union, Callable, Optional, Dict, Sequence
+from typing import Union, Dict, Callable, Optional
 
 import jax
-from functools import partial
 import jax.numpy as jnp
 import numpy as np
-from jax.lax import stop_gradient
 
 from brainpy import check
-from brainpy import math as bm, tools
+from brainpy import math as bm
 from brainpy._src.context import share
-from brainpy._src.initialize import parameter, variable_
-from brainpy._src.dynsys import DynamicalSystemNS
+from brainpy._src.dynsys import DynamicalSystem
+from brainpy._src.initialize import variable_
 from brainpy._src.math.delayvars import ROTATE_UPDATE, CONCAT_UPDATE
 from brainpy._src.mixin import ParamDesc
 from brainpy.check import jit_error
 
 __all__ = [
   'Delay',
-  'VariableDelay',
+  'VarDelay',
   'DataDelay',
+  'DelayAccess',
 ]
 
 
-class Delay(DynamicalSystemNS, ParamDesc):
+class Delay(DynamicalSystem, ParamDesc):
   """Base class for delay variables.
 
   Args:
     time: The delay time.
     init: The initial delay data.
     method: The delay method. Can be ``rotation`` and ``concat``.
     name: The delay name.
@@ -57,17 +57,17 @@
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
   ):
     super().__init__(name=name, mode=mode)
 
     # delay method
     if method is None:
-      if self.mode.is_parent_of(bm.NonBatchingMode):
+      if self.mode.is_one_of(bm.NonBatchingMode, bm.BatchingMode):
         method = ROTATE_UPDATE
-      elif self.mode.is_parent_of(bm.TrainingMode):
+      elif self.mode.is_a(bm.TrainingMode):
         method = CONCAT_UPDATE
       else:
         method = ROTATE_UPDATE
     assert method in [ROTATE_UPDATE, CONCAT_UPDATE]
     self.method = method
 
     # delay length
@@ -125,15 +125,15 @@
     ----------
     delay_step: int, ArrayType
       The delay length used to retrieve the data.
     """
     raise NotImplementedError()
 
 
-class _TargetDelay1(Delay):
+class VariableDelay2(Delay):
   """Delay variable which has a fixed delay length.
 
   The data in this delay variable is arranged as::
 
        delay = 0             [ data
        delay = 1               data
        delay = 2               data
@@ -166,15 +166,14 @@
   not_desc_params = ('time', 'entries')
 
   def __init__(
       self,
 
       # delay target
       target: bm.Variable,
-      sharding: Optional[Sequence[str]] = None,
 
       # delay time
       time: Optional[Union[int, float]] = None,
 
       # delay init
       init: Optional[Union[numbers.Number, bm.Array, jax.Array, Callable]] = None,
 
@@ -194,30 +193,23 @@
     if not isinstance(target, bm.Variable):
       raise ValueError(f'Must be an instance of brainpy.math.Variable. But we got {type(target)}')
 
     if self.mode.is_child_of(bm.BatchingMode):
       assert target.batch_axis is not None
 
     # sharding
-    if sharding is not None:
-      if len(sharding) == target.ndim:
-        sharding = list(sharding)
-      elif len(sharding) + 1 == target.ndim and target.batch_axis is not None:
-        sharding = list(sharding)
-        sharding.insert(target.batch_axis, bm.sharding.BATCH_AXIS)
-      else:
-        raise ValueError('sharding axis names do not match the target dimension. ')
-    self._target_axis_names = tuple(sharding)
-    if sharding is not None:
-      sharding = list(sharding)
+    sharding = None
+    if target.axis_names is not None:
+      sharding = list(target.axis_names)
       sharding.insert(0, bm.sharding.TIME_AXIS)
-    self._data_sharding = tuple(sharding)
+      sharding = tuple(sharding)
+    self.axis_names = sharding
 
     # target
-    self.target = bm.sharding.partition(target, self._target_axis_names)
+    self.target = target
 
     # delay data
     self._init = init
     if self.max_length > 0:
       self._init_data(self.max_length)
     else:
       self.data = None
@@ -349,15 +341,15 @@
     assert delay_step is not None
     if check.is_checking():
       jit_error(bm.any(delay_step > self.max_length), self._check_delay, delay_step)
 
     if self.method == ROTATE_UPDATE:
       i = share.load('i')
       delay_idx = (i + delay_step) % (self.max_length + 1)
-      delay_idx = stop_gradient(delay_idx)
+      delay_idx = jax.lax.stop_gradient(delay_idx)
 
     elif self.method == CONCAT_UPDATE:
       delay_idx = delay_step
 
     else:
       raise ValueError(f'Unknown updating method "{self.method}"')
 
@@ -436,16 +428,16 @@
       sharding = list(sharding)
       sharding.insert(0, bm.sharding.BATCH_AXIS)
     else:
       raise ValueError('sharding axis names do not match the target dimension. ')
   return sharding
 
 
-class VariableDelay(Delay):
-  """Delay variable which has a fixed delay length.
+class VarDelay(Delay):
+  """Generate Delays for the given :py:class:`~.Variable` instance.
 
   The data in this delay variable is arranged as::
 
        delay = 0             [ data
        delay = 1               data
        delay = 2               data
        ...                     ....
@@ -522,16 +514,16 @@
     if self.max_length > 0:
       self._init_data(self.max_length)
     else:
       self.data = None
 
     # other info
     if entries is not None:
-      for entry, value in entries.items():
-        self.register_entry(entry, value)
+      for entry, delay_time in entries.items():
+        self.register_entry(entry, delay_time)
 
   def register_entry(
       self,
       entry: str,
       delay_time: Optional[Union[int, float]],
   ) -> 'Delay':
     """Register an entry to access the data.
@@ -577,19 +569,25 @@
       The data.
     """
     assert isinstance(entry, str), 'entry should be a string for describing the '
     if entry not in self._registered_entries:
       raise KeyError(f'Does not find delay entry "{entry}".')
     delay_step = self._registered_entries[entry]
     if delay_step is None or delay_step == 0.:
-      return self.target.value
+      if len(indices):
+        return self.target[indices]
+      else:
+        return self.target.value
     else:
       assert self.data is not None
       if delay_step == 0:
-        return self.target.value
+        if len(indices):
+          return self.target[indices]
+        else:
+          return self.target.value
       else:
         return self.retrieve(delay_step, *indices)
 
   @property
   def delay_target_shape(self):
     """The data shape of the delay target."""
     return self.target.shape
@@ -614,15 +612,15 @@
     assert delay_step is not None
     if check.is_checking():
       jit_error(delay_step > self.max_length, self._check_delay, delay_step)
 
     if self.method == ROTATE_UPDATE:
       i = share.load('i')
       delay_idx = (i + delay_step - 1) % self.max_length
-      delay_idx = stop_gradient(delay_idx)
+      delay_idx = jax.lax.stop_gradient(delay_idx)
 
     elif self.method == CONCAT_UPDATE:
       delay_idx = delay_step
 
     else:
       raise ValueError(f'Unknown updating method "{self.method}"')
 
@@ -650,15 +648,16 @@
         i = share.load('i')
         idx = bm.as_jax((i - 1) % self.max_length)
         self.data[idx] = latest_value
 
       # update the delay data at the first position
       elif self.method == CONCAT_UPDATE:
         if self.max_length > 1:
-          self.data.value = bm.vstack([latest_value, self.data[1:]])
+          latest_value = bm.expand_dims(latest_value, 0)
+          self.data.value = bm.concat([latest_value, self.data[1:]], axis=0)
         else:
           self.data[0] = latest_value
 
   def reset_state(self, batch_size: int = None):
     """Reset the delay data.
     """
     # initialize delay data
@@ -687,24 +686,23 @@
     # update delay data
     if isinstance(self._init, (bm.Array, jax.Array, numbers.Number)):
       self.data[:] = self._init
     elif callable(self._init):
       self.data[:] = self._init((length,) + self.target.shape, dtype=self.target.dtype)
 
 
-class DataDelay(VariableDelay):
-  
+class DataDelay(VarDelay):
   not_desc_params = ('time', 'entries')
 
   def __init__(
       self,
 
       # delay target
-      target: bm.Variable,
-      target_init: Callable,
+      data: bm.Variable,
+      data_init: Union[Callable, bm.Array, jax.Array],
 
       # delay time
       time: Optional[Union[int, float]] = None,
 
       # delay init
       init: Optional[Union[numbers.Number, bm.Array, jax.Array, Callable]] = None,
 
@@ -714,16 +712,16 @@
       # delay method
       method: Optional[str] = None,
 
       # others
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
   ):
-    self.target_init = target_init
-    super().__init__(target=target,
+    self.target_init = data_init
+    super().__init__(target=data,
                      time=time,
                      init=init,
                      entries=entries,
                      method=method,
                      name=name,
                      mode=mode)
 
@@ -738,7 +736,26 @@
       self,
       latest_value: Union[bm.Array, jax.Array]
   ) -> None:
     """Update delay variable with the new data.
     """
     self.target.value = latest_value
     super().update(latest_value)
+
+
+class DelayAccess(DynamicalSystem):
+  def __init__(
+      self,
+      delay: Delay,
+      time: Union[None, int, float],
+      *indices
+  ):
+    super().__init__(mode=delay.mode)
+    self.delay = delay
+    assert isinstance(delay, Delay)
+    delay.register_entry(self.name, time)
+    self.indices = indices
+
+  def update(self):
+    return self.delay.at(self.name, *self.indices)
+
+
```

## brainpy/_src/deprecations.py

```diff
@@ -4,14 +4,48 @@
 __all__ = [
   'deprecated',
   'deprecation_getattr',
   'deprecation_getattr2',
 ]
 
 
+_update_deprecate_msg = '''
+From brainpy>=2.4.3, update() function no longer needs to receive a global shared argument.
+
+Instead of using:
+
+  def update(self, tdi, *args, **kwagrs):
+     t = tdi['t']
+     ...
+
+Please use:
+
+  def update(self, *args, **kwagrs):
+     t = bp.share['t']
+     ...
+'''
+
+
+_input_deprecate_msg = '''
+From brainpy>=2.4.3, input() function no longer needs to receive a global shared argument.
+
+Instead of using:
+
+  def input(tdi):
+     ...
+
+Please use:
+
+  def input():
+     t = bp.share['t']
+     ...
+'''
+
+
+
 def _deprecate(msg):
   warnings.simplefilter('always', DeprecationWarning)  # turn off filter
   warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
   warnings.simplefilter('default', DeprecationWarning)  # reset filter
 
 
 def deprecated(func):
```

## brainpy/_src/dynsys.py

```diff
@@ -1,52 +1,35 @@
 # -*- coding: utf-8 -*-
 
 import collections
 import gc
-from typing import Union, Dict, Callable, Sequence, Optional, Tuple
+import inspect
+import warnings
+from typing import Union, Dict, Callable, Sequence, Optional, Any
 
-import jax
-import jax.numpy as jnp
 import numpy as np
 
-from brainpy import tools
-from brainpy._src import math as bm
-from brainpy._src.connect import TwoEndConnector, MatConn, IJConn, One2One, All2All
-from brainpy._src.initialize import Initializer, parameter, variable, Uniform, noise as init_noise
-from brainpy._src.integrators import odeint, sdeint
-from brainpy._src.math.object_transform.variables import Variable, VariableView
-from brainpy._src.math.object_transform.base import BrainPyObject, Collector
+from brainpy import tools, math as bm
+from brainpy._src.initialize import parameter, variable_
+from brainpy._src.mixin import AutoDelaySupp, Container, DelayRegister, global_delay_data
 from brainpy.errors import NoImplementationError, UnsupportedError
 from brainpy.types import ArrayType, Shape
+from brainpy._src.deprecations import _update_deprecate_msg
 
 share = None
 
 __all__ = [
-  # general class
+  # general
   'DynamicalSystem',
-  'DynamicalSystemNS',
 
   # containers
-  'Container', 'Network', 'Sequential', 'System',
+  'DynSysGroup', 'Network', 'Sequential',
 
-  # channel models
-  'Channel',
-
-  # neuron models
-  'NeuGroup', 'CondNeuGroup', 'NeuGroupNS',
-
-  # synapse models
-  'SynConn',
-  'TwoEndConn',
-  'SynOut', 'NullSynOut',
-  'SynSTP',
-  'SynLTP',
-
-  # slice
-  'DSView', 'NeuGroupView',
+  # category
+  'Dynamic', 'Projection',
 ]
 
 SLICE_VARS = 'slice_vars'
 
 
 def not_pass_shared(func: Callable):
   """Label the update function as the one without passing shared arguments.
@@ -84,49 +67,45 @@
   func: Callable
     The wrapped function for the class.
   """
   func._new_style = True
   return func
 
 
-class DynamicalSystem(BrainPyObject):
+class DynamicalSystem(bm.BrainPyObject, DelayRegister):
   """Base Dynamical System class.
 
   .. note::
      In general, every instance of :py:class:`~.DynamicalSystem` implemented in
      BrainPy only defines the evolving function at each time step :math:`t`.
 
-     Each subclass of :py:class:`~.DynamicalSystem` may have multiple step functions.
-     For instance, all our implemented neuron model define two step functions:
-
-     - ``.update()`` for the logic updating
-     - ``clear_input()`` for clear all accumulated inputs at this time step.
-
      If users want to define the logic of running models across multiple steps,
      we recommend users to use :py:func:`~.for_loop`, :py:class:`~.LoopOverTime`,
      :py:class:`~.DSRunner`, or :py:class:`~.DSTrainer`.
 
+     To be compatible with previous APIs, :py:class:`~.DynamicalSystem` inherits
+     from the :py:class:`~.DelayRegister`. It's worthy to note that the methods of
+     :py:class:`~.DelayRegister` will be removed in the future, including:
+
+     - ``.register_delay()``
+     - ``.get_delay_data()``
+     - ``.update_local_delays()``
+     - ``.reset_local_delays()``
+
   Parameters
   ----------
   name : optional, str
     The name of the dynamical system.
   mode: optional, Mode
     The model computation mode. It should be instance of :py:class:`~.Mode`.
   """
 
   supported_modes: Optional[Sequence[bm.Mode]] = None
   '''Supported computing modes.'''
 
-  _pass_shared_args: bool = True
-
-  global_delay_data: Dict[str, Tuple[Union[bm.LengthDelay, None], Variable]] = dict()
-  '''Global delay data, which stores the delay variables and corresponding delay targets. 
-  This variable is useful when the same target variable is used in multiple mappings, 
-  as it can reduce the duplicate delay variable registration.'''
-
   def __init__(
       self,
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
   ):
     # mode setting
     mode = bm.get_mode() if mode is None else mode
@@ -137,187 +116,26 @@
 
     if self.supported_modes is not None:
       if not self.mode.is_parent_of(*self.supported_modes):
         raise UnsupportedError(f'The mode only supports computing modes '
                                f'which are parents of {self.supported_modes}, '
                                f'but we got {self.mode}.')
 
-    # local delay variables
-    self.local_delay_vars: Dict[str, bm.LengthDelay] = Collector()
+    # local delay variables:
+    # Compatible for ``DelayRegister``
+    # TODO: will be deprecated in the future
+    self.local_delay_vars: Dict = bm.node_dict()
+
+    # the before- / after-updates used for computing
+    # added after the version of 2.4.3
+    self.before_updates: Dict[str, Callable] = bm.node_dict()
+    self.after_updates: Dict[str, Callable] = bm.node_dict()
 
     # super initialization
-    BrainPyObject.__init__(self, name=name)
-
-  @property
-  def mode(self) -> bm.Mode:
-    """Mode of the model, which is useful to control the multiple behaviors of the model."""
-    return self._mode
-
-  @mode.setter
-  def mode(self, value):
-    if not isinstance(value, bm.Mode):
-      raise ValueError(f'Must be instance of {bm.Mode.__name__}, '
-                       f'but we got {type(value)}: {value}')
-    self._mode = value
-
-  def __repr__(self):
-    return f'{self.__class__.__name__}(name={self.name}, mode={self.mode})'
-
-  def __call__(self, *args, **kwargs):
-    """The shortcut to call ``update`` methods."""
-    global share
-    if share is None:
-      from brainpy._src.context import share
-
-    try:
-      if self._pass_shared_args:
-        if hasattr(self.update, '_new_style') and getattr(self.update, '_new_style'):
-          if len(args) and isinstance(args[0], dict):
-            share.save(**args[0])
-            return self.update(*args[1:], **kwargs)
-          else:
-            return self.update(*args, **kwargs)
-        else:
-          if len(args) and isinstance(args[0], dict):
-            return self.update(*args, **kwargs)
-          else:
-            # If first argument is not shared argument,
-            # we should get the shared arguments from the global context.
-            # However, users should set and update shared arguments
-            # in the global context when using this mode.
-            return self.update(share.get_shargs(), *args, **kwargs)
-      else:
-        if len(args) and isinstance(args[0], dict):  # it may be shared arguments
-          share.save(**args[0])
-          return self.update(*args[1:], **kwargs)
-        else:
-          return self.update(*args, **kwargs)
-    except Exception as e:
-      raise RuntimeError(f'Error occurs when running {self.name}: {self}') from e
-
-  def register_delay(
-      self,
-      identifier: str,
-      delay_step: Optional[Union[int, ArrayType, Callable, Initializer]],
-      delay_target: Variable,
-      initial_delay_data: Union[Initializer, Callable, ArrayType, float, int, bool] = None,
-  ):
-    """Register delay variable.
-
-    Parameters
-    ----------
-    identifier: str
-      The delay variable name.
-    delay_step: Optional, int, ArrayType, callable, Initializer
-      The number of the steps of the delay.
-    delay_target: Variable
-      The target variable for delay.
-    initial_delay_data: float, int, ArrayType, callable, Initializer
-      The initializer for the delay data.
-
-    Returns
-    -------
-    delay_step: int, ArrayType
-      The number of the delay steps.
-    """
-    # delay steps
-    if delay_step is None:
-      delay_type = 'none'
-    elif isinstance(delay_step, (int, np.integer, jnp.integer)):
-      delay_type = 'homo'
-    elif isinstance(delay_step, (bm.ndarray, jnp.ndarray, np.ndarray)):
-      if delay_step.size == 1 and delay_step.ndim == 0:
-        delay_type = 'homo'
-      else:
-        delay_type = 'heter'
-        delay_step = bm.asarray(delay_step)
-    elif callable(delay_step):
-      delay_step = parameter(delay_step, delay_target.shape, allow_none=False)
-      delay_type = 'heter'
-    else:
-      raise ValueError(f'Unknown "delay_steps" type {type(delay_step)}, only support '
-                       f'integer, array of integers, callable function, brainpy.init.Initializer.')
-    if delay_type == 'heter':
-      if delay_step.dtype not in [bm.int32, bm.int64]:
-        raise ValueError('Only support delay steps of int32, int64. If your '
-                         'provide delay time length, please divide the "dt" '
-                         'then provide us the number of delay steps.')
-      if delay_target.shape[0] != delay_step.shape[0]:
-        raise ValueError(f'Shape is mismatched: {delay_target.shape[0]} != {delay_step.shape[0]}')
-    if delay_type != 'none':
-      max_delay_step = int(bm.max(delay_step))
-
-    # delay target
-    if delay_type != 'none':
-      if not isinstance(delay_target, Variable):
-        raise ValueError(f'"delay_target" must be an instance of Variable, but we got {type(delay_target)}')
-
-    # delay variable
-    if delay_type != 'none':
-      if identifier not in self.global_delay_data:
-        delay = bm.LengthDelay(delay_target, max_delay_step, initial_delay_data)
-        self.global_delay_data[identifier] = (delay, delay_target)
-        self.local_delay_vars[identifier] = delay
-      else:
-        delay = self.global_delay_data[identifier][0]
-        if delay is None:
-          delay = bm.LengthDelay(delay_target, max_delay_step, initial_delay_data)
-          self.global_delay_data[identifier] = (delay, delay_target)
-          self.local_delay_vars[identifier] = delay
-        elif delay.num_delay_step - 1 < max_delay_step:
-          self.global_delay_data[identifier][0].reset(delay_target, max_delay_step, initial_delay_data)
-    else:
-      if identifier not in self.global_delay_data:
-        self.global_delay_data[identifier] = (None, delay_target)
-    self.register_implicit_nodes(self.local_delay_vars)
-    return delay_step
-
-  def get_delay_data(
-      self,
-      identifier: str,
-      delay_step: Optional[Union[int, bm.Array, jax.Array]],
-      *indices: Union[int, slice, bm.Array, jax.Array],
-  ):
-    """Get delay data according to the provided delay steps.
-
-    Parameters
-    ----------
-    identifier: str
-      The delay variable name.
-    delay_step: Optional, int, ArrayType
-      The delay length.
-    indices: optional, int, slice, ArrayType
-      The indices of the delay.
-
-    Returns
-    -------
-    delay_data: ArrayType
-      The delay data at the given time.
-    """
-    if delay_step is None:
-      return self.global_delay_data[identifier][1].value
-
-    if identifier in self.global_delay_data:
-      if bm.ndim(delay_step) == 0:
-        return self.global_delay_data[identifier][0](delay_step, *indices)
-      else:
-        if len(indices) == 0:
-          indices = (bm.arange(delay_step.size),)
-        return self.global_delay_data[identifier][0](delay_step, *indices)
-
-    elif identifier in self.local_delay_vars:
-      if bm.ndim(delay_step) == 0:
-        return self.local_delay_vars[identifier](delay_step)
-      else:
-        if len(indices) == 0:
-          indices = (bm.arange(delay_step.size),)
-        return self.local_delay_vars[identifier](delay_step, *indices)
-
-    else:
-      raise ValueError(f'{identifier} is not defined in delay variables.')
+    super().__init__(name=name)
 
   def update(self, *args, **kwargs):
     """The function to specify the updating rule.
 
     Assume any dynamical system depends on the shared variables (`sha`),
     like time variable ``t``, the step precision ``dt``, and the time step `i`.
     """
@@ -336,316 +154,418 @@
       for node in child_nodes.values():
         node.reset_state(*args, **kwargs)
       self.reset_local_delays(child_nodes)
     else:
       raise NotImplementedError('Must implement "reset_state" function by subclass self. '
                                 f'Error of {self.name}')
 
-  def update_local_delays(self, nodes: Union[Sequence, Dict] = None):
-    """Update local delay variables.
+  def clear_input(self):
+    """Clear the input at the current time step."""
+    pass
+
+  def step_run(self, i, *args, **kwargs):
+    """The step run function.
 
-    This function should be called after updating neuron groups or delay sources.
-    For example, in a network model,
+    This function can be directly applied to run the dynamical system.
+    Particularly, ``i`` denotes the running index.
 
+    Args:
+      i: The current running index.
+      *args: The arguments of ``update()`` function.
+      **kwargs: The arguments of ``update()`` function.
 
-    Parameters
-    ----------
-    nodes: sequence, dict
-      The nodes to update their delay variables.
+    Returns:
+      out: The update function returns.
     """
-    # update delays
-    if nodes is None:
-      nodes = tuple(self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique().values())
-    elif isinstance(nodes, DynamicalSystem):
-      nodes = (nodes,)
-    elif isinstance(nodes, dict):
-      nodes = tuple(nodes.values())
-    if not isinstance(nodes, (tuple, list)):
-      raise ValueError('Please provide nodes as a list/tuple/dict of DynamicalSystem.')
-    for node in nodes:
-      for name in node.local_delay_vars:
-        delay = self.global_delay_data[name][0]
-        target = self.global_delay_data[name][1]
-        delay.update(target.value)
-
-  def reset_local_delays(self, nodes: Union[Sequence, Dict] = None):
-    """Reset local delay variables.
-
-    Parameters
-    ----------
-    nodes: sequence, dict
-      The nodes to Reset their delay variables.
+    global share
+    if share is None:
+      from brainpy._src.context import share
+    share.save(i=i, t=i * bm.dt)
+    return self.update(*args, **kwargs)
+
+  @bm.cls_jit(inline=True)
+  def jit_step_run(self, i, *args, **kwargs):
+    """The jitted step function for running.
+
+    Args:
+      i: The current running index.
+      *args: The arguments of ``update()`` function.
+      **kwargs: The arguments of ``update()`` function.
+
+    Returns:
+      out: The update function returns.
     """
-    # reset delays
-    if nodes is None:
-      nodes = self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique().values()
-    elif isinstance(nodes, dict):
-      nodes = nodes.values()
-    for node in nodes:
-      for name in node.local_delay_vars:
-        delay = self.global_delay_data[name][0]
-        target = self.global_delay_data[name][1]
-        delay.reset(target.value)
+    return self.step_run(i, *args, **kwargs)
+
+  @property
+  def mode(self) -> bm.Mode:
+    """Mode of the model, which is useful to control the multiple behaviors of the model."""
+    return self._mode
+
+  @mode.setter
+  def mode(self, value):
+    if not isinstance(value, bm.Mode):
+      raise ValueError(f'Must be instance of {bm.Mode.__name__}, '
+                       f'but we got {type(value)}: {value}')
+    self._mode = value
+
+  def _compatible_update(self, *args, **kwargs):
+    global share
+    if share is None:
+      from brainpy._src.context import share
+    update_fun = super().__getattribute__('update')
+    update_args = tuple(inspect.signature(update_fun).parameters.values())
+
+    if len(update_args) and update_args[0].name in ['tdi', 'sh', 'sha']:
+      # define the update function with:
+      #     update(tdi, *args, **kwargs)
+      #
+      if len(args) > 0:
+        if isinstance(args[0], dict) and all([bm.isscalar(v) for v in args[0].values()]):
+          # define:
+          #    update(tdi, *args, **kwargs)
+          # call:
+          #    update(tdi, *args, **kwargs)
+          ret = update_fun(*args, **kwargs)
+          warnings.warn(_update_deprecate_msg, UserWarning)
+        else:
+          # define:
+          #    update(tdi, *args, **kwargs)
+          # call:
+          #    update(*args, **kwargs)
+          ret = update_fun(share.get_shargs(), *args, **kwargs)
+          warnings.warn(_update_deprecate_msg, UserWarning)
+      else:
+        if update_args[0].name in kwargs:
+          # define:
+          #    update(tdi, *args, **kwargs)
+          # call:
+          #    update(tdi=??, **kwargs)
+          ret = update_fun(**kwargs)
+          warnings.warn(_update_deprecate_msg, UserWarning)
+        else:
+          # define:
+          #    update(tdi, *args, **kwargs)
+          # call:
+          #    update(**kwargs)
+          ret = update_fun(share.get_shargs(), *args, **kwargs)
+          warnings.warn(_update_deprecate_msg, UserWarning)
+      return ret
+
+    try:
+      ba = inspect.signature(update_fun).bind(*args, **kwargs)
+    except TypeError:
+      if len(args) and isinstance(args[0], dict):
+        # user define ``update()`` function which does not receive the shared argument,
+        # but do provide these shared arguments when calling ``update()`` function
+        # -----
+        # change
+        #    update(tdi, *args, **kwargs)
+        # as
+        #    update(*args, **kwargs)
+        share.save(**args[0])
+        ret = update_fun(*args[1:], **kwargs)
+        warnings.warn(_update_deprecate_msg, UserWarning)
+        return ret
+      else:
+        # user define ``update()`` function which receives the shared argument,
+        # but not provide these shared arguments when calling ``update()`` function
+        # -----
+        # change
+        #    update(*args, **kwargs)
+        # as
+        #    update(tdi, *args, **kwargs)
+        ret = update_fun(share.get_shargs(), *args, **kwargs)
+        warnings.warn(_update_deprecate_msg, UserWarning)
+        return ret
+    else:
+      return update_fun(*args, **kwargs)
+
+  def __getattribute__(self, item):
+    if item == 'update':
+      return self._compatible_update  # update function compatible with previous ``update()`` function
+    else:
+      return super().__getattribute__(item)
+
+  def _get_update_fun(self):
+    return object.__getattribute__(self, 'update')
+
+  def __repr__(self):
+    return f'{self.name}(mode={self.mode})'
+
+  def __call__(self, *args, **kwargs):
+    """The shortcut to call ``update`` methods."""
+
+    # ``before_updates``
+    for model in self.before_updates.values():
+      model()
+
+    # update the model self
+    ret = self.update(*args, **kwargs)
+
+    # ``after_updates``
+    for model in self.after_updates.values():
+      model(ret)
+    return ret
 
   def __del__(self):
     """Function for handling `del` behavior.
 
     This function is used to pop out the variables which registered in global delay data.
     """
-    if hasattr(self, 'local_delay_vars'):
-      for key in tuple(self.local_delay_vars.keys()):
-        val = self.global_delay_data.pop(key)
-        del val
-        val = self.local_delay_vars.pop(key)
-        del val
-    if hasattr(self, 'implicit_nodes'):
-      for key in tuple(self.implicit_nodes.keys()):
-        del self.implicit_nodes[key]
-    if hasattr(self, 'implicit_vars'):
-      for key in tuple(self.implicit_vars.keys()):
-        del self.implicit_vars[key]
-    for key in tuple(self.__dict__.keys()):
-      del self.__dict__[key]
-    gc.collect()
-
-  def clear_input(self):
-    pass
+    try:
+      if hasattr(self, 'local_delay_vars'):
+        for key in tuple(self.local_delay_vars.keys()):
+          val = global_delay_data.pop(key)
+          del val
+          val = self.local_delay_vars.pop(key)
+          del val
+      if hasattr(self, 'implicit_nodes'):
+        for key in tuple(self.implicit_nodes.keys()):
+          del self.implicit_nodes[key]
+      if hasattr(self, 'implicit_vars'):
+        for key in tuple(self.implicit_vars.keys()):
+          del self.implicit_vars[key]
+      for key in tuple(self.__dict__.keys()):
+        del self.__dict__[key]
+    finally:
+      gc.collect()
 
   def __rrshift__(self, other):
     """Support using right shift operator to call modules.
 
     Examples
     --------
 
     >>> import brainpy as bp
     >>> x = bp.math.random.rand((10, 10))
     >>> l = bp.layers.Activation(bm.tanh)
     >>> y = x >> l
-
     """
     return self.__call__(other)
 
 
-class Container(DynamicalSystem):
-  """Container object which is designed to add other instances of DynamicalSystem.
+class DynSysGroup(DynamicalSystem, Container):
+  """A group of :py:class:`~.DynamicalSystem`s in which the updating order does not matter.
 
-  Parameters
-  ----------
-  name : str, optional
-    The object name.
-  mode: Mode
-    The mode which controls the model computation.
-  must_be_dynsys_subclass: bool
-    Child classes must be the subclass of :py:class:`DynamicalSystem`.
+  Args:
+    children_as_tuple: The children objects.
+    children_as_dict: The children objects.
+    name: The object name.
+    mode: The mode which controls the model computation.
+    child_type: The type of the children object. Default is :py:class:`DynamicalSystem`.
   """
 
   def __init__(
       self,
-      *dynamical_systems_as_tuple,
-      name: str = None,
-      mode: bm.Mode = None,
-      must_be_dynsys_subclass: bool = True,
-      **dynamical_systems_as_dict
+      *children_as_tuple,
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
+      child_type: type = DynamicalSystem,
+      **children_as_dict
   ):
-    super(Container, self).__init__(name=name, mode=mode)
-
-    if must_be_dynsys_subclass:
-      parent = DynamicalSystem
-      parent_name = DynamicalSystem.__name__
-    else:
-      parent = bm.BrainPyObject
-      parent_name = bm.BrainPyObject.__name__
-
-    # add tuple-typed components
-    for module in dynamical_systems_as_tuple:
-      if isinstance(module, parent):
-        self.implicit_nodes[module.name] = module
-      elif isinstance(module, (list, tuple)):
-        for m in module:
-          if not isinstance(m, parent):
-            raise ValueError(f'Should be instance of {parent_name}. '
-                             f'But we got {type(m)}')
-          self.implicit_nodes[m.name] = m
-      elif isinstance(module, dict):
-        for k, v in module.items():
-          if not isinstance(v, parent):
-            raise ValueError(f'Should be instance of {parent_name}. '
-                             f'But we got {type(v)}')
-          self.implicit_nodes[k] = v
-      else:
-        raise ValueError(f'Cannot parse sub-systems. They should be {parent_name} '
-                         f'or a list/tuple/dict of {parent_name}.')
-    # add dict-typed components
-    for k, v in dynamical_systems_as_dict.items():
-      if not isinstance(v, parent):
-        raise ValueError(f'Should be instance of {parent_name}. '
-                         f'But we got {type(v)}')
-      self.implicit_nodes[k] = v
+    super().__init__(name=name, mode=mode)
 
-  def __repr__(self):
-    cls_name = self.__class__.__name__
-    indent = ' ' * len(cls_name)
-    child_str = [tools.repr_context(repr(val), indent) for val in self.implicit_nodes.values()]
-    string = ", \n".join(child_str)
-    return f'{cls_name}({string})'
+    self.children = bm.node_dict(self.format_elements(child_type, *children_as_tuple, **children_as_dict))
 
-  def update(self, tdi, *args, **kwargs):
-    """Update function of a container.
+  def update(self, *args, **kwargs):
+    """Step function of a network.
 
     In this update function, the update functions in children systems are
     iteratively called.
-
-    Parameters
-    ----------
-    tdi: dict
-      The shared arguments including `t` the time, `dt` the time step, `t` the running index.
     """
-    nodes = self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique()
-    for node in nodes.values():
-      node(tdi)
+    nodes = self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique().not_subset(DynView)
 
-  def __getitem__(self, item):
-    """Overwrite the slice access (`self['']`). """
-    if item in self.implicit_nodes:
-      return self.implicit_nodes[item]
-    else:
-      raise ValueError(f'Unknown item {item}, we only found {list(self.implicit_nodes.keys())}')
+    # update nodes of projections
+    for node in nodes.subset(Projection).values():
+      node()
 
-  def __getattr__(self, item):
-    """Overwrite the dot access (`self.`). """
-    child_ds = super(Container, self).__getattribute__('implicit_nodes')
-    if item in child_ds:
-      return child_ds[item]
-    else:
-      return super(Container, self).__getattribute__(item)
+    # update nodes of dynamics
+    for node in nodes.subset(Dynamic).values():
+      node()
+
+    # update nodes with other types, including delays, ...
+    for node in nodes.not_subset(Dynamic).not_subset(Projection).values():
+      node()
+
+    # update delays
+    # TODO: Will be deprecated in the future
+    self.update_local_delays(nodes)
+
+  def reset_state(self, batch_size=None):
+    nodes = self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique().not_subset(DynView)
+
+    # reset projections
+    for node in nodes.subset(Projection).values():
+      node.reset_state(batch_size)
+
+    # reset dynamics
+    for node in nodes.subset(Dynamic).values():
+      node.reset_state(batch_size)
+
+    # reset other types of nodes, including delays, ...
+    for node in nodes.not_subset(Dynamic).not_subset(Projection).values():
+      node.reset_state(batch_size)
+
+    # reset delays
+    # TODO: will be removed in the future
+    self.reset_local_delays(nodes)
 
   def clear_input(self):
     """Clear inputs in the children classes."""
-    for node in self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique().values():
+    nodes = self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique().not_subset(DynView)
+    for node in nodes.values():
       node.clear_input()
 
 
+class Network(DynSysGroup):
+  """A group of :py:class:`~.DynamicalSystem`s which defines the nodes and edges in a network.
+  """
+  pass
 
-class Network(Container):
-  """Base class to model network objects, an alias of Container.
 
-  Network instantiates a network, which is aimed to load
-  neurons, synapses, and other brain objects.
+class Sequential(DynamicalSystem, AutoDelaySupp, Container):
+  """A sequential `input-output` module.
 
-  Parameters
-  ----------
-  name : str, Optional
-    The network name.
-  monitors : optional, list of str, tuple of str
-    The items to monitor.
-  ds_tuple :
-    A list/tuple container of dynamical system.
-  ds_dict :
-    A dict container of dynamical system.
+  Modules will be added to it in the order they are passed in the
+  constructor. Alternatively, an ``dict`` of modules can be
+  passed in. The ``update()`` method of ``Sequential`` accepts any
+  input and forwards it to the first module it contains. It then
+  "chains" outputs to inputs sequentially for each subsequent module,
+  finally returning the output of the last module.
+
+  The value a ``Sequential`` provides over manually calling a sequence
+  of modules is that it allows treating the whole container as a
+  single module, such that performing a transformation on the
+  ``Sequential`` applies to each of the modules it stores (which are
+  each a registered submodule of the ``Sequential``).
+
+  What's the difference between a ``Sequential`` and a
+  :py:class:`Container`? A ``Container`` is exactly what it
+  sounds like--a container to store :py:class:`DynamicalSystem` s!
+  On the other hand, the layers in a ``Sequential`` are connected
+  in a cascading way.
+
+  Examples
+  --------
+
+  >>> import brainpy as bp
+  >>> import brainpy.math as bm
+  >>>
+  >>> # composing ANN models
+  >>> l = bp.Sequential(bp.layers.Dense(100, 10),
+  >>>                   bm.relu,
+  >>>                   bp.layers.Dense(10, 2))
+  >>> l({}, bm.random.random((256, 100)))
+  >>>
+  >>> # Using Sequential with Dict. This is functionally the
+  >>> # same as the above code
+  >>> l = bp.Sequential(l1=bp.layers.Dense(100, 10),
+  >>>                   l2=bm.relu,
+  >>>                   l3=bp.layers.Dense(10, 2))
+  >>> l({}, bm.random.random((256, 100)))
+
+
+  Args:
+    modules_as_tuple: The children modules.
+    modules_as_dict: The children modules.
+    name: The object name.
+    mode: The object computing context/mode. Default is ``None``.
   """
 
   def __init__(
       self,
-      *ds_tuple,
+      *modules_as_tuple,
       name: str = None,
       mode: bm.Mode = None,
-      **ds_dict
+      **modules_as_dict
   ):
-    super(Network, self).__init__(*ds_tuple, name=name, mode=mode, **ds_dict)
-
-  @not_pass_shared
-  def update(self, *args, **kwargs):
-    """Step function of a network.
+    super().__init__(name=name, mode=mode)
+    self.children = bm.node_dict(self.format_elements(object, *modules_as_tuple, **modules_as_dict))
 
-    In this update function, the update functions in children systems are
-    iteratively called.
+  def update(self, x):
+    """Update function of a sequential model.
     """
-    nodes = self.nodes(level=1, include_self=False)
-    nodes = nodes.subset(DynamicalSystem)
-    nodes = nodes.unique()
-    neuron_groups = nodes.subset(NeuGroup)
-    synapse_groups = nodes.subset(SynConn)
-    ds_views = nodes.subset(DSView)
-    other_nodes = nodes - neuron_groups - synapse_groups - ds_views
-
-    # shared arguments
-
-    # update synapse nodes
-    for node in synapse_groups.values():
-      node()
-
-    # update neuron nodes
-    for node in neuron_groups.values():
-      node()
-
-    # update other types of nodes
-    for node in other_nodes.values():
-      node()
-
-    # update delays
-    self.update_local_delays(nodes)
+    for m in self.children.values():
+      x = m(x)
+    return x
 
-  def reset_state(self, batch_size=None):
-    nodes = self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique()
-    neuron_groups = nodes.subset(NeuGroup)
-    synapse_groups = nodes.subset(SynConn)
+  def return_info(self):
+    last = self[-1]
+    if not isinstance(last, AutoDelaySupp):
+      raise UnsupportedError(f'Does not support "return_info()" because the last node is '
+                             f'not instance of {AutoDelaySupp.__name__}')
+    return last.return_info()
 
-    # reset neuron nodes
-    for node in neuron_groups.values():
-      node.reset_state(batch_size)
+  def __format_key(self, i):
+    return f'l-{i}'
 
-    # reset synapse nodes
-    for node in synapse_groups.values():
-      node.reset_state(batch_size)
+  def __all_nodes(self):
+    nodes = []
+    for i in range(self._num):
+      key = self.__format_key(i)
+      if key not in self._dyn_modules:
+        nodes.append(self._static_modules[key])
+      else:
+        nodes.append(self._dyn_modules[key])
+    return nodes
 
-    # reset other types of nodes
-    for node in (nodes - neuron_groups - synapse_groups).values():
-      node.reset_state(batch_size)
+  def __getitem__(self, key: Union[int, slice, str]):
+    if isinstance(key, str):
+      if key in self.children:
+        return self.children[key]
+      else:
+        raise KeyError(f'Does not find a component named {key} in\n {str(self)}')
+    elif isinstance(key, slice):
+      return Sequential(**dict(tuple(self.children.items())[key]))
+    elif isinstance(key, int):
+      return tuple(self.children.values())[key]
+    elif isinstance(key, (tuple, list)):
+      _all_nodes = tuple(self.children.items())
+      return Sequential(**dict(_all_nodes[k] for k in key))
+    else:
+      raise KeyError(f'Unknown type of key: {type(key)}')
 
-    # reset delays
-    self.reset_local_delays(nodes)
+  def __repr__(self):
+    nodes = self.__all_nodes()
+    entries = '\n'.join(f'  [{i}] {tools.repr_object(x)}' for i, x in enumerate(nodes))
+    return f'{self.__class__.__name__}(\n{entries}\n)'
 
 
-class System(Network):
-  pass
+class Projection(DynamicalSystem):
+  def reset_state(self, *args, **kwargs):
+    pass
 
 
-class NeuGroup(DynamicalSystem):
-  """Base class to model neuronal groups.
+class Dynamic(DynamicalSystem):
+  """Base class to model dynamics.
 
   There are several essential attributes:
 
   - ``size``: the geometry of the neuron group. For example, `(10, )` denotes a line of
     neurons, `(10, 10)` denotes a neuron group aligned in a 2D space, `(10, 15, 4)` denotes
     a 3-dimensional neuron group.
   - ``num``: the flattened number of neurons in the group. For example, `size=(10, )` => \
     `num=10`, `size=(10, 10)` => `num=100`, `size=(10, 15, 4)` => `num=600`.
 
-  Parameters
-  ----------
-  size : int, tuple of int, list of int
-    The neuron group geometry.
-  name : optional, str
-    The name of the dynamic system.
-  keep_size: bool
-    Whether keep the geometry information.
-
-    .. versionadded:: 2.1.13
-  mode: Mode
-    The computing mode.
-
-    .. versionadded:: 2.2.0
+  Args:
+    size: The neuron group geometry.
+    name: The name of the dynamic system.
+    keep_size: Whether keep the geometry information.
+    mode: The computing mode.
   """
 
   def __init__(
       self,
       size: Shape,
       keep_size: bool = False,
-      name: str = None,
-      mode: bm.Mode = None,
+      sharding: Optional[Any] = None,
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
+      method: str = 'exp_auto'
   ):
     # size
     if isinstance(size, (list, tuple)):
       if len(size) <= 0:
         raise ValueError(f'size must be int, or a tuple/list of int. '
                          f'But we got {type(size)}')
       if not isinstance(size[0], (int, np.integer)):
@@ -659,668 +579,190 @@
                        f'But we got {type(size)}')
     self.size = size
     self.keep_size = keep_size
 
     # number of neurons
     self.num = tools.size2num(size)
 
+    # axis names for parallelization
+    self.sharding = sharding
+
+    # integration method
+    self.method = method
+
+    # inputs
+    self.cur_inputs: Dict = bm.node_dict()
+
     # initialize
-    super(NeuGroup, self).__init__(name=name, mode=mode)
+    super().__init__(name=name, mode=mode)
 
   @property
   def varshape(self):
     """The shape of variables in the neuron group."""
     return self.size if self.keep_size else (self.num,)
 
-  def __repr__(self):
-    return f'{self.__class__.__name__}(name={self.name}, mode={self.mode}, size={self.size})'
-
   def get_batch_shape(self, batch_size=None):
     if batch_size is None:
       return self.varshape
     else:
       return (batch_size,) + self.varshape
 
   def update(self, *args, **kwargs):
     """The function to specify the updating rule.
     """
     raise NotImplementedError(f'Subclass of {self.__class__.__name__} must '
                               f'implement "update" function.')
 
-  def clear_input(self):
-    """Function to clear inputs in the neuron group.
-    It will be useful when monitoring inputs of the object received."""
-    pass
-
-  def __getitem__(self, item):
-    return NeuGroupView(target=self, index=item)
-
-
-class SynConn(DynamicalSystem):
-  """Base class to model two-end synaptic connections.
-
-  Parameters
-  ----------
-  pre : NeuGroup
-    Pre-synaptic neuron group.
-  post : NeuGroup
-    Post-synaptic neuron group.
-  conn : optional, ndarray, ArrayType, dict, TwoEndConnector
-    The connection method between pre- and post-synaptic groups.
-  name : str, optional
-    The name of the dynamic system.
-  """
+  def init_param(self, param, shape=None, sharding=None):
+    """Initialize parameters.
 
-  def __init__(
-      self,
-      pre: NeuGroup,
-      post: NeuGroup,
-      conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]] = None,
-      name: str = None,
-      mode: bm.Mode = None,
-  ):
-    super(SynConn, self).__init__(name=name, mode=mode)
+    If ``sharding`` is provided and ``param`` is array, this function will
+    partition the parameter across the default device mesh.
 
-    # pre or post neuron group
-    # ------------------------
-    if not isinstance(pre, (NeuGroup, DynamicalSystem)):
-      raise TypeError('"pre" must be an instance of NeuGroup.')
-    if not isinstance(post, (NeuGroup, DynamicalSystem)):
-      raise TypeError('"post" must be an instance of NeuGroup.')
-    self.pre = pre
-    self.post = post
-
-    # connectivity
-    # ------------
-    if isinstance(conn, TwoEndConnector):
-      self.conn = conn(pre.size, post.size)
-    elif isinstance(conn, (bm.ndarray, np.ndarray, jax.Array)):
-      if (pre.num, post.num) != conn.shape:
-        raise ValueError(f'"conn" is provided as a matrix, and it is expected '
-                         f'to be an array with shape of (pre.num, post.num) = '
-                         f'{(pre.num, post.num)}, however we got {conn.shape}')
-      self.conn = MatConn(conn_mat=conn)
-    elif isinstance(conn, dict):
-      if not ('i' in conn and 'j' in conn):
-        raise ValueError(f'"conn" is provided as a dict, and it is expected to '
-                         f'be a dictionary with "i" and "j" specification, '
-                         f'however we got {conn}')
-      self.conn = IJConn(i=conn['i'], j=conn['j'])
-    elif isinstance(conn, str):
-      self.conn = conn
-    elif conn is None:
-      self.conn = None
-    else:
-      raise ValueError(f'Unknown "conn" type: {conn}')
-
-  def __repr__(self):
-    names = self.__class__.__name__
-    return (f'{names}(name={self.name}, mode={self.mode}, \n'
-            f'{" " * len(names)} pre={self.pre}, \n'
-            f'{" " * len(names)} post={self.post})')
-
-  def check_pre_attrs(self, *attrs):
-    """Check whether pre group satisfies the requirement."""
-    if not hasattr(self, 'pre'):
-      raise ValueError('Please call __init__ function first.')
-    for attr in attrs:
-      if not isinstance(attr, str):
-        raise TypeError(f'Must be string. But got {attr}.')
-      if not hasattr(self.pre, attr):
-        raise ValueError(f'{self} need "pre" neuron group has attribute "{attr}".')
-
-  def check_post_attrs(self, *attrs):
-    """Check whether post group satisfies the requirement."""
-    if not hasattr(self, 'post'):
-      raise ValueError('Please call __init__ function first.')
-    for attr in attrs:
-      if not isinstance(attr, str):
-        raise TypeError(f'Must be string. But got {attr}.')
-      if not hasattr(self.post, attr):
-        raise ValueError(f'{self} need "pre" neuron group has attribute "{attr}".')
-
-  def update(self, *args, **kwargs):
-    """The function to specify the updating rule.
-
-    Assume any dynamical system depends on the shared variables (`sha`),
-    like time variable ``t``, the step precision ``dt``, and the time step `i`.
+    See :py:func:`~.brainpy.math.sharding.device_mesh` for the mesh setting.
     """
-    raise NotImplementedError('Must implement "update" function by subclass self.')
-
-
-class _SynComponent(DynamicalSystem):
-  """Base class for modeling synaptic components,
-  including synaptic output, synaptic short-term plasticity,
-  synaptic long-term plasticity, and others. """
-
-  '''Master of this component.'''
-  master: SynConn
-
-  def __init__(self, *args, **kwargs):
-    super(_SynComponent, self).__init__(*args, **kwargs)
-
-    self._registered = False
-
-  @property
-  def isregistered(self) -> bool:
-    """State of the component, representing whether it has been registered."""
-    return self._registered
-
-  @isregistered.setter
-  def isregistered(self, val: bool):
-    if not isinstance(val, bool):
-      raise ValueError('Must be an instance of bool.')
-    self._registered = val
-
-  def reset_state(self, batch_size=None):
-    pass
-
-  def register_master(self, master: SynConn):
-    if not isinstance(master, SynConn):
-      raise TypeError(f'master must be instance of {SynConn.__name__}, but we got {type(master)}')
-    if self.isregistered:
-      raise ValueError(f'master has been registered, but we got another master going to be registered.')
-    if hasattr(self, 'master') and self.master != master:
-      raise ValueError(f'master has been registered, but we got another master going to be registered.')
-    self.master = master
-    self._registered = True
+    shape = self.varshape if shape is None else shape
+    sharding = self.sharding if sharding is None else sharding
+    return parameter(param,
+                     sizes=shape,
+                     allow_none=False,
+                     sharding=sharding)
+
+  def init_variable(self, var_data, batch_or_mode, shape=None, sharding=None):
+    """Initialize variables.
+
+    If ``sharding`` is provided and ``var_data`` is array, this function will
+    partition the variable across the default device mesh.
+
+    See :py:func:`~.brainpy.math.sharding.device_mesh` for the mesh setting.
+    """
+    shape = self.varshape if shape is None else shape
+    sharding = self.sharding if sharding is None else sharding
+    return variable_(var_data,
+                     sizes=shape,
+                     batch_or_mode=batch_or_mode,
+                     axis_names=sharding,
+                     batch_axis_name=bm.sharding.BATCH_AXIS)
 
   def __repr__(self):
-    return self.__class__.__name__
-
-  def __call__(self, *args, **kwargs):
-    return self.filter(*args, **kwargs)
-
-  def clone(self) -> '_SynComponent':
-    """The function useful to clone a new object when it has been used."""
-    raise NotImplementedError
-
-  def filter(self, g):
-    raise NotImplementedError
-
-
-class SynOut(_SynComponent):
-  """Base class for synaptic current output."""
-
-  def __init__(
-      self,
-      name: str = None,
-      target_var: Union[str, Variable] = None,
-  ):
-    super(SynOut, self).__init__(name=name)
-    # check target variable
-    if target_var is not None:
-      if not isinstance(target_var, (str, Variable)):
-        raise TypeError('"target_var" must be instance of string or Variable. '
-                        f'But we got {type(target_var)}')
-    self.target_var: Optional[Variable] = target_var
-
-  def register_master(self, master: SynConn):
-    super(SynOut, self).register_master(master)
-
-    # initialize target variable to output
-    if isinstance(self.target_var, str):
-      if not hasattr(self.master.post, self.target_var):
-        raise KeyError(f'Post-synaptic group does not have target variable: {self.target_var}')
-      self.target_var = getattr(self.master.post, self.target_var)
-
-  def filter(self, g):
-    if self.target_var is None:
-      return g
-    else:
-      self.target_var += g
-
-  def update(self, tdi):
-    pass
-
-
-class SynSTP(_SynComponent):
-  """Base class for synaptic short-term plasticity."""
-
-  def update(self, tdi, pre_spike):
-    pass
-
-
-class SynLTP(_SynComponent):
-  """Base class for synaptic long-term plasticity."""
-
-  def update(self, tdi, pre_spike):
-    pass
-
-
-class NullSynOut(SynOut):
-  def clone(self):
-    return NullSynOut()
-
-
-class TwoEndConn(SynConn):
-  """Base class to model synaptic connections.
-
-  Parameters
-  ----------
-  pre : NeuGroup
-    Pre-synaptic neuron group.
-  post : NeuGroup
-    Post-synaptic neuron group.
-  conn : optional, ndarray, ArrayType, dict, TwoEndConnector
-    The connection method between pre- and post-synaptic groups.
-  output: Optional, SynOutput
-    The output for the synaptic current.
-
-    .. versionadded:: 2.1.13
-       The output component for a two-end connection model.
-
-  stp: Optional, SynSTP
-    The short-term plasticity model for the synaptic variables.
-
-    .. versionadded:: 2.1.13
-       The short-term plasticity component for a two-end connection model.
-
-  ltp: Optional, SynLTP
-    The long-term plasticity model for the synaptic variables.
-
-    .. versionadded:: 2.1.13
-       The long-term plasticity component for a two-end connection model.
-
-  name: Optional, str
-    The name of the dynamic system.
-  """
-
-  def __init__(
-      self,
-      pre: NeuGroup,
-      post: NeuGroup,
-      conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]] = None,
-      output: SynOut = NullSynOut(),
-      stp: Optional[SynSTP] = None,
-      ltp: Optional[SynLTP] = None,
-      mode: bm.Mode = None,
-      name: str = None,
-  ):
-    super(TwoEndConn, self).__init__(pre=pre,
-                                     post=post,
-                                     conn=conn,
-                                     name=name,
-                                     mode=mode)
-
-    # synaptic output
-    output = NullSynOut() if output is None else output
-    if output.isregistered: output = output.clone()
-    if not isinstance(output, SynOut):
-      raise TypeError(f'output must be instance of {SynOut.__name__}, '
-                      f'but we got {type(output)}')
-    output.register_master(master=self)
-    self.output: SynOut = output
-
-    # short-term synaptic plasticity
-    if stp is not None:
-      if stp.isregistered: stp = stp.clone()
-      if not isinstance(stp, SynSTP):
-        raise TypeError(f'Short-term plasticity must be instance of {SynSTP.__name__}, '
-                        f'but we got {type(stp)}')
-      stp.register_master(master=self)
-    self.stp: SynSTP = stp
-
-    # long-term synaptic plasticity
-    if ltp is not None:
-      if ltp.isregistered: ltp = ltp.clone()
-      if not isinstance(ltp, SynLTP):
-        raise TypeError(f'Long-term plasticity must be instance of {SynLTP.__name__}, '
-                        f'but we got {type(ltp)}')
-      ltp.register_master(master=self)
-    self.ltp: SynLTP = ltp
-
-  def _init_weights(
-      self,
-      weight: Union[float, ArrayType, Initializer, Callable],
-      comp_method: str,
-      sparse_data: str = 'csr'
-  ) -> Tuple[Union[float, ArrayType], ArrayType]:
-    if comp_method not in ['sparse', 'dense']:
-      raise ValueError(f'"comp_method" must be in "sparse" and "dense", but we got {comp_method}')
-    if sparse_data not in ['csr', 'ij', 'coo']:
-      raise ValueError(f'"sparse_data" must be in "csr" and "ij", but we got {sparse_data}')
-    if self.conn is None:
-      raise ValueError(f'Must provide "conn" when initialize the model {self.name}')
-
-    # connections and weights
-    if isinstance(self.conn, One2One):
-      weight = parameter(weight, (self.pre.num,), allow_none=False)
-      conn_mask = None
-
-    elif isinstance(self.conn, All2All):
-      weight = parameter(weight, (self.pre.num, self.post.num), allow_none=False)
-      conn_mask = None
-
-    else:
-      if comp_method == 'sparse':
-        if sparse_data == 'csr':
-          conn_mask = self.conn.require('pre2post')
-        elif sparse_data in ['ij', 'coo']:
-          conn_mask = self.conn.require('post_ids', 'pre_ids')
-        else:
-          ValueError(f'Unknown sparse data type: {sparse_data}')
-        weight = parameter(weight, conn_mask[0].shape, allow_none=False)
-      elif comp_method == 'dense':
-        weight = parameter(weight, (self.pre.num, self.post.num), allow_none=False)
-        conn_mask = self.conn.require('conn_mat')
-      else:
-        raise ValueError(f'Unknown connection type: {comp_method}')
-
-    # training weights
-    if isinstance(self.mode, bm.TrainingMode):
-      weight = bm.TrainVar(weight)
-    return weight, conn_mask
-
-  def _syn2post_with_all2all(self, syn_value, syn_weight):
-    if bm.ndim(syn_weight) == 0:
-      if isinstance(self.mode, bm.BatchingMode):
-        post_vs = bm.sum(syn_value, keepdims=True, axis=tuple(range(syn_value.ndim))[1:])
-      else:
-        post_vs = bm.sum(syn_value)
-      if not self.conn.include_self:
-        post_vs = post_vs - syn_value
-      post_vs = syn_weight * post_vs
-    else:
-      post_vs = syn_value @ syn_weight
-    return post_vs
-
-  def _syn2post_with_one2one(self, syn_value, syn_weight):
-    return syn_value * syn_weight
-
-  def _syn2post_with_dense(self, syn_value, syn_weight, conn_mat):
-    if bm.ndim(syn_weight) == 0:
-      post_vs = (syn_weight * syn_value) @ conn_mat
-    else:
-      post_vs = syn_value @ (syn_weight * conn_mat)
-    return post_vs
-
-
-class TwoEndConnNS(TwoEndConn):
-  """Two-end connection without passing shared arguments."""
-  _pass_shared_args = False
-
-
-class CondNeuGroup(NeuGroup, Container):
-  r"""Base class to model conductance-based neuron group.
-
-  The standard formulation for a conductance-based model is given as
-
-  .. math::
+    return f'{self.name}(mode={self.mode}, size={self.size})'
 
-      C_m {dV \over dt} = \sum_jg_j(E - V) + I_{ext}
-
-  where :math:`g_j=\bar{g}_{j} M^x N^y` is the channel conductance, :math:`E` is the
-  reversal potential, :math:`M` is the activation variable, and :math:`N` is the
-  inactivation variable.
-
-  :math:`M` and :math:`N` have the dynamics of
-
-  .. math::
-
-      {dx \over dt} = \phi_x {x_\infty (V) - x \over \tau_x(V)}
-
-  where :math:`x \in [M, N]`, :math:`\phi_x` is a temperature-dependent factor,
-  :math:`x_\infty` is the steady state, and :math:`\tau_x` is the time constant.
-  Equivalently, the above equation can be written as:
-
-  .. math::
-
-      \frac{d x}{d t}=\phi_{x}\left(\alpha_{x}(1-x)-\beta_{x} x\right)
-
-  where :math:`\alpha_{x}` and :math:`\beta_{x}` are rate constants.
-
-  .. versionadded:: 2.1.9
-     Model the conductance-based neuron model.
-
-  Parameters
-  ----------
-  size : int, sequence of int
-    The network size of this neuron group.
-  method: str
-    The numerical integration method.
-  name : optional, str
-    The neuron group name.
-
-  See Also
-  --------
-  Channel
-
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      C: Union[float, ArrayType, Initializer, Callable] = 1.,
-      A: Union[float, ArrayType, Initializer, Callable] = 1e-3,
-      V_th: Union[float, ArrayType, Initializer, Callable] = 0.,
-      V_initializer: Union[Initializer, Callable, ArrayType] = Uniform(-70, -60.),
-      noise: Union[float, ArrayType, Initializer, Callable] = None,
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-      **channels
-  ):
-    NeuGroup.__init__(self, size, keep_size=keep_size, mode=mode)
-    Container.__init__(self, **channels, name=name, mode=mode)
-
-    # parameters for neurons
-    self.C = C
-    self.A = A
-    self.V_th = V_th
-    self.noise = init_noise(noise, self.varshape, num_vars=3)
-    self._V_initializer = V_initializer
-
-    # variables
-    self.V = variable(V_initializer, self.mode, self.varshape)
-    self.input = variable(bm.zeros, self.mode, self.varshape)
-    self.spike = variable(lambda s: bm.zeros(s, dtype=bool), self.mode, self.varshape)
-
-    # function
-    if self.noise is None:
-      self.integral = odeint(f=self.derivative, method=method)
-    else:
-      self.integral = sdeint(f=self.derivative, g=self.noise, method=method)
-
-  def derivative(self, V, t):
-    Iext = self.input.value * (1e-3 / self.A)
-    channels = self.nodes(level=1, include_self=False).subset(Channel).unique()
-    for ch in channels.values():
-      Iext = Iext + ch.current(V)
-    return Iext / self.C
-
-  def reset_state(self, batch_size=None):
-    self.V.value = variable(self._V_initializer, batch_size, self.varshape)
-    self.spike.value = variable(lambda s: bm.zeros(s, dtype=bool), batch_size, self.varshape)
-    self.input.value = variable(bm.zeros, batch_size, self.varshape)
-    for channel in self.nodes(level=1, include_self=False).subset(Channel).unique().values():
-      channel.reset_state(self.V.value, batch_size=batch_size)
-
-  def update(self, tdi, *args, **kwargs):
-    V = self.integral(self.V.value, tdi['t'], tdi['dt'])
-
-    channels = self.nodes(level=1, include_self=False).subset(Channel).unique()
-    # check whether the children channels have the correct parents.
-    check_master(type(self), **channels)
-
-    # update variables
-    for node in channels.values():
-      node.update(tdi, self.V.value)
-    self.spike.value = bm.logical_and(V >= self.V_th, self.V < self.V_th)
-    self.V.value = V
-
-  def register_implicit_nodes(self, *channels, **named_channels):
-    check_master(type(self), *channels, **named_channels)
-    super(CondNeuGroup, self).register_implicit_nodes(*channels, **named_channels)
-
-  def clear_input(self):
-    """Useful for monitoring inputs. """
-    self.input.value = bm.zeros_like(self.input.value)
-
-
-class Channel(DynamicalSystem):
-  """Abstract channel class."""
-
-  master_type = CondNeuGroup
-
-  def __init__(
-      self,
-      size: Union[int, Sequence[int]],
-      name: str = None,
-      keep_size: bool = False,
-      mode: bm.Mode = None,
-  ):
-    super(Channel, self).__init__(name=name, mode=mode)
-    # the geometry size
-    self.size = tools.to_size(size)
-    # the number of elements
-    self.num = tools.size2num(self.size)
-    # variable shape
-    self.keep_size = keep_size
-
-  @property
-  def varshape(self):
-    return self.size if self.keep_size else self.num
-
-  def update(self, tdi, V):
-    raise NotImplementedError('Must be implemented by the subclass.')
-
-  def current(self, V):
-    raise NotImplementedError('Must be implemented by the subclass.')
-
-  def reset_state(self, V, batch_size=None):
-    raise NotImplementedError('Must be implemented by the subclass.')
-
-
-def _check(master, child):
-  if not hasattr(child, 'master_type'):
-    raise ValueError('Child class should define "master_type" to specify the type of the master. '
-                     f'But we did not found it in {child}')
-  if not issubclass(master, child.master_type):
-    raise TypeError(f'Type does not match. {child} requires a master with type '
-                    f'of {child.master_type}, but the master now is {master}.')
-
-
-def check_master(master, *channels, **named_channels):
-  for channel in channels:
-    if isinstance(channel, Channel):
-      _check(master, channel)
-    elif isinstance(channel, (list, tuple)):
-      for ch in channel:
-        _check(master, ch)
-    elif isinstance(channel, dict):
-      for ch in channel.values():
-        _check(master, ch)
-    else:
-      raise ValueError(f'Do not support {type(channel)}.')
-  for channel in named_channels.values():
-    if not isinstance(channel, Channel):
-      raise ValueError(f'Do not support {type(channel)}. ')
-    _check(master, channel)
+  def __getitem__(self, item):
+    return DynView(target=self, index=item)
 
 
-class DSView(DynamicalSystem):
+class DynView(Dynamic):
   """DSView, an object used to get a view of a dynamical system instance.
 
   It can get a subset view of variables in a dynamical system instance.
   For instance,
 
   >>> import brainpy as bp
   >>> hh = bp.neurons.HH(10)
-  >>> DSView(hh, slice(5, 10, None))
+  >>> DynView(hh, slice(5, 10, None))
   >>> # or, simply
   >>> hh[5:]
   """
 
   def __init__(
       self,
-      target: DynamicalSystem,
+      target: Dynamic,
       index: Union[slice, Sequence, ArrayType],
-      varshape: Tuple[int, ...] = None,
-      name: str = None,
-      mode: bm.Mode = None
+      name: Optional[str] = None,
   ):
-    # initialization
-    DynamicalSystem.__init__(self, name=name, mode=mode)
-
     # check target
-    if not isinstance(target, DynamicalSystem):
-      raise TypeError(f'Should be instance of DynamicalSystem, but we got {type(target)}.')
+    if not isinstance(target, Dynamic):
+      raise TypeError(f'Should be instance of {Dynamic.__name__}, but we got {type(target)}.')
     self.target = target  # the target object to slice
 
     # check slicing
     if isinstance(index, (int, slice)):
       index = (index,)
     self.index = index  # the slice
+    if len(self.index) > len(target.varshape):
+      raise ValueError(f"Length of the index should be less than "
+                       f"that of the target's varshape. But we "
+                       f"got {len(self.index)} > {len(target.varshape)}")
 
     # get all variables for slicing
-    if not hasattr(self.target, SLICE_VARS):
-      if varshape is None:
-        if isinstance(target, NeuGroup):
-          varshape = target.varshape
-        else:
-          raise UnsupportedError('Should provide varshape when the target does '
-                                 f'not define its {SLICE_VARS}')
-      all_vars = target.vars(level=1, include_self=True, method='relative')
-      all_vars = {k: v for k, v in all_vars.items()}  # TODO
-      # all_vars = {k: v for k, v in all_vars.items() if v.nobatch_shape == varshape}
-    else:
+    if hasattr(self.target, SLICE_VARS):
       all_vars = {}
       for var_str in getattr(self.target, SLICE_VARS):
         v = eval(f'target.{var_str}')
         all_vars[var_str] = v
+    else:
+      all_vars = target.vars(level=1, include_self=True, method='relative')
+      all_vars = {k: v for k, v in all_vars.items()}  # TODO
+      # all_vars = {k: v for k, v in all_vars.items() if v.nobatch_shape == varshape}
 
     # slice variables
     self.slice_vars = dict()
     for k, v in all_vars.items():
       if v.batch_axis is not None:
-        index = ((self.index[:v.batch_axis] +
-                  (slice(None, None, None),) +
-                  self.index[v.batch_axis:])
-                 if len(self.index) > v.batch_axis else
-                 (self.index + tuple([slice(None, None, None)
-                                      for _ in range(v.batch_axis - len(self.index) + 1)])))
+        index = (
+          (self.index[:v.batch_axis] + (slice(None, None, None),) + self.index[v.batch_axis:])
+          if (len(self.index) > v.batch_axis) else
+          (self.index + tuple([slice(None, None, None) for _ in range(v.batch_axis - len(self.index) + 1)]))
+        )
       else:
         index = self.index
-      self.slice_vars[k] = VariableView(v, index)
+      self.slice_vars[k] = bm.VariableView(v, index)
 
     # sub-nodes
     nodes = target.nodes(method='relative', level=1, include_self=False).subset(DynamicalSystem)
     for k, node in nodes.items():
-      if isinstance(node, NeuGroup):
-        node = NeuGroupView(node, self.index)
+      if isinstance(node, Dynamic):
+        node = DynView(node, self.index)
       else:
-        node = DSView(node, self.index, varshape)
+        node = DynView(node, self.index)
       setattr(self, k, node)
 
+    # initialization
+    # get size
+    size = []
+    for i, idx in enumerate(self.index):
+      if isinstance(idx, int):
+        size.append(1)
+      elif isinstance(idx, slice):
+        size.append(_slice_to_num(idx, target.varshape[i]))
+      else:
+        # should be a list/tuple/array of int
+        # do not check again
+        if not isinstance(idx, collections.Iterable):
+          raise TypeError('Should be an iterable object of int.')
+        size.append(len(idx))
+    size += list(target.varshape[len(self.index):])
+
+    super().__init__(size, keep_size=target.keep_size, name=name, mode=target.mode)
+
   def __repr__(self):
-    return f'{self.__class__.__name__}(target={self.target}, index={self.index})'
+    return f'{self.name}(target={self.target}, index={self.index})'
 
   def __getattribute__(self, item):
     try:
       slice_vars = object.__getattribute__(self, 'slice_vars')
       if item in slice_vars:
         value = slice_vars[item]
         return value
       return object.__getattribute__(self, item)
     except AttributeError:
       return object.__getattribute__(self, item)
 
   def __setattr__(self, key, value):
     if hasattr(self, 'slice_vars'):
-      slice_vars = super(DSView, self).__getattribute__('slice_vars')
+      slice_vars = super().__getattribute__('slice_vars')
       if key in slice_vars:
         v = slice_vars[key]
         v.value = value
         return
-    super(DSView, self).__setattr__(key, value)
+    super(DynView, self).__setattr__(key, value)
 
   def update(self, *args, **kwargs):
-    raise NoImplementationError(f'DSView {self} cannot be updated. Please update its parent {self.target}')
+    raise NoImplementationError(f'{DynView.__name__} {self} cannot be updated. '
+                                f'Please update its parent {self.target}')
 
   def reset_state(self, batch_size=None):
     pass
 
 
 @tools.numba_jit
 def _slice_to_num(slice_: slice, length: int):
@@ -1344,170 +786,7 @@
     step = 1
   # number
   num = 0
   while start < stop:
     start += step
     num += 1
   return num
-
-
-class NeuGroupView(DSView, NeuGroup):
-  """A view for a neuron group instance."""
-
-  def __init__(
-      self,
-      target: NeuGroup,
-      index: Union[slice, Sequence, ArrayType],
-      name: str = None,
-      mode: bm.Mode = None
-  ):
-    DSView.__init__(self, target, index)
-
-    # check slicing
-    var_shapes = target.varshape
-    if len(self.index) > len(var_shapes):
-      raise ValueError(f"Length of the index should be less than "
-                       f"that of the target's varshape. But we "
-                       f"got {len(self.index)} > {len(var_shapes)}")
-
-    # get size
-    size = []
-    for i, idx in enumerate(self.index):
-      if isinstance(idx, int):
-        size.append(1)
-      elif isinstance(idx, slice):
-        size.append(_slice_to_num(idx, var_shapes[i]))
-      else:
-        # should be a list/tuple/array of int
-        # do not check again
-        if not isinstance(idx, collections.Iterable):
-          raise TypeError('Should be an iterable object of int.')
-        size.append(len(idx))
-    size += list(var_shapes[len(self.index):])
-
-    # initialization
-    NeuGroup.__init__(self, tuple(size), name=name, mode=mode)
-
-
-class DynamicalSystemNS(DynamicalSystem):
-  """Dynamical system without the need to pass shared parameters into ``update()`` function."""
-
-  _pass_shared_args = False
-
-
-class Sequential(DynamicalSystemNS):
-  """A sequential `input-output` module.
-
-  Modules will be added to it in the order they are passed in the
-  constructor. Alternatively, an ``dict`` of modules can be
-  passed in. The ``update()`` method of ``Sequential`` accepts any
-  input and forwards it to the first module it contains. It then
-  "chains" outputs to inputs sequentially for each subsequent module,
-  finally returning the output of the last module.
-
-  The value a ``Sequential`` provides over manually calling a sequence
-  of modules is that it allows treating the whole container as a
-  single module, such that performing a transformation on the
-  ``Sequential`` applies to each of the modules it stores (which are
-  each a registered submodule of the ``Sequential``).
-
-  What's the difference between a ``Sequential`` and a
-  :py:class:`Container`? A ``Container`` is exactly what it
-  sounds like--a container to store :py:class:`DynamicalSystem` s!
-  On the other hand, the layers in a ``Sequential`` are connected
-  in a cascading way.
-
-  Examples
-  --------
-
-  >>> import brainpy as bp
-  >>> import brainpy.math as bm
-  >>>
-  >>> # composing ANN models
-  >>> l = bp.Sequential(bp.layers.Dense(100, 10),
-  >>>                   bm.relu,
-  >>>                   bp.layers.Dense(10, 2))
-  >>> l({}, bm.random.random((256, 100)))
-  >>>
-  >>> # Using Sequential with Dict. This is functionally the
-  >>> # same as the above code
-  >>> l = bp.Sequential(l1=bp.layers.Dense(100, 10),
-  >>>                   l2=bm.relu,
-  >>>                   l3=bp.layers.Dense(10, 2))
-  >>> l({}, bm.random.random((256, 100)))
-
-  Parameters
-  ----------
-  name: str
-    The object name.
-  mode: Mode
-    The object computing context/mode. Default is ``None``.
-  """
-
-  def __init__(
-      self,
-      *modules_as_tuple,
-      name: str = None,
-      mode: bm.Mode = None,
-      **modules_as_dict
-  ):
-    super().__init__(name=name, mode=mode)
-    self._dyn_modules = bm.NodeDict()
-    self._static_modules = dict()
-    i = 0
-    for m in modules_as_tuple + tuple(modules_as_dict.values()):
-      key = self.__format_key(i)
-      if isinstance(m, bm.BrainPyObject):
-        self._dyn_modules[key] = m
-      else:
-        self._static_modules[key] = m
-      i += 1
-    self._num = i
-
-  def __format_key(self, i):
-    return f'l-{i}'
-
-  def __all_nodes(self):
-    nodes = []
-    for i in range(self._num):
-      key = self.__format_key(i)
-      if key not in self._dyn_modules:
-        nodes.append(self._static_modules[key])
-      else:
-        nodes.append(self._dyn_modules[key])
-    return nodes
-
-  def __getitem__(self, key: Union[int, slice, str]):
-    if isinstance(key, str):
-      if key in self._dyn_modules:
-        return self._dyn_modules[key]
-      elif key in self._static_modules:
-        return self._static_modules[key]
-      else:
-        raise KeyError(f'Does not find a component named {key} in\n {str(self)}')
-    elif isinstance(key, slice):
-      return Sequential(*(self.__all_nodes()[key]))
-    elif isinstance(key, int):
-      return self.__all_nodes()[key]
-    elif isinstance(key, (tuple, list)):
-      _all_nodes = self.__all_nodes()
-      return Sequential(*[_all_nodes[k] for k in key])
-    else:
-      raise KeyError(f'Unknown type of key: {type(key)}')
-
-  def __repr__(self):
-    nodes = self.__all_nodes()
-    entries = '\n'.join(f'  [{i}] {tools.repr_object(x)}' for i, x in enumerate(nodes))
-    return f'{self.__class__.__name__}(\n{entries}\n)'
-
-  def update(self, x):
-    """Update function of a sequential model.
-    """
-    for m in self.__all_nodes():
-      x = m(x)
-    return x
-
-
-class NeuGroupNS(NeuGroup):
-  """Base class for neuron group without shared arguments passed."""
-  _pass_shared_args = False
-
```

## brainpy/_src/mixin.py

```diff
@@ -1,75 +1,120 @@
-from typing import Optional, Sequence, Union, Tuple, Callable
+import numbers
+import sys
 from dataclasses import dataclass
-from brainpy import tools, math as bm
+from typing import Union, Dict, Callable, Sequence, Optional, TypeVar
+from typing import (_SpecialForm, _type_check, _remove_dups_flatten)
+
+import jax
+import jax.numpy as jnp
+import numpy as np
+
+from brainpy import math as bm, tools
+from brainpy._src.math.object_transform.naming import get_unique_name
+from brainpy._src.initialize import parameter
+from brainpy.types import ArrayType
+
+if sys.version_info.minor > 8:
+  from typing import (_UnionGenericAlias)
+else:
+  from typing import (_GenericAlias, _tp_cache)
+
+DynamicalSystem = None
 
 __all__ = [
   'MixIn',
   'ParamDesc',
+  'ParamDescInit',
   'AlignPost',
-  'ProjAutoDelay',
+  'AutoDelaySupp',
+  'NoSH',
+  'Container',
+  'TreeNode',
+  'BindCondData',
+  'JointType',
 ]
 
+global_delay_data = dict()
+
 
 class MixIn(object):
+  """Base MixIn object."""
   pass
 
 
-class DelayedInit(object):
-  """Delayed initialization.
-  """
+class ParamDesc(MixIn):
+  """:py:class:`~.MixIn` indicates the function for describing initialization parameters.
 
-  def __init__(
-      self,
-      cls: type,
-      identifier,
-      *args,
-      **kwargs
-  ):
-    self.cls = cls
-    self.args = args
-    self.kwargs = kwargs
-    self._identifier = identifier
+  This mixin enables the subclass has a classmethod ``desc``, which
+  produces an instance of :py:class:`~.ParamDescInit`.
 
-  def __call__(self, *args, **kwargs):
-    return self.cls(*self.args, *args, **self.kwargs, **kwargs)
+  Note this MixIn can be applied in any Python object.
+  """
 
-  def init(self, *args, **kwargs):
-    return self.__call__(*args, **kwargs)
+  not_desc_params: Optional[Sequence[str]] = None
 
   @classmethod
-  def __class_getitem__(cls, item):
-    return cls
+  def desc(cls, *args, **kwargs) -> 'ParamDescInit':
+    return ParamDescInit(cls, *args, **kwargs)
 
 
-class ParamDesc(MixIn):
-  """Parameter description MixIn.
-
-  This mixin enables the subclass has a classmethod ``desc``, which
-  produces an instance of :py:class:`~.DelayedInit`.
+class ParamDescInit(object):
+  """Delayed initialization for parameter describers.
   """
 
-  not_desc_params: Optional[Sequence[str]] = None
+  def __init__(self, cls: type, *desc_tuple, **desc_dict):
+    self.cls = cls
 
-  @classmethod
-  def desc(cls, *args, **kwargs) -> DelayedInit:
-    # cls_args = list(inspect.signature(cls.__init__).parameters.values())[1:]
-    # names = [arg.name for arg in cls_args]
-    # defaults = [arg.default for arg in cls_args]
-    if cls.not_desc_params is not None:
-      repr_kwargs = {k: v for k, v in kwargs.items() if k not in cls.not_desc_params}
+    # arguments
+    self.args = desc_tuple
+    self.kwargs = desc_dict
+
+    # identifier
+    if isinstance(cls, _JointGenericAlias):
+      name = str(cls)
+      repr_kwargs = {k: v for k, v in desc_dict.items()}
     else:
-      repr_kwargs = {k: v for k, v in kwargs.items()}
+      assert isinstance(cls, type)
+      if issubclass(cls, ParamDesc) and (cls.not_desc_params is not None):
+        repr_kwargs = {k: v for k, v in desc_dict.items() if k not in cls.not_desc_params}
+      else:
+        repr_kwargs = {k: v for k, v in desc_dict.items()}
+      name = cls.__name__
     for k in tuple(repr_kwargs.keys()):
       if isinstance(repr_kwargs[k], bm.Variable):
         repr_kwargs[k] = id(repr_kwargs[k])
     repr_args = tools.repr_dict(repr_kwargs)
-    if len(args):
-      repr_args = f"{', '.join([repr(arg) for arg in args])}, {repr_args}"
-    return DelayedInit(cls, f'{cls.__name__}({repr_args})', *args, **kwargs)
+    if len(desc_tuple):
+      repr_args = f"{', '.join([repr(arg) for arg in desc_tuple])}, {repr_args}"
+    self._identifier = f'{name}({repr_args})'
+
+  def __call__(self, *args, **kwargs):
+    return self.cls(*self.args, *args, **self.kwargs, **kwargs)
+
+  def init(self, *args, **kwargs):
+    return self.__call__(*args, **kwargs)
+
+  def __instancecheck__(self, instance):
+    if not isinstance(instance, ParamDescInit):
+      return False
+    if not issubclass(instance.cls, self.cls):
+      return False
+    return True
+
+  @classmethod
+  def __class_getitem__(cls, item: type):
+    return ParamDescInit(item)
+
+  @property
+  def identifier(self):
+    return self._identifier
+
+  @identifier.setter
+  def identifier(self, value):
+    self._identifier = value
 
 
 class AlignPost(MixIn):
   """Align post MixIn.
 
   This class provides a ``add_current()`` function for
   add external currents.
@@ -78,18 +123,496 @@
   def add_current(self, *args, **kwargs):
     raise NotImplementedError
 
 
 @dataclass
 class ReturnInfo:
   size: Sequence[int]
-  axis_names: Sequence[str]
-  batch_or_mode: Optional[Union[int, bm.Mode]]
-  init: Callable
+  axis_names: Optional[Sequence[str]] = None
+  batch_or_mode: Optional[Union[int, bm.Mode]] = None
+  data: Union[Callable, bm.Array, jax.Array] = bm.zeros
+
+  def get_data(self):
+    if isinstance(self.data, Callable):
+      if isinstance(self.batch_or_mode, int):
+        size = (self.batch_or_mode,) + tuple(self.size)
+      elif isinstance(self.batch_or_mode, bm.NonBatchingMode):
+        size = tuple(self.size)
+      elif isinstance(self.batch_or_mode, bm.BatchingMode):
+        size = (self.batch_or_mode.batch_size,) + tuple(self.size)
+      else:
+        size = tuple(self.size)
+      init = self.data(size)
+    elif isinstance(self.data, (bm.Array, jax.Array)):
+      init = self.data
+    else:
+      raise ValueError
+    return init
 
 
-class ProjAutoDelay(MixIn):
-  """Support for automatic delay in synaptic projection :py:class:`~.SynProj`."""
+class AutoDelaySupp(MixIn):
+  """``MixIn`` to support the automatic delay in synaptic projection :py:class:`~.SynProj`."""
+
+  def return_info(self) -> Union[bm.Variable, ReturnInfo]:
+    raise NotImplementedError('Must implement the "return_info()" function.')
+
+
+class NoSH(MixIn):
+  """``MixIn`` to indicate that no shared parameters should be passed into the ``update()`` function."""
+
+  def __init__(self, *args, **kwargs):
+    self._pass_shared_args = False
+
+
+class Container(MixIn):
+  """Container :py:class:`~.MixIn` which wrap a group of objects.
+  """
+  children: bm.node_dict
+
+  def __getitem__(self, item):
+    """Overwrite the slice access (`self['']`). """
+    if item in self.children:
+      return self.children[item]
+    else:
+      raise ValueError(f'Unknown item {item}, we only found {list(self.children.keys())}')
+
+  def __getattr__(self, item):
+    """Overwrite the dot access (`self.`). """
+    if item == 'children':
+      return super().__getattribute__('children')
+    else:
+      children = super().__getattribute__('children')
+      if item in children:
+        return children[item]
+      else:
+        return super().__getattribute__(item)
+
+  def __repr__(self):
+    cls_name = self.__class__.__name__
+    indent = ' ' * len(cls_name)
+    child_str = [tools.repr_context(repr(val), indent) for val in self.children.values()]
+    string = ", \n".join(child_str)
+    return f'{cls_name}({string})'
+
+  def __get_elem_name(self, elem):
+    if isinstance(elem, bm.BrainPyObject):
+      return elem.name
+    else:
+      return get_unique_name('ContainerElem')
+
+  def format_elements(self, child_type: type, *children_as_tuple, **children_as_dict):
+    res = dict()
+
+    # add tuple-typed components
+    for module in children_as_tuple:
+      if isinstance(module, child_type):
+        res[self.__get_elem_name(module)] = module
+      elif isinstance(module, (list, tuple)):
+        for m in module:
+          if not isinstance(m, child_type):
+            raise ValueError(f'Should be instance of {child_type.__name__}. '
+                             f'But we got {type(m)}')
+          res[self.__get_elem_name(m)] = m
+      elif isinstance(module, dict):
+        for k, v in module.items():
+          if not isinstance(v, child_type):
+            raise ValueError(f'Should be instance of {child_type.__name__}. '
+                             f'But we got {type(v)}')
+          res[k] = v
+      else:
+        raise ValueError(f'Cannot parse sub-systems. They should be {child_type.__name__} '
+                         f'or a list/tuple/dict of {child_type.__name__}.')
+    # add dict-typed components
+    for k, v in children_as_dict.items():
+      if not isinstance(v, child_type):
+        raise ValueError(f'Should be instance of {child_type.__name__}. '
+                         f'But we got {type(v)}')
+      res[k] = v
+    return res
+
+  def add_elem(self, **elements):
+    """Add new elements.
+
+    >>> obj = Container()
+    >>> obj.add_elem(a=1.)
+
+    Args:
+      elements: children objects.
+    """
+    # self.check_hierarchies(type(self), **elements)
+    self.children.update(self.format_elements(object, **elements))
+
+
+class TreeNode(MixIn):
+  """Tree node. """
+
+  master_type: type
+
+  def check_hierarchies(self, root, *leaves, **named_leaves):
+    global DynamicalSystem
+    if DynamicalSystem is None:
+      from brainpy._src.dynsys import DynamicalSystem
+
+    for leaf in leaves:
+      if isinstance(leaf, DynamicalSystem):
+        self.check_hierarchy(root, leaf)
+      elif isinstance(leaf, (list, tuple)):
+        self.check_hierarchies(root, *leaf)
+      elif isinstance(leaf, dict):
+        self.check_hierarchies(root, **leaf)
+      else:
+        raise ValueError(f'Do not support {type(leaf)}.')
+    for leaf in named_leaves.values():
+      if not isinstance(leaf, DynamicalSystem):
+        raise ValueError(f'Do not support {type(leaf)}. Must be instance of {DynamicalSystem.__name__}')
+      self.check_hierarchy(root, leaf)
+
+  def check_hierarchy(self, root, leaf):
+    if hasattr(leaf, 'master_type'):
+      master_type = leaf.master_type
+    else:
+      raise ValueError('Child class should define "master_type" to '
+                       'specify the type of the root node. '
+                       f'But we did not found it in {leaf}')
+    if not issubclass(root, master_type):
+      raise TypeError(f'Type does not match. {leaf} requires a master with type '
+                      f'of {leaf.master_type}, but the master now is {root}.')
+
+
+class DelayRegister(MixIn):
+  local_delay_vars: bm.node_dict
+
+  def register_delay(
+      self,
+      identifier: str,
+      delay_step: Optional[Union[int, ArrayType, Callable]],
+      delay_target: bm.Variable,
+      initial_delay_data: Union[Callable, ArrayType, numbers.Number] = None,
+  ):
+    """Register delay variable.
+
+    Parameters
+    ----------
+    identifier: str
+      The delay variable name.
+    delay_step: Optional, int, ArrayType, callable, Initializer
+      The number of the steps of the delay.
+    delay_target: Variable
+      The target variable for delay.
+    initial_delay_data: float, int, ArrayType, callable, Initializer
+      The initializer for the delay data.
+
+    Returns
+    -------
+    delay_step: int, ArrayType
+      The number of the delay steps.
+    """
+    # delay steps
+    if delay_step is None:
+      delay_type = 'none'
+    elif isinstance(delay_step, (int, np.integer, jnp.integer)):
+      delay_type = 'homo'
+    elif isinstance(delay_step, (bm.ndarray, jnp.ndarray, np.ndarray)):
+      if delay_step.size == 1 and delay_step.ndim == 0:
+        delay_type = 'homo'
+      else:
+        delay_type = 'heter'
+        delay_step = bm.asarray(delay_step)
+    elif callable(delay_step):
+      delay_step = parameter(delay_step, delay_target.shape, allow_none=False)
+      delay_type = 'heter'
+    else:
+      raise ValueError(f'Unknown "delay_steps" type {type(delay_step)}, only support '
+                       f'integer, array of integers, callable function, brainpy.init.Initializer.')
+    if delay_type == 'heter':
+      if delay_step.dtype not in [bm.int32, bm.int64]:
+        raise ValueError('Only support delay steps of int32, int64. If your '
+                         'provide delay time length, please divide the "dt" '
+                         'then provide us the number of delay steps.')
+      if delay_target.shape[0] != delay_step.shape[0]:
+        raise ValueError(f'Shape is mismatched: {delay_target.shape[0]} != {delay_step.shape[0]}')
+    if delay_type != 'none':
+      max_delay_step = int(bm.max(delay_step))
+
+    # delay target
+    if delay_type != 'none':
+      if not isinstance(delay_target, bm.Variable):
+        raise ValueError(f'"delay_target" must be an instance of Variable, but we got {type(delay_target)}')
+
+    # delay variable
+    # TODO
+    if delay_type != 'none':
+      if identifier not in global_delay_data:
+        delay = bm.LengthDelay(delay_target, max_delay_step, initial_delay_data)
+        global_delay_data[identifier] = (delay, delay_target)
+        self.local_delay_vars[identifier] = delay
+      else:
+        delay = global_delay_data[identifier][0]
+        if delay is None:
+          delay = bm.LengthDelay(delay_target, max_delay_step, initial_delay_data)
+          global_delay_data[identifier] = (delay, delay_target)
+          self.local_delay_vars[identifier] = delay
+        elif delay.num_delay_step - 1 < max_delay_step:
+          global_delay_data[identifier][0].reset(delay_target, max_delay_step, initial_delay_data)
+    else:
+      if identifier not in global_delay_data:
+        global_delay_data[identifier] = (None, delay_target)
+    return delay_step
+
+  def get_delay_data(
+      self,
+      identifier: str,
+      delay_step: Optional[Union[int, bm.Array, jax.Array]],
+      *indices: Union[int, slice, bm.Array, jax.Array],
+  ):
+    """Get delay data according to the provided delay steps.
+
+    Parameters
+    ----------
+    identifier: str
+      The delay variable name.
+    delay_step: Optional, int, ArrayType
+      The delay length.
+    indices: optional, int, slice, ArrayType
+      The indices of the delay.
+
+    Returns
+    -------
+    delay_data: ArrayType
+      The delay data at the given time.
+    """
+    if delay_step is None:
+      return global_delay_data[identifier][1].value
+
+    if identifier in global_delay_data:
+      if bm.ndim(delay_step) == 0:
+        return global_delay_data[identifier][0](delay_step, *indices)
+      else:
+        if len(indices) == 0:
+          indices = (bm.arange(delay_step.size),)
+        return global_delay_data[identifier][0](delay_step, *indices)
+
+    elif identifier in self.local_delay_vars:
+      if bm.ndim(delay_step) == 0:
+        return self.local_delay_vars[identifier](delay_step)
+      else:
+        if len(indices) == 0:
+          indices = (bm.arange(delay_step.size),)
+        return self.local_delay_vars[identifier](delay_step, *indices)
+
+    else:
+      raise ValueError(f'{identifier} is not defined in delay variables.')
+
+  def update_local_delays(self, nodes: Union[Sequence, Dict] = None):
+    """Update local delay variables.
+
+    This function should be called after updating neuron groups or delay sources.
+    For example, in a network model,
+
+
+    Parameters
+    ----------
+    nodes: sequence, dict
+      The nodes to update their delay variables.
+    """
+    global DynamicalSystem
+    if DynamicalSystem is None:
+      from brainpy._src.dynsys import DynamicalSystem
+
+    # update delays
+    if nodes is None:
+      nodes = tuple(self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique().values())
+    elif isinstance(nodes, dict):
+      nodes = tuple(nodes.values())
+    if not isinstance(nodes, (tuple, list)):
+      nodes = (nodes,)
+    for node in nodes:
+      for name in node.local_delay_vars:
+        delay = global_delay_data[name][0]
+        target = global_delay_data[name][1]
+        delay.update(target.value)
+
+  def reset_local_delays(self, nodes: Union[Sequence, Dict] = None):
+    """Reset local delay variables.
+
+    Parameters
+    ----------
+    nodes: sequence, dict
+      The nodes to Reset their delay variables.
+    """
+    global DynamicalSystem
+    if DynamicalSystem is None:
+      from brainpy._src.dynsys import DynamicalSystem
+
+    # reset delays
+    if nodes is None:
+      nodes = self.nodes(level=1, include_self=False).subset(DynamicalSystem).unique().values()
+    elif isinstance(nodes, dict):
+      nodes = nodes.values()
+    for node in nodes:
+      for name in node.local_delay_vars:
+        delay = global_delay_data[name][0]
+        target = global_delay_data[name][1]
+        delay.reset(target.value)
+
+  def get_delay_var(self, name):
+    return global_delay_data[name]
+
+
+class BindCondData(MixIn):
+  """Bind temporary conductance data.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self._conductance = None
+
+  def bind_cond(self, conductance):
+    self._conductance = conductance
+
+  def unbind_cond(self):
+    self._conductance = None
+
+
+T = TypeVar('T')
+
+
+def get_type(types):
+  class NewType(type):
+    def __instancecheck__(self, other):
+      cls_of_other = other.__class__
+      return all([issubclass(cls_of_other, cls) for cls in types])
+
+  return NewType
+
+
+class _MetaUnionType(type):
+  def __new__(cls, name, bases, dct):
+    if isinstance(bases, type):
+      bases = (bases,)
+    elif isinstance(bases, (list, tuple)):
+      bases = tuple(bases)
+      for base in bases:
+        assert isinstance(base, type), f'Must be type. But got {base}'
+    else:
+      raise TypeError(f'Must be type. But got {bases}')
+    return super().__new__(cls, name, bases, dct)
+
+  def __instancecheck__(self, other):
+    cls_of_other = other.__class__
+    return all([issubclass(cls_of_other, cls) for cls in self.__bases__])
+
+  def __subclasscheck__(self, subclass):
+    return all([issubclass(subclass, cls) for cls in self.__bases__])
+
+
+class UnionType2(MixIn):
+  """Union type for multiple types.
+
+  >>> import brainpy as bp
+  >>>
+  >>> isinstance(bp.dyn.Expon(1), JointType[bp.DynamicalSystem, bp.mixin.ParamDesc, bp.mixin.AutoDelaySupp])
+  """
+
+  @classmethod
+  def __class_getitem__(cls, types: Union[type, Sequence[type]]) -> type:
+    return _MetaUnionType('UnionType', types, {})
 
-  def return_for_delay(self) -> Union[bm.Variable, ReturnInfo]:
-    raise NotImplementedError
 
+if sys.version_info.minor > 8:
+  class _JointGenericAlias(_UnionGenericAlias, _root=True):
+    def __subclasscheck__(self, subclass):
+      return all([issubclass(subclass, cls) for cls in set(self.__args__)])
+
+
+  @_SpecialForm
+  def JointType(self, parameters):
+    """Joint type; JointType[X, Y] means both X and Y.
+
+    To define a union, use e.g. Union[int, str].
+
+    Details:
+    - The arguments must be types and there must be at least one.
+    - None as an argument is a special case and is replaced by `type(None)`.
+    - Unions of unions are flattened, e.g.::
+
+        JointType[JointType[int, str], float] == JointType[int, str, float]
+
+    - Unions of a single argument vanish, e.g.::
+
+        JointType[int] == int  # The constructor actually returns int
+
+    - Redundant arguments are skipped, e.g.::
+
+        JointType[int, str, int] == JointType[int, str]
+
+    - When comparing unions, the argument order is ignored, e.g.::
+
+        JointType[int, str] == JointType[str, int]
+
+    - You cannot subclass or instantiate a union.
+    - You can use Optional[X] as a shorthand for JointType[X, None].
+    """
+    if parameters == ():
+      raise TypeError("Cannot take a Joint of no types.")
+    if not isinstance(parameters, tuple):
+      parameters = (parameters,)
+    msg = "JointType[arg, ...]: each arg must be a type."
+    parameters = tuple(_type_check(p, msg) for p in parameters)
+    parameters = _remove_dups_flatten(parameters)
+    if len(parameters) == 1:
+      return parameters[0]
+    return _JointGenericAlias(self, parameters)
+
+else:
+  class _JointGenericAlias(_GenericAlias, _root=True):
+    def __subclasscheck__(self, subclass):
+      return all([issubclass(subclass, cls) for cls in set(self.__args__)])
+
+
+  class _SpecialForm2(_SpecialForm, _root=True):
+    @_tp_cache
+    def __getitem__(self, parameters):
+      if self._name == 'JointType':
+        if parameters == ():
+          raise TypeError("Cannot take a Joint of no types.")
+        if not isinstance(parameters, tuple):
+          parameters = (parameters,)
+        msg = "JointType[arg, ...]: each arg must be a type."
+        parameters = tuple(_type_check(p, msg) for p in parameters)
+        parameters = _remove_dups_flatten(parameters)
+        if len(parameters) == 1:
+          return parameters[0]
+        return _JointGenericAlias(self, parameters)
+      else:
+        return super().__getitem__(parameters)
+
+
+  JointType = _SpecialForm2(
+    'JointType',
+    doc="""Joint type; JointType[X, Y] means both X and Y.
+  
+    To define a union, use e.g. JointType[int, str].  
+    
+    Details:
+    
+    - The arguments must be types and there must be at least one.
+    - None as an argument is a special case and is replaced by `type(None)`.
+    - Unions of unions are flattened, e.g.::
+  
+        JointType[JointType[int, str], float] == JointType[int, str, float]
+  
+    - Unions of a single argument vanish, e.g.::
+  
+        JointType[int] == int  # The constructor actually returns int
+  
+    - Redundant arguments are skipped, e.g.::
+  
+        JointType[int, str, int] == JointType[int, str]
+  
+    - When comparing unions, the argument order is ignored, e.g.::
+  
+        JointType[int, str] == JointType[str, int]
+  
+    - You cannot subclass or instantiate a union.
+    - You can use Optional[X] as a shorthand for JointType[X, None].
+    """
+  )
```

## brainpy/_src/runners.py

```diff
@@ -1,39 +1,48 @@
 # -*- coding: utf-8 -*-
-
+import functools
+import inspect
 import time
 import warnings
 from collections.abc import Iterable
-from functools import partial
-from typing import Dict, Union, Sequence, Callable, Tuple, Optional
+from typing import Dict, Union, Sequence, Callable, Tuple, Optional, Any
 
 import jax
 import jax.numpy as jnp
 import numpy as np
 import tqdm.auto
 from jax.experimental.host_callback import id_tap
 from jax.tree_util import tree_map, tree_flatten
 
 from brainpy import math as bm, tools
-from brainpy._src.dynsys import DynamicalSystem
 from brainpy._src.context import share
+from brainpy._src.deprecations import _input_deprecate_msg
+from brainpy._src.dynsys import DynamicalSystem
 from brainpy._src.running.runner import Runner
-from brainpy.check import serialize_kwargs
 from brainpy.errors import RunningError
-from brainpy.types import ArrayType, Output, Monitor
-
+from brainpy.types import Output, Monitor
 
 __all__ = [
   'DSRunner',
 ]
 
 SUPPORTED_INPUT_OPS = ['-', '+', '*', '/', '=']
 SUPPORTED_INPUT_TYPE = ['fix', 'iter', 'func']
 
 
+def _call_fun_with_share(f, *args, **kwargs):
+  try:
+    sha = share.get_shargs()
+    inspect.signature(f).bind(sha, *args, **kwargs)
+    warnings.warn(_input_deprecate_msg, UserWarning)
+    return f(sha, *args, **kwargs)
+  except TypeError:
+    return f(*args, **kwargs)
+
+
 def _is_brainpy_array(x):
   return isinstance(x, bm.Array)
 
 
 def check_and_format_inputs(host, inputs):
   """Check inputs and get the formatted inputs for the given population.
 
@@ -74,61 +83,36 @@
       raise RunningError(f'Input operation only supports '
                          f'"{SUPPORTED_INPUT_OPS}", '
                          f'not "{one_input[3]}".')
 
   # 2. get targets and attributes
   # ---------
   inputs_which_found_target = []
-  inputs_not_found_target = []
 
   # checking 1: absolute access
   #    Check whether the input target node is accessible,
   #    and check whether the target node has the attribute
-  nodes = None
   for one_input in inputs:
     key = one_input[0]
     if isinstance(key, bm.Variable):
       real_target = key
     elif isinstance(key, str):
-      if nodes is None:
-        nodes = host.nodes(method='absolute', level=-1, include_self=True)
       splits = key.split('.')
-      target = '.'.join(splits[:-1])
-      key = splits[-1]
-      if target == '':
-        real_target = host
-      else:
-        if target not in nodes:
-          inputs_not_found_target.append(one_input)
-          continue
-        real_target = nodes[target]
-      if not hasattr(real_target, key):
-        raise RunningError(f'Input target key "{key}" is not defined in {real_target}.')
-      real_target = getattr(real_target, key)
+      target = host
+      try:
+        for split in splits:
+          target = getattr(target, split)
+      except AttributeError:
+        raise AttributeError(f'target {target} does not have "{split}"')
+      real_target = target
     else:
       raise RunningError(f'For each input, input[0] must be a string  to '
                          f'specify variable of the target, but we got {key}.')
     inputs_which_found_target.append((real_target,) + tuple(one_input[1:]))
 
-  # checking 2: relative access
-  #    Check whether the input target node is accessible
-  #    and check whether the target node has the attribute
-  if len(inputs_not_found_target):
-    nodes = host.nodes(method='relative', level=-1, include_self=True)
-    for one_input in inputs_not_found_target:
-      splits = one_input[0].split('.')
-      target, key = '.'.join(splits[:-1]), splits[-1]
-      if target not in nodes:
-        raise RunningError(f'Input target "{target}" is not defined in {host}.')
-      real_target = nodes[target]
-      if not hasattr(real_target, key):
-        raise RunningError(f'Input target key "{key}" is not defined in {real_target}.')
-      real_target = getattr(real_target, key)
-      inputs_which_found_target.append((real_target,) + tuple(one_input[1:]))
-
   # 3. format inputs
   # ---------
   formatted_inputs = []
   for one_input in inputs_which_found_target:
     # input value
     data_value = one_input[1]
 
@@ -261,25 +245,25 @@
   monitors: Optional, sequence of str, dict, Monitor
     Variables to monitor.
 
     - A list of string. Like ``monitors=['a', 'b', 'c']``.
     - A list of string with index specification. Like ``monitors=[('a', 1), ('b', [1,3,5]), 'c']``
     - A dict with the explicit monitor target, like: ``monitors={'a': model.spike, 'b': model.V}``
     - A dict with the index specification, like: ``monitors={'a': (model.spike, 0), 'b': (model.V, [1,2])}``
-    - A dict with the callable function, like ``monitors={'a': lambda tdi: model.spike[:5]}``
+    - A dict with the callable function, like ``monitors={'a': lambda: model.spike[:5]}``
 
     .. versionchanged:: 2.3.1
        ``fun_monitors`` are merged into ``monitors``.
   fun_monitors: dict
     Monitoring variables by a dict of callable functions.
     The dict ``key`` should be a string for the later retrieval by ``runner.mon[key]``.
     The dict ``value`` should be a callable function which receives two arguments: ``t`` and ``dt``.
     .. code-block::
-       fun_monitors = {'spike': lambda tdi: model.spike[:10],
-                       'V10': lambda tdi: model.V[10]}
+       fun_monitors = {'spike': lambda: model.spike[:10],
+                       'V10': lambda: model.V[10]}
 
     .. deprecated:: 2.3.1
        Will be removed since version 2.4.0.
   jit: bool, dict
     The JIT settings.
     Using dict is able to set the jit mode at different phase,
     for instance, ``jit={'predict': True, 'fit': False}``.
@@ -338,25 +322,24 @@
       # deprecated
       fun_inputs: Optional[Callable] = None,
       fun_monitors: Optional[Dict[str, Callable]] = None,
   ):
     if not isinstance(target, DynamicalSystem):
       raise RunningError(f'"target" must be an instance of {DynamicalSystem.__name__}, '
                          f'but we got {type(target)}: {target}')
-    super(DSRunner, self).__init__(target=target,
-                                   monitors=monitors,
-                                   fun_monitors=fun_monitors,
-                                   jit=jit,
-                                   progress_bar=progress_bar,
-                                   dyn_vars=dyn_vars,
-                                   numpy_mon_after_run=numpy_mon_after_run)
+    super().__init__(target=target,
+                     monitors=monitors,
+                     fun_monitors=fun_monitors,
+                     jit=jit,
+                     progress_bar=progress_bar,
+                     dyn_vars=dyn_vars,
+                     numpy_mon_after_run=numpy_mon_after_run)
 
     # t0 and i0
     self.i0 = 0
-    self._t0 = t0
     self.t0 = t0
     if data_first_axis is None:
       data_first_axis = 'B' if isinstance(self.target.mode, bm.BatchingMode) else 'T'
     assert data_first_axis in ['B', 'T']
     self.data_first_axis = data_first_axis
 
     # parameters
@@ -373,15 +356,15 @@
     self._fun_inputs = fun_inputs
     if callable(inputs):
       self._inputs = inputs
     else:
       self._inputs = check_and_format_inputs(host=target, inputs=inputs)
 
     # run function
-    self._f_predict_compiled = dict()
+    self._jit_step_func_predict = bm.jit(self._step_func_predict, static_argnames=['shared_args'])
 
     # monitors
     self._memory_efficient = memory_efficient
     if memory_efficient and not numpy_mon_after_run:
       raise ValueError('When setting "gpu_memory_efficient=True", "numpy_mon_after_run" can not be False.')
 
   def __repr__(self):
@@ -392,23 +375,22 @@
             f'{indent}jit={self.jit},\n'
             f'{indent}dt={self.dt},\n'
             f'{indent}data_first_axis={self.data_first_axis})')
 
   def reset_state(self):
     """Reset state of the ``DSRunner``."""
     self.i0 = 0
-    self.t0 = self._t0
 
   def predict(
       self,
       duration: float = None,
-      inputs: Union[ArrayType, Sequence[ArrayType], Dict[str, ArrayType]] = None,
+      inputs: Any = None,
       reset_state: bool = False,
-      shared_args: Dict = None,
       eval_time: bool = False,
+      shared_args: Dict = None,
 
       # deprecated
       inputs_are_batching: bool = None,
   ) -> Union[Output, Tuple[float, Output]]:
     """Running a duration with the given target model. See `.predict()` function
     for more details.
 
@@ -435,18 +417,18 @@
       Whether the ``inputs`` are batching. If `True`, the batching axis is the
       first dimension.
 
       .. deprecated:: 2.3.1
          Will be removed after version 2.4.0.
     reset_state: bool
       Whether reset the model states.
-    shared_args: optional, dict
-      The shared arguments across different layers.
     eval_time: bool
       Whether ro evaluate the running time.
+    shared_args: optional, dict
+      The shared arguments across different layers.
 
     Returns
     -------
     output: ArrayType, dict, sequence
       The model output.
     """
 
@@ -473,127 +455,123 @@
 
     # reset the states of the model and the runner
     if reset_state:
       self.target.reset_state(self._get_input_batch_size(inputs))
       self.reset_state()
 
     # shared arguments and inputs
-    if shared_args is None:
-      shared_args = dict()
-    shared_args['fit'] = shared_args.get('fit', False)
-    shared = tools.DotDict(i=np.arange(num_step, dtype=bm.int_))
-    shared['t'] = shared['i'] * self.dt
-    shared['i'] += self.i0
-    shared['t'] += self.t0
+    indices = np.arange(self.i0, self.i0 + num_step, dtype=bm.int_)
 
     if isinstance(self.target.mode, bm.BatchingMode) and self.data_first_axis == 'B':
       inputs = tree_map(lambda x: jnp.moveaxis(x, 0, 1), inputs)
 
     # build monitor
-    for key in self.mon.var_names:
+    for key in self._monitors.keys():
       self.mon[key] = []  # reshape the monitor items
 
     # init progress bar
     if self.progress_bar:
       self._pbar = tqdm.auto.tqdm(total=num_step)
       self._pbar.set_description(description, refresh=True)
 
     # running
     if eval_time:
       t0 = time.time()
-    with jax.disable_jit(not self.jit['predict']):
-      outputs, hists = self._predict(xs=(shared['t'], shared['i'], inputs), shared_args=shared_args)
+    if inputs is None:
+      inputs = tuple()
+    if not isinstance(inputs, (tuple, list)):
+      inputs = (inputs,)
+    outputs, hists = self._predict(indices, *inputs, shared_args=shared_args)
     if eval_time:
       running_time = time.time() - t0
 
     # close the progress bar
     if self.progress_bar:
       self._pbar.close()
 
     # post-running for monitors
     if self._memory_efficient:
-      self.mon['ts'] = shared['t'] + self.dt
-      for key in self.mon.var_names:
+      self.mon['ts'] = indices * self.dt + self.t0
+      for key in self._monitors.keys():
         self.mon[key] = np.asarray(self.mon[key])
     else:
-      hists['ts'] = shared['t'] + self.dt
+      hists['ts'] = indices * self.dt + self.t0
       if self.numpy_mon_after_run:
         hists = tree_map(lambda a: np.asarray(a), hists, is_leaf=lambda a: isinstance(a, bm.Array))
+      else:
+        hists['ts'] = bm.as_jax(hists['ts'])
       for key in hists.keys():
         self.mon[key] = hists[key]
     self.i0 += num_step
-    self.t0 += (num_step * self.dt if duration is None else duration)
     return outputs if not eval_time else (running_time, outputs)
 
   def run(self, *args, **kwargs) -> Union[Output, Tuple[float, Output]]:
     """Same as :py:func:`~.DSRunner.predict`.
     """
     return self.predict(*args, **kwargs)
 
   def __call__(self, *args, **kwargs) -> Union[Output, Tuple[float, Output]]:
     """Same as :py:func:`~.DSRunner.predict`.
     """
     return self.predict(*args, **kwargs)
 
-  def _predict(
-      self,
-      xs: Sequence,
-      shared_args: Dict = None,
-  ) -> Union[Output, Monitor]:
+  def _predict(self, indices, *xs, shared_args=None) -> Union[Output, Monitor]:
     """Predict the output according to the inputs.
 
     Parameters
     ----------
     xs: sequence
-      Must be a tuple/list of data, including `(times, indices, inputs)`.
       If `inputs` is not None, it should be a tensor with the shape of
       :math:`(num_time, ...)`.
     shared_args: optional, dict
       The shared keyword arguments.
 
     Returns
     -------
     outputs, hists
       A tuple of pair of (outputs, hists).
     """
-    _predict_func = self._get_f_predict(shared_args)
-    outs_and_mons = _predict_func(xs)
+    if shared_args is None:
+      shared_args = dict()
+    shared_args = tools.DotDict(shared_args)
+
+    outs_and_mons = self._fun_predict(indices, *xs, shared_args=shared_args)
     if isinstance(self.target.mode, bm.BatchingMode) and self.data_first_axis == 'B':
       outs_and_mons = tree_map(lambda x: jnp.moveaxis(x, 0, 1) if x.ndim >= 2 else x,
                                outs_and_mons)
     return outs_and_mons
 
-  def _step_func_monitor(self, shared):
+  def _step_func_monitor(self):
     res = dict()
     for key, val in self._monitors.items():
       if callable(val):
-        res[key] = val(shared)
+        res[key] = _call_fun_with_share(val)
       else:
         (variable, idx) = val
         if idx is None:
           res[key] = variable.value
         else:
           res[key] = variable[bm.as_jax(idx)]
     return res
 
-  def _step_func_input(self, shared):
+  def _step_func_input(self):
     if self._fun_inputs is not None:
-      self._fun_inputs(shared)
+      self._fun_inputs(share.get_shargs())
     if callable(self._inputs):
-      self._inputs(shared)
+      _call_fun_with_share(self._inputs)
     else:
       for ops, values in self._inputs['fixed'].items():
         for var, data in values:
           _f_ops(ops, var, data)
       for ops, values in self._inputs['array'].items():
         for var, data in values:
-          _f_ops(ops, var, data[shared['i']])
+          _f_ops(ops, var, data[share['i']])
       for ops, values in self._inputs['functional'].items():
         for var, data in values:
-          _f_ops(ops, var, data(shared))
+          _f_ops(ops, var, _call_fun_with_share(data))
       for ops, values in self._inputs['iterated'].items():
         for var, data in values:
           _f_ops(ops, var, next(data))
 
   def _get_input_batch_size(self, xs=None) -> Optional[int]:
     """Get the batch size in the given input data."""
     if xs is None:
@@ -632,71 +610,54 @@
     else:
       raise ValueError
 
   def _step_mon_on_cpu(self, args, transforms):
     for key, val in args.items():
       self.mon[key].append(val)
 
-  def _step_func_predict(self, shared_args, t, i, x):
+  def _step_func_predict(self, i, *x, shared_args=None):
     # input step
-    shared = tools.DotDict(t=t, i=i, dt=self.dt)
-    shared.update(shared_args)
-    share.save(**shared)
-    self._step_func_input(shared)
+    if shared_args is not None:
+      assert isinstance(shared_args, dict)
+      share.save(**shared_args)
+    share.save(t=self.t0 + i * self.dt, i=i, dt=self.dt)
+    self._step_func_input()
 
     # dynamics update step
-    args = () if x is None else (x,)
-    out = self.target(*args)
+    out = self.target(*x)
 
     # monitor step
-    shared['t'] += self.dt
-    mon = self._step_func_monitor(shared)
+    mon = self._step_func_monitor()
 
     # finally
     if self.progress_bar:
       id_tap(lambda *arg: self._pbar.update(), ())
-    share.clear_shargs()
+    # share.clear_shargs()
     self.target.clear_input()
 
     if self._memory_efficient:
       id_tap(self._step_mon_on_cpu, mon)
       return out, None
     else:
       return out, mon
 
-  def _get_f_predict(self, shared_args: Dict = None):
-    if shared_args is None:
-      shared_args = dict()
-
-    shared_kwargs_str = serialize_kwargs(shared_args)
-    if shared_kwargs_str not in self._f_predict_compiled:
-
-      if self._memory_efficient:
-        _jit_step = bm.jit(partial(self._step_func_predict, shared_args))
-
-        def run_func(all_inputs):
-          outs = None
-          times, indices, xs = all_inputs
-          for i in range(times.shape[0]):
-            out, _ = _jit_step(times[i], indices[i], tree_map(lambda a: a[i], xs))
-            if outs is None:
-              outs = tree_map(lambda a: [], out)
-            outs = tree_map(lambda a, o: o.append(a), out, outs)
-          outs = tree_map(lambda a: bm.as_jax(a), outs)
-          return outs, None
-
+  def _fun_predict(self, indices, *inputs, shared_args=None):
+    if self._memory_efficient:
+      if self.jit['predict']:
+        run_fun = self._jit_step_func_predict
       else:
-        step = partial(self._step_func_predict, shared_args)
-
-        def run_func(all_inputs):
-          return bm.for_loop(step, all_inputs, jit=self.jit['predict'])
-
-      self._f_predict_compiled[shared_kwargs_str] = run_func
-
-    return self._f_predict_compiled[shared_kwargs_str]
-
-  def __del__(self):
-    if hasattr(self, '_f_predict_compiled'):
-      for key in tuple(self._f_predict_compiled.keys()):
-        self._f_predict_compiled.pop(key)
-    super(DSRunner, self).__del__()
+        run_fun = self._step_func_predict
 
+      outs = None
+      for i in range(indices.shape[0]):
+        out, _ = run_fun(indices[i], *tree_map(lambda a: a[i], inputs), shared_args=shared_args)
+        if outs is None:
+          outs = tree_map(lambda a: [], out)
+        outs = tree_map(lambda a, o: o.append(a), out, outs)
+      outs = tree_map(lambda a: bm.as_jax(a), outs)
+      return outs, None
+
+    else:
+      return bm.for_loop(self._step_func_predict,
+                         (indices, *inputs),
+                         jit=self.jit['predict'],
+                         unroll_kwargs={'shared_args': shared_args})
```

## brainpy/_src/transform.py

```diff
@@ -1,26 +1,27 @@
 # -*- coding: utf-8 -*-
+
 import functools
 from typing import Union, Optional, Dict, Sequence
 
 import jax.numpy as jnp
 from jax.tree_util import tree_flatten, tree_unflatten, tree_map
 
 from brainpy import tools, math as bm
 from brainpy._src.context import share
+from brainpy._src.dynsys import DynamicalSystem
 from brainpy.check import is_float, is_integer
 from brainpy.types import PyTree
-from brainpy._src.dynsys import DynamicalSystem, DynamicalSystemNS
 
 __all__ = [
   'LoopOverTime',
 ]
 
 
-class LoopOverTime(DynamicalSystemNS):
+class LoopOverTime(DynamicalSystem):
   """Transform a single step :py:class:`~.DynamicalSystem`
   into a multiple-step forward propagation :py:class:`~.BrainPyObject`.
 
   .. note::
 
      This object transforms a :py:class:`~.DynamicalSystem` into a :py:class:`~.BrainPyObject`.
```

## brainpy/_src/analysis/highdim/slow_points.py

```diff
@@ -1,27 +1,31 @@
 # -*- coding: utf-8 -*-
 
+import inspect
 import math
 import time
+import warnings
 from typing import Callable, Union, Dict, Sequence, Tuple
 
 import jax.numpy as jnp
 import numpy as np
 import jax
 from jax.scipy.optimize import minimize
 from jax.tree_util import tree_flatten, tree_map
 
 import brainpy._src.math as bm
 from brainpy import optim, losses
 from brainpy._src.analysis import utils, base, constants
 from brainpy._src.dynsys import DynamicalSystem
+from brainpy._src.context import share
 from brainpy._src.runners import check_and_format_inputs, _f_ops
-from brainpy._src.tools.dicts import DotDict
 from brainpy.errors import AnalyzerError, UnsupportedError
 from brainpy.types import ArrayType
+from brainpy._src.deprecations import _input_deprecate_msg
+
 
 __all__ = [
   'SlowPointFinder',
 ]
 
 F_OPT_SOLVER = 'function_for_opt_solver'
 F_GRADIENT_DESCENT = 'function_for_gradient_descent'
@@ -119,15 +123,15 @@
       target_vars: Dict[str, bm.Variable] = None,
       excluded_vars: Union[Sequence[bm.Variable], Dict[str, bm.Variable]] = None,
 
       # deprecated
       f_loss_batch: Callable = None,
       fun_inputs: Callable = None,
   ):
-    super(SlowPointFinder, self).__init__()
+    super().__init__()
 
     # static arguments
     if not isinstance(args, tuple):
       raise ValueError(f'args must be an instance of tuple, but we got {type(args)}')
     self.args = args
 
     # update function
@@ -510,15 +514,15 @@
       num_fps = self._fixed_points.shape[0]
     if num_fps <= 1:
       return
 
     # Compute pairwise distances between all fixed points.
     distances = np.asarray(utils.euclidean_distance_jax(self.fixed_points, num_fps))
 
-    # Find second smallest element in each column of the pairwise distance matrix.
+    # Find the second smallest element in each column of the pairwise distance matrix.
     # This corresponds to the closest neighbor for each fixed point.
     closest_neighbor = np.partition(distances, kth=1, axis=0)[1]
 
     # Return data with outliers removed and indices of kept datapoints.
     keep_ids = np.where(closest_neighbor < tolerance)[0]
     self._fixed_points = tree_map(lambda a: a[keep_ids], self._fixed_points)
     self._selected_ids = self._selected_ids[keep_ids]
@@ -632,29 +636,34 @@
         L = np.linalg.pinv(eig_vectors).T  # as columns
         L = L[:, indices]
       decompositions.append({'eig_values': eig_values[indices],
                              'R': eig_vectors[:, indices],
                              'L': L})
     return decompositions
 
-  def _step_func_input(self, shared):
+  def _step_func_input(self):
     if self._inputs is None:
       return
     elif callable(self._inputs):
-      self._inputs(shared)
+      try:
+        ba = inspect.signature(self._inputs).bind(dict())
+        self._inputs(share.get_shargs())
+        warnings.warn(_input_deprecate_msg, UserWarning)
+      except TypeError:
+        self._inputs()
     else:
       for ops, values in self._inputs['fixed'].items():
         for var, data in values:
           _f_ops(ops, var, data)
       for ops, values in self._inputs['array'].items():
         if len(values) > 0:
           raise UnsupportedError
       for ops, values in self._inputs['functional'].items():
         for var, data in values:
-          _f_ops(ops, var, data(shared))
+          _f_ops(ops, var, data(share.get_shargs()))
       for ops, values in self._inputs['iterated'].items():
         if len(values) > 0:
           raise UnsupportedError
 
   def _get_f_eval_loss(self, ):
     name = 'f_eval_loss'
     if name not in self._opt_functions:
@@ -728,34 +737,34 @@
   def _generate_ds_cell_function(
       self, target,
       t: float = None,
       dt: float = None,
   ):
     if dt is None: dt = bm.get_dt()
     if t is None: t = 0.
-    shared = DotDict(t=t, dt=dt, i=0)
 
     def f_cell(h: Dict):
+      share.save(t=t, i=0, dt=dt)
+
       # update target variables
       for k, v in self.target_vars.items():
         v.value = (bm.asarray(h[k], dtype=v.dtype)
                    if v.batch_axis is None else
                    bm.asarray(bm.expand_dims(h[k], axis=v.batch_axis), dtype=v.dtype))
 
       # update excluded variables
       for k, v in self.excluded_vars.items():
         v.value = self.excluded_data[k]
 
       # add inputs
       target.clear_input()
-      self._step_func_input(shared)
+      self._step_func_input()
 
       # call update functions
-      args = (shared,) + self.args
-      target(*args)
+      target(*self.args)
 
       # get new states
       new_h = {k: (v.value if (v.batch_axis is None) else jnp.squeeze(v.value, axis=v.batch_axis))
                for k, v in self.target_vars.items()}
       return new_h
 
     return f_cell
```

## brainpy/_src/analysis/lowdim/lowdim_analyzer.py

```diff
@@ -95,15 +95,16 @@
     self.target_vars = Collector(target_vars)
     self.target_var_names = list(self.target_vars.keys())  # list of target vars
     for key in self.target_vars.keys():
       if key not in self.model.variables:
         raise errors.AnalyzerError(f'{key} is not a dynamical variable in {self.model}.')
       value = self.target_vars[key]
       if value[0] > value[1]:
-        raise errors.AnalyzerError(f'The range of variable {key} is reversed, which means {value[0]} should be smaller than {value[1]}.')
+        raise errors.AnalyzerError(
+          f'The range of variable {key} is reversed, which means {value[0]} should be smaller than {value[1]}.')
 
     # fixed variables
     # ----------------
     if fixed_vars is None:
       fixed_vars = dict()
     if not isinstance(fixed_vars, dict):
       raise errors.AnalyzerError('"fixed_vars" must be a dict with the format '
@@ -242,15 +243,15 @@
 
       {dx \over dt} = f(x, t)
 
   Actually, the analysis for 1D system is purely numerically.
   """
 
   def __init__(self, *args, **kwargs):
-    super(Num1DAnalyzer, self).__init__(*args, **kwargs)
+    super().__init__(*args, **kwargs)
     self.x_var = self.target_var_names[0]
     if len(self.target_vars) < 1:
       raise errors.AnalyzerError(f'{Num1DAnalyzer.__name__} only supports dynamical system '
                                  f'with >= 1 variables. But we got {len(self.target_vars)} '
                                  f'variables in {self.model}.')
 
   @property
@@ -403,15 +404,15 @@
 
       {dx \over dt} = fx(x, t, y)
 
       {dy \over dt} = fy(y, t, x)
   """
 
   def __init__(self, *args, **kwargs):
-    super(Num2DAnalyzer, self).__init__(*args, **kwargs)
+    super().__init__(*args, **kwargs)
     if len(self.target_vars) < 2:
       raise errors.AnalyzerError(f'{Num1DAnalyzer.__name__} only supports dynamical system '
                                  f'with >= 2 variables. But we got {len(self.target_vars)} '
                                  f'variables in {self.model}.')
     self.y_var = self.target_var_names[1]
 
   @property
@@ -1024,15 +1025,15 @@
       all_ids = jnp.concatenate(all_ids)
       all_args = tuple(jnp.concatenate(args) for args in all_args)
       return all_fps, all_ids, all_args
 
 
 class Num3DAnalyzer(Num2DAnalyzer):
   def __init__(self, *args, **kwargs):
-    super(Num3DAnalyzer, self).__init__(*args, **kwargs)
+    super().__init__(*args, **kwargs)
     if len(self.target_vars) < 3:
       raise errors.AnalyzerError(f'{Num1DAnalyzer.__name__} only supports dynamical system '
                                  f'with >= 3 variables. But we got {len(self.target_vars)} '
                                  f'variables in {self.model}.')
     self.z_var = self.target_var_names[2]
 
   @property
@@ -1041,11 +1042,7 @@
     if C.F_fz not in self.analyzed_results:
       variables, arguments = utils.get_args(self.model.f_derivatives[self.z_var])
       wrapper = utils.std_derivative(arguments, self.target_var_names, self.target_par_names)
       f = wrapper(self.model.f_derivatives[self.z_var])
       f = partial(f, **(self.pars_update + self.fixed_vars))
       self.analyzed_results[C.F_fz] = jax.jit(f, device=self.jit_device)
     return self.analyzed_results[C.F_fz]
-
-  def fz_signs(self, pars=(), cache=False):
-    xyz = tuple(self.resolutions.values())
-    return utils.get_sign2(self.F_fz, *xyz, args=pars)
```

## brainpy/_src/analysis/lowdim/lowdim_bifurcation.py

```diff
@@ -27,21 +27,21 @@
   """Bifurcation analysis of 1D system.
 
   Using this class, we can make co-dimension1 or co-dimension2 bifurcation analysis.
   """
 
   def __init__(self, model, target_pars, target_vars, fixed_vars=None,
                pars_update=None, resolutions=None, options=None):
-    super(Bifurcation1D, self).__init__(model=model,
-                                        target_pars=target_pars,
-                                        target_vars=target_vars,
-                                        fixed_vars=fixed_vars,
-                                        pars_update=pars_update,
-                                        resolutions=resolutions,
-                                        options=options)
+    super().__init__(model=model,
+                     target_pars=target_pars,
+                     target_vars=target_vars,
+                     fixed_vars=fixed_vars,
+                     pars_update=pars_update,
+                     resolutions=resolutions,
+                     options=options)
 
     if len(self.target_pars) == 0:
       raise ValueError
 
   @property
   def F_vmap_dfxdx(self):
     if C.F_vmap_dfxdx not in self.analyzed_results:
@@ -142,21 +142,21 @@
   """Bifurcation analysis of 2D system.
 
   Using this class, we can make co-dimension1 or co-dimension2 bifurcation analysis.
   """
 
   def __init__(self, model, target_pars, target_vars, fixed_vars=None,
                pars_update=None, resolutions=None, options=None):
-    super(Bifurcation2D, self).__init__(model=model,
-                                        target_pars=target_pars,
-                                        target_vars=target_vars,
-                                        fixed_vars=fixed_vars,
-                                        pars_update=pars_update,
-                                        resolutions=resolutions,
-                                        options=options)
+    super().__init__(model=model,
+                     target_pars=target_pars,
+                     target_vars=target_vars,
+                     fixed_vars=fixed_vars,
+                     pars_update=pars_update,
+                     resolutions=resolutions,
+                     options=options)
 
     if len(self.target_pars) == 0:
       raise ValueError
 
     self._fixed_points = None
 
   @property
@@ -454,21 +454,21 @@
       fast_vars: dict,
       slow_vars: dict,
       fixed_vars: dict = None,
       pars_update: dict = None,
       resolutions=None,
       options: dict = None
   ):
-    super(FastSlow1D, self).__init__(model=model,
-                                     target_pars=slow_vars,
-                                     target_vars=fast_vars,
-                                     fixed_vars=fixed_vars,
-                                     pars_update=pars_update,
-                                     resolutions=resolutions,
-                                     options=options)
+    super().__init__(model=model,
+                     target_pars=slow_vars,
+                     target_vars=fast_vars,
+                     fixed_vars=fixed_vars,
+                     pars_update=pars_update,
+                     resolutions=resolutions,
+                     options=options)
 
     # standard integrators
     self._std_integrators = dict()
     for key, intg in self.model.name2integral.items():
       wrap_x = utils.std_derivative(utils.get_args(self.model.name2derivative[key])[1],
                                     self.target_var_names + self.target_par_names, [])
       self._std_integrators[key] = partial(wrap_x(self.model.name2integral[key]),
@@ -545,21 +545,21 @@
       fast_vars: dict,
       slow_vars: dict,
       fixed_vars: dict = None,
       pars_update: dict = None,
       resolutions=0.1,
       options: dict = None
   ):
-    super(FastSlow2D, self).__init__(model=model,
-                                     target_pars=slow_vars,
-                                     target_vars=fast_vars,
-                                     fixed_vars=fixed_vars,
-                                     pars_update=pars_update,
-                                     resolutions=resolutions,
-                                     options=options)
+    super().__init__(model=model,
+                     target_pars=slow_vars,
+                     target_vars=fast_vars,
+                     fixed_vars=fixed_vars,
+                     pars_update=pars_update,
+                     resolutions=resolutions,
+                     options=options)
     # standard integrators
     self._std_integrators = dict()
     for key, intg in self.model.name2integral.items():
       wrap_x = utils.std_derivative(utils.get_args(self.model.name2derivative[key])[1],
                                     self.target_var_names + self.target_par_names, [])
       self._std_integrators[key] = partial(wrap_x(self.model.name2integral[key]),
                                            **(self.pars_update + self.fixed_vars))
```

## brainpy/_src/analysis/lowdim/lowdim_phase_plane.py

```diff
@@ -51,21 +51,21 @@
                target_pars=None,
                pars_update=None,
                resolutions=None,
                **kwargs):
     if (target_pars is not None) and len(target_pars) > 0:
       raise errors.AnalyzerError(f'Phase plane analysis does not support "target_pars". '
                                  f'While we detect "target_pars={target_pars}".')
-    super(PhasePlane1D, self).__init__(model=model,
-                                       target_vars=target_vars,
-                                       fixed_vars=fixed_vars,
-                                       target_pars=target_pars,
-                                       pars_update=pars_update,
-                                       resolutions=resolutions,
-                                       **kwargs)
+    super().__init__(model=model,
+                     target_vars=target_vars,
+                     fixed_vars=fixed_vars,
+                     target_pars=target_pars,
+                     pars_update=pars_update,
+                     resolutions=resolutions,
+                     **kwargs)
     # utils.output(f'I am {PhasePlane1D.__name__}.')
 
   def plot_vector_field(self, show=False, with_plot=True, with_return=False):
     """Plot the vector filed."""
     global pyplot
     if pyplot is None: from matplotlib import pyplot
     utils.output('I am creating the vector field ...')
@@ -146,21 +146,21 @@
                target_pars=None,
                pars_update=None,
                resolutions=None,
                **kwargs):
     if (target_pars is not None) and len(target_pars) > 0:
       raise errors.AnalyzerError(f'Phase plane analysis does not support "target_pars". '
                                  f'While we detect "target_pars={target_pars}".')
-    super(PhasePlane2D, self).__init__(model=model,
-                                       target_vars=target_vars,
-                                       fixed_vars=fixed_vars,
-                                       target_pars=target_pars,
-                                       pars_update=pars_update,
-                                       resolutions=resolutions,
-                                       **kwargs)
+    super().__init__(model=model,
+                     target_vars=target_vars,
+                     fixed_vars=fixed_vars,
+                     target_pars=target_pars,
+                     pars_update=pars_update,
+                     resolutions=resolutions,
+                     **kwargs)
 
   @property
   def F_vmap_brentq_fy(self):
     if C.F_vmap_brentq_fy not in self.analyzed_results:
       f_opt = jax.jit(vmap(utils.jax_brentq(self.F_fy)))
       self.analyzed_results[C.F_vmap_brentq_fy] = f_opt
     return self.analyzed_results[C.F_vmap_brentq_fy]
@@ -247,27 +247,27 @@
     xy_values_in_fx, = self._get_fx_nullcline_points(coords=x_coord, tol=tol_nullcline)
     x_values_in_fx = np.asarray(xy_values_in_fx[:, 0])
     y_values_in_fx = np.asarray(xy_values_in_fx[:, 1])
 
     if with_plot:
       if x_style is None:
         x_style = dict(color='cornflowerblue', alpha=.7, fmt='.')
-      line_args = (x_style.pop('fmt'), ) if 'fmt' in x_style else tuple()
+      line_args = (x_style.pop('fmt'),) if 'fmt' in x_style else tuple()
       pyplot.plot(x_values_in_fx, y_values_in_fx, *line_args, **x_style, label=f"{self.x_var} nullcline")
 
     # Nullcline of the y variable
     utils.output('I am computing fy-nullcline ...')
     xy_values_in_fy, = self._get_fy_nullcline_points(coords=y_coord, tol=tol_nullcline)
     x_values_in_fy = np.asarray(xy_values_in_fy[:, 0])
     y_values_in_fy = np.asarray(xy_values_in_fy[:, 1])
 
     if with_plot:
       if y_style is None:
         y_style = dict(color='lightcoral', alpha=.7, fmt='.')
-      line_args = (y_style.pop('fmt'), ) if 'fmt' in y_style else tuple()
+      line_args = (y_style.pop('fmt'),) if 'fmt' in y_style else tuple()
       pyplot.plot(x_values_in_fy, y_values_in_fy, *line_args, **y_style, label=f"{self.y_var} nullcline")
 
     if with_plot:
       pyplot.xlabel(self.x_var)
       pyplot.ylabel(self.y_var)
       scale = (self.lim_scale - 1.) / 2
       pyplot.xlim(*utils.rescale(self.target_vars[self.x_var], scale=scale))
```

## brainpy/_src/analysis/utils/model.py

```diff
@@ -1,14 +1,15 @@
 # -*- coding: utf-8 -*-
 
 
 from brainpy._src.math.object_transform import Variable
 from brainpy._src.math.environment import get_float
 from brainpy._src.math.interoperability import as_jax
 from brainpy._src.dynsys import DynamicalSystem
+from brainpy._src.context import share
 from brainpy._src.runners import DSRunner
 from brainpy._src.integrators.base import Integrator
 from brainpy._src.integrators.joint_eq import JointEq
 from brainpy._src.integrators.ode.base import ODEIntegrator
 from brainpy._src.integrators.ode.generic import odeint
 from brainpy.errors import AnalyzerError, UnsupportedError
 
@@ -122,24 +123,20 @@
     assert isinstance(pars, dict)
     self.pars = [as_jax(v, dtype=get_float()) for k, v in pars.items()]
 
     # integrals
     self.integrals = integrals
 
     # runner
-    self.runner = DSRunner(self,
-                           monitors=list(initial_vars.keys()),
-                           dyn_vars=self.vars().unique(),
-                           dt=dt,
-                           progress_bar=False)
+    self.runner = DSRunner(self, monitors=list(initial_vars.keys()), dt=dt, progress_bar=False)
 
-  def update(self, sha):
+  def update(self):
     all_vars = list(self.implicit_vars.values())
     for key, intg in self.integrals.items():
-      self.implicit_vars[key].update(intg(*all_vars, *self.pars, dt=sha['dt']))
+      self.implicit_vars[key].update(intg(*all_vars, *self.pars, dt=share['dt']))
 
   def __getattr__(self, item):
     child_vars = super(TrajectModel, self).__getattribute__('implicit_vars')
     if item in child_vars:
       return child_vars[item]
     else:
       return super(TrajectModel, self).__getattribute__(item)
```

## brainpy/_src/connect/base.py

```diff
@@ -421,15 +421,15 @@
           return bm.as_jax(r[0], dtype=IDX_DTYPE), bm.as_jax(r[1], dtype=IDX_DTYPE)
         elif CONN_MAT in structures and _has_mat_imp:
           return bm.as_jax(self.build_mat(), dtype=MAT_DTYPE)
         elif PRE_IDS in structures and _has_coo_imp:
           return bm.as_jax(self.build_coo()[0], dtype=IDX_DTYPE)
         elif POST_IDS in structures and _has_coo_imp:
           return bm.as_jax(self.build_coo()[1], dtype=IDX_DTYPE)
-        elif COO in structures and not _has_coo_imp:
+        elif COO in structures and _has_coo_imp:
           return bm.as_jax(self.build_coo(), dtype=IDX_DTYPE)
 
       elif len(structures) == 2:
         if (PRE_IDS in structures and POST_IDS in structures and _has_coo_imp):
           r = self.build_coo()
           if structures[0] == PRE_IDS:
             return bm.as_jax(r[0], dtype=IDX_DTYPE), bm.as_jax(r[1], dtype=IDX_DTYPE)
@@ -722,15 +722,15 @@
   if data is None:
     return pre_ids_new, indptr_new
   else:
     data_new = data[sort_ids]
     return pre_ids_new, indptr_new, data_new
 
 
-def visualizeMat(mat, description):
+def visualizeMat(mat, description='Untitled'):
   try:
     import seaborn as sns
     import matplotlib.pyplot as plt
   except (ModuleNotFoundError, ImportError):
     print('Please install seaborn and matplotlib for this function')
     return
   sns.heatmap(mat, cmap='viridis')
```

## brainpy/_src/connect/custom_conn.py

```diff
@@ -1,9 +1,9 @@
 # -*- coding: utf-8 -*-
-
+import jax
 import jax.numpy as jnp
 import numpy as np
 
 from brainpy import math as bm
 from brainpy import tools
 from brainpy.errors import ConnectorError
 from .base import *
@@ -18,15 +18,15 @@
 
 class MatConn(TwoEndConnector):
   """Connector built from the dense connection matrix."""
 
   def __init__(self, conn_mat, **kwargs):
     super(MatConn, self).__init__(**kwargs)
 
-    assert isinstance(conn_mat, (np.ndarray, bm.Array, jnp.ndarray)) and conn_mat.ndim == 2
+    assert isinstance(conn_mat, (np.ndarray, bm.Array, jax.Array)) and conn_mat.ndim == 2
     self.pre_num, self.post_num = conn_mat.shape
     self.pre_size, self.post_size = (self.pre_num,), (self.post_num,)
 
     self.conn_mat = jnp.asarray(conn_mat).astype(MAT_DTYPE)
 
   def __call__(self, pre_size, post_size):
     assert self.pre_num == tools.size2num(pre_size)
```

## brainpy/_src/connect/random_conn.py

```diff
@@ -1,13 +1,14 @@
 # -*- coding: utf-8 -*-
 from functools import partial
 from typing import Optional
 
 from jax import vmap, jit, numpy as jnp
 import numpy as np
+from numba import njit, prange
 
 import brainpy.math as bm
 from brainpy.errors import ConnectorError
 from brainpy.tools import numba_seed, numba_jit, numba_range, format_seed
 from brainpy._src.tools.package import SUPPORT_NUMBA
 from .base import *
 
@@ -679,50 +680,76 @@
     self._connect = numba_jit(_random_subset)
 
   def __repr__(self):
     return (f'{self.__class__.__name__}(m={self.m}, '
             f'directed={self.directed}, '
             f'seed={self.seed})')
 
-  def build_conn(self):
+  def build_mat(self, isOptimized=True):
     assert self.pre_num == self.post_num
 
     # seed
-    self.seed = self.rng.randint(1, int(1e7))
+    self.rng = np.random.RandomState(self.seed)
     numba_seed(self.seed)
 
     num_node = self.pre_num
     if self.m < 1 or self.m >= num_node:
       raise ConnectorError(f"BarabsiAlbert network must have m >= 1 and "
                            f"m < n, while m = {self.m} and n = {num_node}")
 
     # Add m initial nodes (m0 in barabasi-speak)
     conn = np.zeros((num_node, num_node), dtype=MAT_DTYPE)
     # Target nodes for new edges
     targets = list(range(self.m))
     # List of existing nodes, with nodes repeated once for each adjacent edge
-    repeated_nodes = []
+
+    if not isOptimized:
+      repeated_nodes = []
+      # Start adding the other n-m nodes. The first node is m.
+      source = self.m
+      while source < num_node:
+        # Add edges to m nodes from the source.
+        origins = [source] * self.m
+        conn[origins, targets] = True
+        if not self.directed:
+          conn[targets, origins] = True
+        # Add one node to the list for each new edge just created.
+        repeated_nodes.extend(targets)
+        # And the new node "source" has m edges to add to the list.
+        repeated_nodes.extend([source] * self.m)
+        # Now choose m unique nodes from the existing nodes
+        # Pick uniformly from repeated_nodes (preferential attachment)
+        targets = list(self._connect(np.asarray(repeated_nodes), self.m))
+        source += 1
+      return conn
+
+    # List of existing nodes, with nodes repeated once for each adjacent edge
+    # Preallocate repeated_nodes as a numpy array
+    repeated_nodes = np.empty(2 * num_node * self.m, dtype=int)
+    size_repeated_nodes = 0
     # Start adding the other n-m nodes. The first node is m.
     source = self.m
     while source < num_node:
       # Add edges to m nodes from the source.
       origins = [source] * self.m
       conn[origins, targets] = True
       if not self.directed:
         conn[targets, origins] = True
       # Add one node to the list for each new edge just created.
-      repeated_nodes.extend(targets)
+      repeated_nodes[size_repeated_nodes:size_repeated_nodes + self.m] = targets
+      size_repeated_nodes += self.m
       # And the new node "source" has m edges to add to the list.
-      repeated_nodes.extend([source] * self.m)
+      repeated_nodes[size_repeated_nodes:size_repeated_nodes + self.m] = source
+      size_repeated_nodes += self.m
       # Now choose m unique nodes from the existing nodes
       # Pick uniformly from repeated_nodes (preferential attachment)
-      targets = list(self._connect(np.asarray(repeated_nodes), self.m))
+      targets = list(self._connect(repeated_nodes[:size_repeated_nodes], self.m))
       source += 1
 
-    return 'mat', conn
+    return conn
 
 
 class ScaleFreeBADual(TwoEndConnector):
   r"""Build a random graph according to the dual BarabsiAlbert preferential
   attachment model.
 
   A graph of :math::`num\_node` nodes is grown by attaching new nodes each with either $m_1$
@@ -769,58 +796,90 @@
 
     self._connect = numba_jit(_random_subset)
 
   def __repr__(self):
     return (f'{self.__class__.__name__}(m1={self.m1}, m2={self.m2}, '
             f'p={self.p}, directed={self.directed}, seed={self.seed})')
 
-  def build_conn(self):
+  def build_mat(self, isOptimized=True):
     assert self.pre_num == self.post_num
     # seed
-    self.seed = self.rng.randint(1, int(1e7))
+    self.rng = np.random.RandomState(self.seed)
     numba_seed(self.seed)
 
     num_node = self.pre_num
     if self.m1 < 1 or self.m1 >= num_node:
       raise ConnectorError(f"Dual BarabsiAlbert network must have m1 >= 1 and m1 < num_node, "
                            f"while m1 = {self.m1} and num_node = {num_node}.")
     if self.m2 < 1 or self.m2 >= num_node:
       raise ConnectorError(f"Dual BarabsiAlbert network must have m2 >= 1 and m2 < num_node, "
                            f"while m2 = {self.m2} and num_node = {num_node}.")
     if self.p < 0 or self.p > 1:
       raise ConnectorError(f"Dual BarabsiAlbert network must have 0 <= p <= 1, while p = {self.p}")
 
     # Add max(m1,m2) initial nodes (m0 in barabasi-speak)
     conn = np.zeros((num_node, num_node), dtype=MAT_DTYPE)
+
+    if not isOptimized:
+      # List of existing nodes, with nodes repeated once for each adjacent edge
+      repeated_nodes = []
+      # Start adding the remaining nodes.
+      source = max(self.m1, self.m2)
+      # Pick which m to use first time (m1 or m2)
+      m = self.m1 if self.rng.random() < self.p else self.m2
+      # Target nodes for new edges
+      targets = list(range(m))
+      while source < num_node:
+        # Add edges to m nodes from the source.
+        origins = [source] * m
+        conn[origins, targets] = True
+        if not self.directed:
+          conn[targets, origins] = True
+        # Add one node to the list for each new edge just created.
+        repeated_nodes.extend(targets)
+        # And the new node "source" has m edges to add to the list.
+        repeated_nodes.extend([source] * m)
+        # Pick which m to use next time (m1 or m2)
+        m = self.m1 if self.rng.random() < self.p else self.m2
+        # Now choose m unique nodes from the existing nodes
+        # Pick uniformly from repeated_nodes (preferential attachment)
+        targets = list(self._connect(np.asarray(repeated_nodes), m))
+        source += 1
+      return conn
+
     # List of existing nodes, with nodes repeated once for each adjacent edge
-    repeated_nodes = []
+    # Preallocate repeated_nodes as a numpy array
+    repeated_nodes = np.empty(2 * num_node * max(self.m1, self.m2), dtype=int)
+    size_repeated_nodes = 0
     # Start adding the remaining nodes.
     source = max(self.m1, self.m2)
     # Pick which m to use first time (m1 or m2)
     m = self.m1 if self.rng.random() < self.p else self.m2
     # Target nodes for new edges
     targets = list(range(m))
     while source < num_node:
       # Add edges to m nodes from the source.
       origins = [source] * m
       conn[origins, targets] = True
       if not self.directed:
         conn[targets, origins] = True
       # Add one node to the list for each new edge just created.
-      repeated_nodes.extend(targets)
+      repeated_nodes[size_repeated_nodes:size_repeated_nodes + m] = targets
+      size_repeated_nodes += m
       # And the new node "source" has m edges to add to the list.
-      repeated_nodes.extend([source] * m)
+      repeated_nodes[size_repeated_nodes:size_repeated_nodes + m] = source
+      size_repeated_nodes += m
       # Pick which m to use next time (m1 or m2)
       m = self.m1 if self.rng.random() < self.p else self.m2
       # Now choose m unique nodes from the existing nodes
       # Pick uniformly from repeated_nodes (preferential attachment)
-      targets = list(self._connect(np.asarray(repeated_nodes), m))
+      targets = list(self._connect(repeated_nodes[:size_repeated_nodes], m))
       source += 1
 
-    return 'mat', conn
+    return conn
 
 
 class PowerLaw(TwoEndConnector):
   """Holme and Kim algorithm for growing graphs with powerlaw
   degree distribution and approximate average clustering.
 
   Parameters
@@ -882,59 +941,107 @@
       return targets
 
     self._connect = numba_jit(_random_subset)
 
   def __repr__(self):
     return (f'{self.__class__.__name__}(m={self.m}, p={self.p}, directed={self.directed}, seed={self.seed})')
 
-  def build_conn(self):
+  def build_mat(self, isOptimized=True):
     assert self.pre_num == self.post_num
     # seed
-    self.seed = self.rng.randint(1, int(1e7))
+    self.rng = np.random.RandomState(self.seed)
     numba_seed(self.seed)
     num_node = self.pre_num
     if self.m < 1 or num_node < self.m:
       raise ConnectorError(f"Must have m>1 and m<n, while m={self.m} and n={num_node}")
     # add m initial nodes (m0 in barabasi-speak)
     conn = np.zeros((num_node, num_node), dtype=MAT_DTYPE)
-    repeated_nodes = list(range(self.m))  # list of existing nodes to sample from
-    # with nodes repeated once for each adjacent edge
+
+    if not isOptimized:
+      repeated_nodes = list(range(self.m))  # list of existing nodes to sample from
+      # with nodes repeated once for each adjacent edge
+      source = self.m  # next node is m
+      while source < num_node:  # Now add the other n-1 nodes
+        possible_targets = self._connect(np.asarray(repeated_nodes), self.m)
+        # do one preferential attachment for new node
+        target = possible_targets.pop()
+        conn[source, target] = True
+        if not self.directed:
+          conn[target, source] = True
+        repeated_nodes.append(target)  # add one node to list for each new link
+        count = 1
+        while count < self.m:  # add m-1 more new links
+          if self.rng.random() < self.p:  # clustering step: add triangle
+            neighbors = np.where(conn[target])[0]
+            neighborhood = [nbr for nbr in neighbors if not conn[source, nbr] and not nbr == source]
+            if neighborhood:  # if there is a neighbor without a link
+              nbr = self.rng.choice(neighborhood)
+              conn[source, nbr] = True  # add triangle
+              if not self.directed:
+                conn[nbr, source] = True
+              repeated_nodes.append(nbr)
+              count = count + 1
+              continue  # go to top of while loop
+          # else do preferential attachment step if above fails
+          target = possible_targets.pop()
+          conn[source, target] = True
+          if not self.directed:
+            conn[target, source] = True
+          repeated_nodes.append(target)
+          count = count + 1
+        repeated_nodes.extend([source] * self.m)  # add source node to list m times
+        source += 1
+      return conn
+
+    # Preallocate repeated_nodes as a numpy array
+    repeated_nodes = np.empty(2 * num_node * self.m, dtype=int)
+    repeated_nodes[:self.m] = np.arange(self.m)
+    size_repeated_nodes = self.m
+
     source = self.m  # next node is m
     while source < num_node:  # Now add the other n-1 nodes
-      possible_targets = self._connect(np.asarray(repeated_nodes), self.m)
+      possible_targets = list(self._connect(repeated_nodes[:size_repeated_nodes], self.m))
+      possible_targets.reverse()
+
       # do one preferential attachment for new node
       target = possible_targets.pop()
       conn[source, target] = True
       if not self.directed:
         conn[target, source] = True
-      repeated_nodes.append(target)  # add one node to list for each new link
+      repeated_nodes[size_repeated_nodes] = target
+      size_repeated_nodes += 1
+
       count = 1
       while count < self.m:  # add m-1 more new links
         if self.rng.random() < self.p:  # clustering step: add triangle
           neighbors = np.where(conn[target])[0]
-          neighborhood = [nbr for nbr in neighbors if not conn[source, nbr] and not nbr == source]
+          neighborhood = [nbr for nbr in neighbors if not conn[source, nbr] and nbr != source]
           if neighborhood:  # if there is a neighbor without a link
             nbr = self.rng.choice(neighborhood)
             conn[source, nbr] = True  # add triangle
             if not self.directed:
               conn[nbr, source] = True
-            repeated_nodes.append(nbr)
-            count = count + 1
+            repeated_nodes[size_repeated_nodes] = nbr
+            size_repeated_nodes += 1
+            count += 1
             continue  # go to top of while loop
+
         # else do preferential attachment step if above fails
         target = possible_targets.pop()
         conn[source, target] = True
         if not self.directed:
           conn[target, source] = True
-        repeated_nodes.append(target)
-        count = count + 1
-      repeated_nodes.extend([source] * self.m)  # add source node to list m times
-      source += 1
+        repeated_nodes[size_repeated_nodes] = target
+        size_repeated_nodes += 1
+        count += 1
 
-    return 'mat', conn
+      repeated_nodes[size_repeated_nodes:size_repeated_nodes + self.m] = source
+      size_repeated_nodes += self.m
+      source += 1
+    return conn
 
 
 @numba_jit
 def pos2ind(pos, size):
   idx = 0
   for i, p in enumerate(pos):
     idx += p * np.prod(size[i + 1:])
@@ -969,14 +1076,62 @@
     self.dist = dist
     self.seed = format_seed(seed)
     self.rng = np.random.RandomState(self.seed)
     self.include_self = include_self
 
     rng = np.random if SUPPORT_NUMBA else self.rng
 
+    # @njit(parallel=True)
+    # def _connect_1d_jit_parallel(pre_pos, pre_size, post_size, n_dim):
+    #   all_post_ids = np.zeros(post_size[0], dtype=np.int32)
+    #   all_pre_ids = np.zeros(post_size[0], dtype=np.int32)
+    #   size = 0
+    #
+    #   if rng.random() < pre_ratio:
+    #     normalized_pos = np.zeros(n_dim)
+    #     for i in prange(n_dim):  # Use prange for potential parallelism
+    #       pre_len = pre_size[i]
+    #       post_len = post_size[i]
+    #       normalized_pos[i] = pre_pos[i] * post_len / pre_len
+    #     for i in prange(post_size[0]):
+    #       post_pos = np.asarray((i,))
+    #       d = np.abs(pre_pos[0] - post_pos[0])  # Adjust the distance calculation
+    #       if d <= dist:
+    #         if d == 0. and not include_self:
+    #           continue
+    #         if rng.random() <= prob:
+    #           all_post_ids[size] = pos2ind(post_pos, post_size)
+    #           all_pre_ids[size] = pos2ind(pre_pos, pre_size)
+    #           size += 1
+    #   return all_pre_ids[:size], all_post_ids[:size]  # Return filled part of the arrays
+
+    @njit
+    def _connect_1d_jit(pre_pos, pre_size, post_size, n_dim):
+      all_post_ids = np.zeros(post_size[0], dtype=np.int32)
+      all_pre_ids = np.zeros(post_size[0], dtype=np.int32)
+      size = 0
+
+      if rng.random() < pre_ratio:
+        normalized_pos = np.zeros(n_dim)
+        for i in range(n_dim):
+          pre_len = pre_size[i]
+          post_len = post_size[i]
+          normalized_pos[i] = pre_pos[i] * post_len / pre_len
+        for i in range(post_size[0]):
+          post_pos = np.asarray((i,))
+          d = np.abs(pre_pos[0] - post_pos[0])
+          if d <= dist:
+            if d == 0. and not include_self:
+              continue
+            if rng.random() <= prob:
+              all_post_ids[size] = pos2ind(post_pos, post_size)
+              all_pre_ids[size] = pos2ind(pre_pos, pre_size)
+              size += 1
+      return all_pre_ids[:size], all_post_ids[:size]
+
     def _connect_1d(pre_pos, pre_size, post_size, n_dim):
       all_post_ids = []
       all_pre_ids = []
       if rng.random() < pre_ratio:
         normalized_pos = []
         for i in range(n_dim):
           pre_len = pre_size[i]
@@ -989,14 +1144,40 @@
             if d == 0. and not include_self:
               continue
             if rng.random() <= prob:
               all_post_ids.append(pos2ind(post_pos, post_size))
               all_pre_ids.append(pos2ind(pre_pos, pre_size))
       return all_pre_ids, all_post_ids
 
+    @njit
+    def _connect_2d_jit(pre_pos, pre_size, post_size, n_dim):
+      max_size = post_size[0] * post_size[1]
+      all_post_ids = np.zeros(max_size, dtype=np.int32)
+      all_pre_ids = np.zeros(max_size, dtype=np.int32)
+      size = 0
+
+      if rng.random() < pre_ratio:
+        normalized_pos = np.zeros(n_dim)
+        for i in range(n_dim):
+          pre_len = pre_size[i]
+          post_len = post_size[i]
+          normalized_pos[i] = pre_pos[i] * post_len / pre_len
+        for i in range(post_size[0]):
+          for j in range(post_size[1]):
+            post_pos = np.asarray((i, j))
+            d = np.sqrt(np.sum(np.square(pre_pos - post_pos)))
+            if d <= dist:
+              if d == 0. and not include_self:
+                continue
+              if rng.random() <= prob:
+                all_post_ids[size] = pos2ind(post_pos, post_size)
+                all_pre_ids[size] = pos2ind(pre_pos, pre_size)
+                size += 1
+      return all_pre_ids[:size], all_post_ids[:size]  # Return filled part of the arrays
+
     def _connect_2d(pre_pos, pre_size, post_size, n_dim):
       all_post_ids = []
       all_pre_ids = []
       if rng.random() < pre_ratio:
         normalized_pos = []
         for i in range(n_dim):
           pre_len = pre_size[i]
@@ -1010,14 +1191,41 @@
               if d == 0. and not include_self:
                 continue
               if np.random.random() <= prob:
                 all_post_ids.append(pos2ind(post_pos, post_size))
                 all_pre_ids.append(pos2ind(pre_pos, pre_size))
       return all_pre_ids, all_post_ids
 
+    @njit
+    def _connect_3d_jit(pre_pos, pre_size, post_size, n_dim):
+      max_size = post_size[0] * post_size[1] * post_size[2]
+      all_post_ids = np.zeros(max_size, dtype=np.int32)
+      all_pre_ids = np.zeros(max_size, dtype=np.int32)
+      size = 0
+
+      if rng.random() < pre_ratio:
+        normalized_pos = np.zeros(n_dim)
+        for i in range(n_dim):
+          pre_len = pre_size[i]
+          post_len = post_size[i]
+          normalized_pos[i] = pre_pos[i] * post_len / pre_len
+        for i in range(post_size[0]):
+          for j in range(post_size[1]):
+            for k in range(post_size[2]):
+              post_pos = np.asarray((i, j, k))
+              d = np.sqrt(np.sum(np.square(pre_pos - post_pos)))
+              if d <= dist:
+                if d == 0. and not include_self:
+                  continue
+                if rng.random() <= prob:
+                  all_post_ids[size] = pos2ind(post_pos, post_size)
+                  all_pre_ids[size] = pos2ind(pre_pos, pre_size)
+                  size += 1
+      return all_pre_ids[:size], all_post_ids[:size]
+
     def _connect_3d(pre_pos, pre_size, post_size, n_dim):
       all_post_ids = []
       all_pre_ids = []
       if rng.random() < pre_ratio:
         normalized_pos = []
         for i in range(n_dim):
           pre_len = pre_size[i]
@@ -1032,14 +1240,42 @@
                 if d == 0. and not include_self:
                   continue
                 if np.random.random() <= prob:
                   all_post_ids.append(pos2ind(post_pos, post_size))
                   all_pre_ids.append(pos2ind(pre_pos, pre_size))
       return all_pre_ids, all_post_ids
 
+    @njit
+    def _connect_4d_jit(pre_pos, pre_size, post_size, n_dim):
+      max_size = post_size[0] * post_size[1] * post_size[2] * post_size[3]
+      all_post_ids = np.zeros(max_size, dtype=np.int32)
+      all_pre_ids = np.zeros(max_size, dtype=np.int32)
+      size = 0
+
+      if rng.random() < pre_ratio:
+        normalized_pos = np.zeros(n_dim)
+        for i in range(n_dim):
+          pre_len = pre_size[i]
+          post_len = post_size[i]
+          normalized_pos[i] = pre_pos[i] * post_len / pre_len
+        for i in range(post_size[0]):
+          for j in range(post_size[1]):
+            for k in range(post_size[2]):
+              for l in range(post_size[3]):
+                post_pos = np.asarray((i, j, k, l))
+                d = np.sqrt(np.sum(np.square(pre_pos - post_pos)))
+                if d <= dist:
+                  if d == 0. and not include_self:
+                    continue
+                  if rng.random() <= prob:
+                    all_post_ids[size] = pos2ind(post_pos, post_size)
+                    all_pre_ids[size] = pos2ind(pre_pos, pre_size)
+                    size += 1
+      return all_pre_ids[:size], all_post_ids[:size]
+
     def _connect_4d(pre_pos, pre_size, post_size, n_dim):
       all_post_ids = []
       all_pre_ids = []
       if rng.random() < pre_ratio:
         normalized_pos = []
         for i in range(n_dim):
           pre_len = pre_size[i]
@@ -1060,41 +1296,62 @@
       return all_pre_ids, all_post_ids
 
     self._connect_1d = numba_jit(_connect_1d)
     self._connect_2d = numba_jit(_connect_2d)
     self._connect_3d = numba_jit(_connect_3d)
     self._connect_4d = numba_jit(_connect_4d)
 
-  def build_coo(self):
+    self._connect_1d_jit = _connect_1d_jit
+    self._connect_2d_jit = _connect_2d_jit
+    self._connect_3d_jit = _connect_3d_jit
+    self._connect_4d_jit = _connect_4d_jit
+
+
+  def build_coo(self, isOptimized=True):
     if len(self.pre_size) != len(self.post_size):
       raise ValueError('The dimensions of shapes of two objects to establish connections should '
                        f'be the same. But we got dimension {len(self.pre_size)} != {len(self.post_size)}. '
                        f'Specifically, pre size = {self.pre_size}, post size = {self.post_size}')
-    self.seed = self.rng.randint(1, int(1e7))
+    self.rng = np.random.RandomState(self.seed)
     numba_seed(self.seed)
 
     # connections
     n_dim = len(self.pre_size)
-    if n_dim == 1:
-      f = self._connect_1d
-    elif n_dim == 2:
-      f = self._connect_2d
-    elif n_dim == 3:
-      f = self._connect_3d
-    elif n_dim == 4:
-      f = self._connect_4d
+    if not isOptimized:
+      if n_dim == 1:
+        f = self._connect_1d
+      elif n_dim == 2:
+        f = self._connect_2d
+      elif n_dim == 3:
+        f = self._connect_3d
+      elif n_dim == 4:
+        f = self._connect_4d
+      else:
+        raise NotImplementedError('Does not support the network dimension bigger than 4.')
     else:
-      raise NotImplementedError('Does not support the network dimension bigger than 4.')
+      if n_dim == 1:
+        f = self._connect_1d_jit
+      elif n_dim == 2:
+        f = self._connect_2d_jit
+      elif n_dim == 3:
+        f = self._connect_3d_jit
+      elif n_dim == 4:
+        f = self._connect_4d_jit
+      else:
+        raise NotImplementedError('Does not support the network dimension bigger than 4.')
+
+
 
     pre_size = np.asarray(self.pre_size)
     post_size = np.asarray(self.post_size)
     connected_pres = []
     connected_posts = []
     pre_ids = np.meshgrid(*(np.arange(p) for p in self.pre_size), indexing='ij')
     pre_ids = tuple([(np.moveaxis(p, 0, 1).flatten()) if p.ndim > 1 else p.flatten() for p in pre_ids])
     size = np.prod(pre_size)
+
     for i in range(size):
       pre_pos = np.asarray([p[i] for p in pre_ids])
       pres, posts = f(pre_pos, pre_size=pre_size, post_size=post_size, n_dim=n_dim)
       connected_pres.extend(pres)
       connected_posts.extend(posts)
     return np.asarray(connected_pres), np.asarray(connected_posts)
```

## brainpy/_src/dnn/activations.py

```diff
@@ -1,22 +1,24 @@
 from typing import Optional
 
 from brainpy import math as bm
+from brainpy._src.dnn.base import Layer
 from brainpy.types import ArrayType
-from .base import Layer
 
-__all__ = ['Threshold', 'ReLU', 'RReLU', 'Hardtanh', 'ReLU6', 'Sigmoid', 'Hardsigmoid', 'Tanh',
-           'SiLU', 'Mish', 'Hardswish', 'ELU', 'CELU', 'SELU', 'GLU', 'GELU', 'Hardshrink', 'LeakyReLU',
-           'LogSigmoid', 'Softplus', 'Softshrink', 'PReLU', 'Softsign', 'Tanhshrink',
-           'Softmin', 'Softmax', 'Softmax2d', 'LogSoftmax']
+__all__ = [
+  'Threshold', 'ReLU', 'RReLU', 'Hardtanh', 'ReLU6', 'Sigmoid', 'Hardsigmoid', 'Tanh',
+  'SiLU', 'Mish', 'Hardswish', 'ELU', 'CELU', 'SELU', 'GLU', 'GELU', 'Hardshrink', 'LeakyReLU',
+  'LogSigmoid', 'Softplus', 'Softshrink', 'PReLU', 'Softsign', 'Tanhshrink',
+  'Softmin', 'Softmax', 'Softmax2d', 'LogSoftmax'
+]
 
 
 def _inplace(inp, val, inplace):
   if inplace:
-    assert isinstance(input, bm.Array), 'input must be instance of brainpy.math.Array if inplace=True'
+    assert isinstance(inp, bm.Array), 'input must be instance of brainpy.math.Array if inplace=True'
     inp.value = val
     return inp
   else:
     return val
 
 
 class Threshold(Layer):
@@ -40,15 +42,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Threshold(0.1, 20)
+      >>> m = bp.dnn.Threshold(0.1, 20)
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['threshold', 'value', 'inplace']
 
   threshold: float
   value: float
@@ -83,24 +85,24 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.ReLU()
+      >>> m = bp.dnn.ReLU()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
 
 
     An implementation of CReLU - https://arxiv.org/abs/1603.05201
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.ReLU()
+      >>> m = bp.dnn.ReLU()
       >>> input = bm.random.randn(2).unsqueeze(0)
       >>> output = bm.cat((m(input), m(-input)))
   """
   __constants__ = ['inplace']
   inplace: bool
 
   def __init__(self, inplace: bool = False):
@@ -145,15 +147,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.RReLU(0.1, 0.3)
+      >>> m = bp.dnn.RReLU(0.1, 0.3)
       >>> input = bm.random.randn(2)
       >>> output = m(input)
 
   .. _`Empirical Evaluation of Rectified Activations in Convolutional Network`:
       https://arxiv.org/abs/1505.00853
   """
   __constants__ = ['lower', 'upper', 'inplace']
@@ -206,15 +208,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Hardtanh(-2, 2)
+      >>> m = bp.dnn.Hardtanh(-2, 2)
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['min_val', 'max_val', 'inplace']
 
   min_val: float
   max_val: float
@@ -256,15 +258,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.ReLU6()
+      >>> m = bp.dnn.test_ReLU6()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
 
   def __init__(self, inplace: bool = False):
     super().__init__(0., 6., inplace)
 
@@ -284,15 +286,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Sigmoid()
+      >>> m = bp.dnn.Sigmoid()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
 
   def update(self, input: ArrayType) -> ArrayType:
     return bm.sigmoid(input)
 
@@ -316,15 +318,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Hardsigmoid()
+      >>> m = bp.dnn.Hardsigmoid()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['inplace']
 
   inplace: bool
 
@@ -349,15 +351,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Tanh()
+      >>> m = bp.dnn.Tanh()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
 
   def update(self, input: ArrayType) -> ArrayType:
     return bm.tanh(input)
 
@@ -372,24 +374,26 @@
   .. note::
       See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_
       where the SiLU (Sigmoid Linear Unit) was originally coined, and see
       `Sigmoid-Weighted Linear Units for Neural Network Function Approximation
       in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:
       a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_
       where the SiLU was experimented with later.
+  Args:
+      inplace: can optionally do the operation in-place. Default: ``False``
 
   Shape:
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.SiLU()
+      >>> m = bp.dnn.SiLU()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['inplace']
   inplace: bool
 
   def __init__(self, inplace: bool = False):
@@ -410,23 +414,26 @@
 
   .. math::
       \text{Mish}(x) = x * \text{Tanh}(\text{Softplus}(x))
 
   .. note::
       See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_
 
+  Args:
+      inplace: can optionally do the operation in-place. Default: ``False``
+
   Shape:
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Mish()
+      >>> m = bp.dnn.Mish()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['inplace']
   inplace: bool
 
   def __init__(self, inplace: bool = False):
@@ -461,15 +468,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Hardswish()
+      >>> m = bp.dnn.Hardswish()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['inplace']
 
   inplace: bool
 
@@ -502,15 +509,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.ELU()
+      >>> m = bp.dnn.ELU()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['alpha', 'inplace']
   alpha: float
   inplace: bool
 
@@ -543,15 +550,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.CELU()
+      >>> m = bp.dnn.CELU()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
 
   .. _`Continuously Differentiable Exponential Linear Units`:
       https://arxiv.org/abs/1704.07483
   """
   __constants__ = ['alpha', 'inplace']
@@ -589,15 +596,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.SELU()
+      >>> m = bp.dnn.SELU()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
 
   .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515
   """
   __constants__ = ['inplace']
   inplace: bool
@@ -627,15 +634,15 @@
         dimensions
       - Output: :math:`(\ast_1, M, \ast_2)` where :math:`M=N/2`
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.GLU()
+      >>> m = bp.dnn.GLU()
       >>> input = bm.random.randn(4, 2)
       >>> output = m(input)
   """
   __constants__ = ['dim']
   dim: int
 
   def __init__(self, dim: int = -1) -> None:
@@ -668,15 +675,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.GELU()
+      >>> m = bp.dnn.GELU()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['approximate']
   approximate: bool
 
   def __init__(self, approximate: bool = False) -> None:
@@ -710,15 +717,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Hardshrink()
+      >>> m = bp.dnn.Hardshrink()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['lambd']
   lambd: float
 
   def __init__(self, lambd: float = 0.5) -> None:
@@ -758,15 +765,15 @@
         dimensions
       - Output: :math:`(*)`, same shape as the input
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.LeakyReLU(0.1)
+      >>> m = bp.dnn.LeakyReLU(0.1)
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['inplace', 'negative_slope']
   inplace: bool
   negative_slope: float
 
@@ -793,15 +800,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.LogSigmoid()
+      >>> m = bp.dnn.LogSigmoid()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
 
   def update(self, input: ArrayType) -> ArrayType:
     return bm.log_sigmoid(input)
 
@@ -824,15 +831,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Softplus()
+      >>> m = bp.dnn.Softplus()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['beta', 'threshold']
   beta: int
   threshold: int
 
@@ -866,15 +873,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Softshrink()
+      >>> m = bp.dnn.Softshrink()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['lambd']
   lambd: float
 
   def __init__(self, lambd: float = 0.5) -> None:
@@ -899,16 +906,16 @@
   .. math::
       \text{PReLU}(x) =
       \begin{cases}
       x, & \text{ if } x \geq 0 \\
       ax, & \text{ otherwise }
       \end{cases}
 
-  Here :math:`a` is a learnable parameter. When called without arguments, `bp.layers.PReLU()` uses a single
-  parameter :math:`a` across all input channels. If called with `bp.layers.PReLU(nChannels)`,
+  Here :math:`a` is a learnable parameter. When called without arguments, `bp.dnn.PReLU()` uses a single
+  parameter :math:`a` across all input channels. If called with `bp.dnn.PReLU(nChannels)`,
   a separate :math:`a` is used for each input channel.
 
 
   .. note::
       weight decay should not be used when learning :math:`a` for good performance.
 
   .. note::
@@ -929,15 +936,15 @@
   Attributes:
       weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.PReLU()
+      >>> m = bp.dnn.PReLU()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
   __constants__ = ['num_parameters']
   num_parameters: int
 
   def __init__(self, num_parameters: int = 1, init: float = 0.25, dtype=None) -> None:
@@ -962,15 +969,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Softsign()
+      >>> m = bp.dnn.Softsign()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
 
   def update(self, input: ArrayType) -> ArrayType:
     return bm.soft_sign(input)
 
@@ -985,15 +992,15 @@
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Tanhshrink()
+      >>> m = bp.dnn.Tanhshrink()
       >>> input = bm.random.randn(2)
       >>> output = m(input)
   """
 
   def update(self, input: ArrayType) -> ArrayType:
     return bm.tanh_shrink(input)
 
@@ -1021,15 +1028,15 @@
       a Tensor of the same dimension and shape as the input, with
       values in the range [0, 1]
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Softmin(dim=1)
+      >>> m = bp.dnn.Softmin(dim=1)
       >>> input = bm.random.randn(2, 3)
       >>> output = m(input)
   """
   __constants__ = ['dim']
   dim: Optional[int]
 
   def __init__(self, dim: Optional[int] = None) -> None:
@@ -1074,15 +1081,15 @@
       which expects the Log to be computed between the Softmax and itself.
       Use `LogSoftmax` instead (it's faster and has better numerical properties).
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Softmax(dim=1)
+      >>> m = bp.dnn.Softmax(dim=1)
       >>> input = bm.random.randn(2, 3)
       >>> output = m(input)
 
   """
   __constants__ = ['dim']
   dim: Optional[int]
 
@@ -1111,22 +1118,22 @@
       a Tensor of the same dimension and shape as the input with
       values in the range [0, 1]
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.Softmax2d()
+      >>> m = bp.dnn.Softmax2d()
       >>> # you softmax over the 2nd dimension
       >>> input = bm.random.randn(2, 3, 12, 13)
       >>> output = m(input)
   """
 
   def update(self, input: ArrayType) -> ArrayType:
-    assert input.dim() == 4 or input.dim() == 3, 'Softmax2d requires a 3D or 4D tensor as input'
+    assert input.ndim == 4 or input.ndim == 3, 'Softmax2d requires a 3D or 4D tensor as input'
     return bm.softmax(input, -3)
 
 
 class LogSoftmax(Layer):
   r"""Applies the :math:`\log(\text{Softmax}(x))` function to an n-dimensional
   input Tensor. The LogSoftmax formulation can be simplified as:
 
@@ -1145,15 +1152,15 @@
       a Tensor of the same dimension and shape as the input with
       values in the range [-inf, 0)
 
   Examples::
 
       >>> import brainpy as bp
       >>> import brainpy.math as bm
-      >>> m = bp.layers.LogSoftmax(dim=1)
+      >>> m = bp.dnn.LogSoftmax(dim=1)
       >>> input = bm.random.randn(2, 3)
       >>> output = m(input)
   """
   __constants__ = ['dim']
   dim: Optional[int]
 
   def __init__(self, dim: Optional[int] = None) -> None:
```

## brainpy/_src/dnn/base.py

```diff
@@ -1,8 +1,14 @@
-from brainpy._src.dynsys import DynamicalSystemNS
+from brainpy._src.dynsys import DynamicalSystem
 
 
-class Layer(DynamicalSystemNS):
+__all__ = [
+  'Layer'
+]
+
+
+class Layer(DynamicalSystem):
   """Base class for a layer of artificial neural network."""
 
   def reset_state(self, *args, **kwargs):
     pass
+
```

## brainpy/_src/dnn/conv.py

```diff
@@ -3,15 +3,15 @@
 from typing import Union, Tuple, Optional, Sequence, Callable
 
 from jax import lax
 
 from brainpy import math as bm, tools, check
 from brainpy._src.initialize import Initializer, XavierNormal, ZeroInit, parameter
 from brainpy.types import ArrayType
-from .base import Layer
+from brainpy._src.dnn.base import Layer
 
 __all__ = [
   'Conv1d', 'Conv2d', 'Conv3d',
   'Conv1D', 'Conv2D', 'Conv3D',
   'ConvTranspose1d', 'ConvTranspose2d', 'ConvTranspose3d',
 ]
 
@@ -77,14 +77,16 @@
     The optional mask of the weights.
   mode: Mode
     The computation mode of the current object. Default it is `training`.
   name: str, Optional
     The name of the object.
   """
 
+  supported_modes = (bm.TrainingMode, bm.BatchingMode)
+
   def __init__(
       self,
       num_spatial_dims: int,
       in_channels: int,
       out_channels: int,
       kernel_size: Union[int, Tuple[int, ...]],
       stride: Union[int, Tuple[int, ...]] = 1,
@@ -458,14 +460,16 @@
 
 Conv1D = Conv1d
 Conv2D = Conv2d
 Conv3D = Conv3d
 
 
 class _GeneralConvTranspose(Layer):
+  supported_modes = (bm.TrainingMode, bm.BatchingMode)
+
   def __init__(
       self,
       num_spatial_dims: int,
       in_channels: int,
       out_channels: int,
       kernel_size: Union[int, Tuple[int, ...]],
       stride: Union[int, Tuple[int, ...]] = 1,
@@ -703,15 +707,15 @@
       b_init: Optional bias initialization. By default, zeros.
       data_format: The data format of the input. Either ``NDHWC`` or ``NCDHW``.
         By default, ``NDHWC``.
       mask: Optional mask of the weights.
       name: The name of the module.
     """
     super().__init__(
-      num_spatial_dims=1,
+      num_spatial_dims=3,
       in_channels=in_channels,
       out_channels=out_channels,
       kernel_size=kernel_size,
       stride=stride,
       padding=padding,
       w_initializer=w_initializer,
       b_initializer=b_initializer,
```

## brainpy/_src/dnn/dropout.py

```diff
@@ -1,26 +1,27 @@
 # -*- coding: utf-8 -*-
 
+from typing import Optional
 
 from brainpy._src.context import share
 from brainpy import math as bm, check
-from .base import Layer
+from brainpy._src.dnn.base import Layer
 
 __all__ = [
   'Dropout'
 ]
 
 
 class Dropout(Layer):
   """A layer that stochastically ignores a subset of inputs each training step.
 
   In training, to compensate for the fraction of input values dropped (`rate`),
   all surviving values are multiplied by `1 / (1 - rate)`.
 
-  This layer is active only during training (`mode=brainpy.modes.training`). In other
+  This layer is active only during training (``mode=brainpy.math.training_mode``). In other
   circumstances it is a no-op.
 
   .. [1] Srivastava, Nitish, et al. "Dropout: a simple way to prevent
          neural networks from overfitting." The journal of machine learning
          research 15.1 (2014): 1929-1958.
 
   Args:
@@ -29,24 +30,22 @@
     name: str. The name of the dynamic system.
 
   """
 
   def __init__(
       self,
       prob: float,
-      mode: bm.Mode = None,
-      name: str = None
+      mode: Optional[bm.Mode] = None,
+      name: Optional[str] = None
   ):
-    """
-
-
-    """
     super(Dropout, self).__init__(mode=mode, name=name)
     self.prob = check.is_float(prob, min_bound=0., max_bound=1.)
 
-  def update(self, x):
-    if share.load('fit'):
+  def update(self, x, fit: Optional[bool] = None):
+    if fit is None:
+      fit = share['fit']
+    if fit:
       keep_mask = bm.random.bernoulli(self.prob, x.shape)
       return bm.where(keep_mask, x / self.prob, 0.)
     else:
       return x
```

## brainpy/_src/dnn/function.py

```diff
@@ -1,15 +1,15 @@
 # -*- coding: utf-8 -*-
 
 from typing import Callable
 from typing import Optional
 
 import brainpy.math as bm
 from brainpy import check
-from .base import Layer
+from brainpy._src.dnn.base import Layer
 
 __all__ = [
   'Activation',
   'Flatten',
   'FunAsLayer',
 ]
```

## brainpy/_src/dnn/interoperation_flax.py

```diff
@@ -1,16 +1,17 @@
 
 import jax
 import dataclasses
 from typing import Dict
 from jax.tree_util import tree_flatten, tree_map, tree_unflatten
 
 from brainpy import math as bm
-from brainpy._src.dynsys import DynamicalSystemNS, DynamicalSystem
+from brainpy._src.dynsys import DynamicalSystem
 from brainpy._src.context import share
+from brainpy._src.dnn.base import Layer
 
 try:
   import flax  # noqa
   from flax.linen.recurrent import RNNCellBase
 except:
   flax = None
   RNNCellBase = object
@@ -30,15 +31,15 @@
     return a
 
 
 def _is_bp(a):
   return isinstance(a, bm.Array)
 
 
-class FromFlax(DynamicalSystemNS):
+class FromFlax(Layer):
   """
   Transform a Flax module as a BrainPy :py:class:`~.DynamicalSystem`.
 
   Parameters
   ----------
   flax_module: Any
     The flax Module.
```

## brainpy/_src/dnn/linear.py

```diff
@@ -1,24 +1,25 @@
 # -*- coding: utf-8 -*-
 
 
 from typing import Dict, Optional, Union, Callable
 
 import jax
+import numpy as np
 import jax.numpy as jnp
 
 from brainpy import math as bm
 from brainpy._src import connect, initialize as init
 from brainpy._src.context import share
 from brainpy.algorithms import OnlineAlgorithm, OfflineAlgorithm
 from brainpy.check import is_initializer
 from brainpy.errors import MathError
 from brainpy.initialize import XavierNormal, ZeroInit, Initializer, parameter
 from brainpy.types import ArrayType, Sharding
-from .base import Layer
+from brainpy._src.dnn.base import Layer
 
 __all__ = [
   'Dense', 'Linear',
   'Identity',
   'AllToAll',
   'OneToOne',
   'MaskedLinear',
@@ -59,16 +60,16 @@
 
   def __init__(
       self,
       num_in: int,
       num_out: int,
       W_initializer: Union[Initializer, Callable, ArrayType] = XavierNormal(),
       b_initializer: Optional[Union[Initializer, Callable, ArrayType]] = ZeroInit(),
-      mode: bm.Mode = None,
-      name: str = None,
+      mode: Optional[bm.Mode] = None,
+      name: Optional[str] = None,
   ):
     super(Dense, self).__init__(mode=mode, name=name)
 
     # shape
     self.num_in = num_in
     self.num_out = num_out
     if num_in < 0:
@@ -312,58 +313,65 @@
     self.weight = weight
 
   def update(self, pre_val):
     return pre_val * self.weight
 
 
 class MaskedLinear(Layer):
-  r"""Synaptic matrix multiplication with dense computation.
+  r"""Synaptic matrix multiplication with masked dense computation.
 
   It performs the computation of:
 
   .. math::
 
      y = x @ M
 
   where :math:`y` is the postsynaptic value, :math:`x` the presynaptic value,
   :math:`M` the synaptic weight using a dense matrix.
 
+  >>> import brainpy as bp
+  >>> l = bp.dnn.MaskedLinear(bp.conn.FixedProb(0.1, pre=100, post=100),
+  >>>                         weight=0.1)
+
   Args:
-    mask: TwoEndConnector. The connection.
+    conn: TwoEndConnector. The connection.
     weight: Synaptic weights. Can be a scalar, array, or callable function.
+    mask_fun: Masking function.
     sharding: The sharding strategy. 
     mode: The synaptic computing mode.
     name: The synapse model name.
   """
 
   def __init__(
       self,
-      mask: connect.TwoEndConnector,
+      conn: connect.TwoEndConnector,
       weight: Union[float, ArrayType, Callable],
+      mask_fun: Callable = Identity(),
       sharding: Optional[Sharding] = None,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
   ):
     super().__init__(name=name, mode=mode)
 
-    assert isinstance(mask, connect.TwoEndConnector)
-    self.conn = mask
+    assert isinstance(conn, connect.TwoEndConnector)
+    self.conn = conn
     self.sharding = sharding
+    self.mask_fun = mask_fun
 
     # weight
-    weight = init.parameter(weight, (mask.pre_num, mask.post_num), sharding=sharding)
+    weight = init.parameter(weight, (conn.pre_num, conn.post_num), sharding=sharding)
     if isinstance(self.mode, bm.TrainingMode):
       weight = bm.TrainVar(weight)
     self.weight = weight
 
     # connection
     self.mask = bm.sharding.partition(self.conn.require('conn_mat'), sharding=sharding)
 
   def update(self, x):
-    return x @ (self.weight * self.mask)
+    return x @ self.mask_fun(self.weight * self.mask)
 
 
 class CSRLinear(Layer):
   r"""Synaptic matrix multiplication with CSR sparse computation.
 
   It performs the computation of:
 
@@ -631,27 +639,27 @@
 
   def __init__(
       self,
       num_in: int,
       num_out: int,
       prob: float,
       weight: float,
-      seed: int,
+      seed: Optional[int] = None,
       sharding: Optional[Sharding] = None,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       transpose: bool = False,
       atomic: bool = False,
   ):
     super().__init__(name=name, mode=mode)
 
     self.prob = prob
     self.sharding = sharding
     self.transpose = transpose
-    self.seed = seed
+    self.seed = np.random.randint(0, 100000) if seed is None else seed
     self.atomic = atomic
     self.num_in = num_in
     self.num_out = num_out
 
     # weight
     if isinstance(self.mode, bm.TrainingMode):
       weight = bm.TrainVar(weight)
@@ -712,27 +720,27 @@
   def __init__(
       self,
       num_in: int,
       num_out: int,
       prob: float,
       w_low: float,
       w_high: float,
-      seed: int,
+      seed: Optional[int] = None,
       sharding: Optional[Sharding] = None,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       transpose: bool = False,
       atomic: bool = False,
   ):
     super().__init__(name=name, mode=mode)
 
     self.prob = prob
     self.sharding = sharding
     self.transpose = transpose
-    self.seed = seed
+    self.seed = np.random.randint(0, 100000) if seed is None else seed
     self.atomic = atomic
     self.num_in = num_in
     self.num_out = num_out
 
     # weight
     self.w_low = w_low
     self.w_high = w_high
@@ -792,27 +800,27 @@
   def __init__(
       self,
       num_in: int,
       num_out: int,
       prob: float,
       w_mu: float,
       w_sigma: float,
-      seed: int,
+      seed: Optional[int] = None,
       sharding: Optional[Sharding] = None,
       transpose: bool = False,
       atomic: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
   ):
     super().__init__(name=name, mode=mode)
 
     self.prob = prob
     self.sharding = sharding
     self.transpose = transpose
-    self.seed = seed
+    self.seed = np.random.randint(0, 100000) if seed is None else seed
     self.atomic = atomic
     self.num_in = num_in
     self.num_out = num_out
 
     # weight
     self.w_mu = w_mu
     self.w_sigma = w_sigma
@@ -870,27 +878,27 @@
 
   def __init__(
       self,
       num_in: int,
       num_out: int,
       prob: float,
       weight: float,
-      seed: int,
+      seed: Optional[int] = None,
       sharding: Optional[Sharding] = None,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       transpose: bool = False,
       atomic: bool = False,
   ):
     super().__init__(name=name, mode=mode)
 
     self.prob = prob
     self.sharding = sharding
     self.transpose = transpose
-    self.seed = seed
+    self.seed = np.random.randint(0, 1000000) if seed is None else seed
     self.atomic = atomic
     self.num_in = num_in
     self.num_out = num_out
 
     # weight
     if isinstance(self.mode, bm.TrainingMode):
       weight = bm.TrainVar(weight)
@@ -951,27 +959,27 @@
   def __init__(
       self,
       num_in: int,
       num_out: int,
       prob: float,
       w_low: float,
       w_high: float,
-      seed: int,
+      seed: Optional[int] = None,
       sharding: Optional[Sharding] = None,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       transpose: bool = False,
       atomic: bool = False,
   ):
     super().__init__(name=name, mode=mode)
 
     self.prob = prob
     self.sharding = sharding
     self.transpose = transpose
-    self.seed = seed
+    self.seed = np.random.randint(0, 100000) if seed is None else seed
     self.atomic = atomic
     self.num_in = num_in
     self.num_out = num_out
 
     # weight
     self.w_low = w_low
     self.w_high = w_high
@@ -1031,27 +1039,27 @@
   def __init__(
       self,
       num_in: int,
       num_out: int,
       prob: float,
       w_mu: float,
       w_sigma: float,
-      seed: int,
+      seed: Optional[int] = None,
       sharding: Optional[Sharding] = None,
       transpose: bool = False,
       atomic: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
   ):
     super().__init__(name=name, mode=mode)
 
     self.prob = prob
     self.sharding = sharding
     self.transpose = transpose
-    self.seed = seed
+    self.seed = np.random.randint(0, 100000) if seed is None else seed
     self.atomic = atomic
     self.num_in = num_in
     self.num_out = num_out
 
     # weight
     self.w_mu = w_mu
     self.w_sigma = w_sigma
```

## brainpy/_src/dnn/normalization.py

```diff
@@ -4,15 +4,15 @@
 
 from jax import lax, numpy as jnp
 
 from brainpy._src.context import share
 from brainpy import math as bm, check
 from brainpy.initialize import ZeroInit, OneInit, Initializer, parameter
 from brainpy.types import ArrayType
-from .base import Layer
+from brainpy._src.dnn.base import Layer
 
 __all__ = [
   'BatchNorm1d',
   'BatchNorm2d',
   'BatchNorm3d',
   'BatchNorm1D',
   'BatchNorm2D',
```

## brainpy/_src/dnn/nvar.py

```diff
@@ -4,15 +4,15 @@
 from typing import Union, Sequence, List, Optional
 
 import jax.numpy as jnp
 import numpy as np
 
 import brainpy.math as bm
 from brainpy import check
-from .base import Layer
+from brainpy._src.dnn.base import Layer
 
 __all__ = [
   'NVAR'
 ]
 
 
 def _comb(N, k):
```

## brainpy/_src/dnn/pooling.py

```diff
@@ -3,15 +3,15 @@
 from typing import Union, Tuple, Sequence, Optional, Callable, List, Any
 
 import jax
 import jax.numpy as jnp
 import numpy as np
 
 from brainpy import math as bm, check
-from .base import Layer
+from brainpy._src.dnn.base import Layer
 
 __all__ = [
   'MaxPool',
   'MinPool',
   'AvgPool',
   'AvgPool1d',
   'AvgPool2d',
@@ -767,14 +767,16 @@
       Inputs. Should be a JAX array of shape `(..., dim_1, dim_2, channels)`
       or `(..., dim_1, dim_2)`.
     """
     x = bm.as_jax(x)
 
     # channel axis
     channel_axis = self.channel_axis
+
+
     if channel_axis:
       if not 0 <= abs(channel_axis) < x.ndim:
         raise ValueError(f"Invalid channel axis {channel_axis} for {x.shape}")
       if channel_axis < 0:
         channel_axis = x.ndim + channel_axis
     # input dimension
     if (x.ndim - (0 if channel_axis is None else 1)) < len(self.target_shape):
```

## brainpy/_src/dnn/reservoir.py

```diff
@@ -5,15 +5,15 @@
 import jax.numpy as jnp
 
 import brainpy.math as bm
 from brainpy._src.initialize import Normal, ZeroInit, Initializer, parameter, variable
 from brainpy import check
 from brainpy.tools import to_size
 from brainpy.types import ArrayType
-from .base import Layer
+from brainpy._src.dnn.base import Layer
 
 __all__ = [
   'Reservoir',
 ]
 
 
 class Reservoir(Layer):
```

## brainpy/_src/dnn/rnncells.py

```diff
@@ -3,15 +3,15 @@
 import warnings
 from typing import Union, Callable, Sequence, Optional, Tuple
 
 import jax.numpy as jnp
 
 import brainpy.math as bm
 from brainpy.math import activations
-from .base import Layer
+from brainpy._src.dnn.base import Layer
 from brainpy.check import (is_integer,
                            is_initializer)
 from brainpy.initialize import (XavierNormal,
                                 ZeroInit,
                                 Orthogonal,
                                 parameter,
                                 variable,
```

## brainpy/_src/dyn/_docs.py

```diff
@@ -7,14 +7,17 @@
 '''.strip()
 
 dpneu_doc = '''
     spk_fun: callable. The spike activation function.
     detach_spk: bool.
     method: str. The numerical integration method.
     spk_type: The spike data type.
+    spk_reset: The way to reset the membrane potential when the neuron generates spikes.
+        This parameter only works when the computing mode is ``TrainingMode``.
+        It can be ``soft`` and ``hard``. Default is ``soft``.
 '''.strip()
 
 ref_doc = '''
     tau_ref: float, ArrayType, callable. Refractory period length (ms).
     has_ref_var: bool. Whether has the refractory variable. Default is ``False``.
 '''.strip()
```

## brainpy/_src/dyn/base.py

```diff
@@ -1,181 +1,24 @@
-from typing import Sequence, Union, Callable, Any, Optional, Dict
+# -*- coding: utf-8 -*-
 
-import brainpy.math as bm
-from brainpy._src.dyn._docs import pneu_doc, dpneu_doc
-from brainpy._src.dynsys import NeuGroupNS, DynamicalSystemNS
-from brainpy._src.initialize.generic import parameter, variable_
-from brainpy._src.mixin import ParamDesc, ProjAutoDelay
-from brainpy.check import is_callable
+from brainpy._src.dynsys import Dynamic
+from brainpy._src.mixin import AutoDelaySupp, ParamDesc
 
 __all__ = [
-  'NeuDyn',
-  'SynDyn',
-  'SynOut',
+  'NeuDyn', 'SynDyn', 'IonChaDyn',
 ]
 
 
-class NeuDyn(NeuGroupNS, ProjAutoDelay):
-  """Parallelizable Neuron Group.
-
-  Args:
-    {pneu}
-  """
-
-  def __init__(
-      self,
-      size: Union[int, Sequence[int]],
-      sharding: Any = None,
-      keep_size: bool = False,
-      mode: bm.Mode = None,
-      name: str = None,
-      method: str = 'exp_auto'
-  ):
-    super().__init__(size=size,
-                     mode=mode,
-                     keep_size=keep_size,
-                     name=name)
-
-    # axis names for parallelization
-    self.sharding = sharding
-
-    # integration method
-    self.method = method
-
-    # the before- / after-updates used for computing
-    self.before_updates: Dict[str, Callable] = bm.node_dict()
-    self.after_updates: Dict[str, Callable] = bm.node_dict()
-
-    # outputs
-    self.cur_inputs: Dict[str, SynOut] = bm.node_dict()
-
-  def init_param(self, param, shape=None, sharding=None):
-    """Initialize parameters.
-
-    If ``sharding`` is provided and ``param`` is array, this function will
-    partition the parameter across the default device mesh.
-
-    See :py:func:`~.brainpy.math.sharding.device_mesh` for the mesh setting.
-    """
-    shape = self.varshape if shape is None else shape
-    sharding = self.sharding if sharding is None else sharding
-    return parameter(param,
-                     sizes=shape,
-                     allow_none=False,
-                     sharding=sharding)
-
-  def init_variable(self, var_data, batch_or_mode, shape=None, sharding=None):
-    """Initialize variables.
-
-    If ``sharding`` is provided and ``var_data`` is array, this function will
-    partition the variable across the default device mesh.
-
-    See :py:func:`~.brainpy.math.sharding.device_mesh` for the mesh setting.
-    """
-    shape = self.varshape if shape is None else shape
-    sharding = self.sharding if sharding is None else sharding
-    return variable_(var_data,
-                     sizes=shape,
-                     batch_or_mode=batch_or_mode,
-                     axis_names=sharding,
-                     batch_axis_name=bm.sharding.BATCH_AXIS)
-
-  def __call__(self, *args, **kwargs):
-    # update ``before_updates``
-    for model in tuple(self.before_updates.values()):
-      model()
-
-    # update the model self
-    ret = super().__call__(*args, **kwargs)
-
-    # update ``after_updates``
-    for model in tuple(self.after_updates.values()):
-      model(ret)
-    return ret
-
-
-NeuDyn.__doc__ = NeuDyn.__doc__.format(pneu=pneu_doc)
-
-
-class GradNeuDyn(NeuDyn):
-  """Differentiable and Parallelizable Neuron Group.
-
-  Args:
-    {pneu}
-    {dpneu}
-  """
-
-  supported_modes = (bm.TrainingMode, bm.NonBatchingMode)
-
-  def __init__(
-      self,
-      size: Union[int, Sequence[int]],
-      sharding: Any = None,
-      keep_size: bool = False,
-      mode: Optional[bm.Mode] = None,
-      name: Optional[str] = None,
-      method: str = 'exp_auto',
-
-      spk_fun: Callable = bm.surrogate.InvSquareGrad(),
-      spk_type: Any = None,
-      detach_spk: bool = False,
-  ):
-    super().__init__(size=size,
-                     mode=mode,
-                     keep_size=keep_size,
-                     name=name,
-                     sharding=sharding,
-                     method=method)
-
-    self.spk_fun = is_callable(spk_fun)
-    self.detach_spk = detach_spk
-    self._spk_type = spk_type
-
-  @property
-  def spk_type(self):
-    if self._spk_type is None:
-      return bm.float_ if isinstance(self.mode, bm.TrainingMode) else bm.bool_
-    else:
-      return self._spk_type
-
-
-GradNeuDyn.__doc__ = GradNeuDyn.__doc__.format(pneu=pneu_doc, dpneu=dpneu_doc)
-
-
-class SynDyn(NeuDyn, ParamDesc):
-  """Parallelizable synaptic dynamics.
-
-  :py:class:`~.PSynDyn` is a subclass of :py:class:`~.ParamDesc`, because it uses
-  the parameter description to describe the uniqueness of the synapse model.
-  """
+class NeuDyn(Dynamic, AutoDelaySupp):
+  """Neuronal Dynamics."""
   pass
 
 
-class SynOut(DynamicalSystemNS, ParamDesc):
-  def __init__(
-      self,
-      name: Optional[str] = None,
-  ):
-    super().__init__(name=name)
-    self._conductance = None
-
-  def bind_cond(self, conductance):
-    self._conductance = conductance
-
-  def unbind_cond(self):
-    self._conductance = None
-
-  def __call__(self, *args, **kwargs):
-    if self._conductance is None:
-      raise ValueError(f'Please first pack data at the current step using '
-                       f'".bind_cond(data)". {self}')
-    ret = self.update(self._conductance, *args, **kwargs)
-    return ret
-
-
-class HHTypeNeuLTC(NeuDyn):
+class SynDyn(Dynamic, AutoDelaySupp, ParamDesc):
+  """Synaptic Dynamics."""
   pass
 
 
-class HHTypeNeu(HHTypeNeuLTC):
+class IonChaDyn(Dynamic):
+  """Ion Channel Dynamics."""
   pass
```

## brainpy/_src/dyn/channels/__init__.py

```diff
@@ -1,25 +1,9 @@
 # -*- coding: utf-8 -*-
 
-"""
-
-Access through ``brainpy.channels``.
-"""
-
-from . import base, Ca, IH, K, Na, KCa, leaky
-
-__all__ = []
-__all__ += base.__all__
-__all__ += K.__all__
-__all__ += Na.__all__
-__all__ += Ca.__all__
-__all__ += IH.__all__
-__all__ += KCa.__all__
-__all__ += leaky.__all__
-
 from .base import *
-from .K import *
-from .Na import *
-from .IH import *
-from .Ca import *
-from .KCa import *
+from .potassium import *
+from .sodium import *
+from .hyperpolarization_activated import *
+from .calcium import *
+from .potassium_calcium import *
 from .leaky import *
```

## brainpy/_src/dyn/channels/base.py

```diff
@@ -1,158 +1,31 @@
 # -*- coding: utf-8 -*-
 
-from typing import Union
-
-import brainpy.math as bm
-from brainpy._src.dynsys import Container, CondNeuGroup, Channel, check_master
-from brainpy.types import Shape
+from brainpy._src.dyn.base import IonChaDyn
+from brainpy._src.mixin import TreeNode
+from brainpy._src.dyn.neurons.hh import HHTypedNeuron
 
 __all__ = [
-  'Ion', 'IonChannel',
-
-  # ions
-  'Calcium',
-
-  # ion channels
-  'IhChannel', 'CalciumChannel', 'SodiumChannel', 'PotassiumChannel', 'LeakyChannel',
+  'IonChannel',
 ]
 
 
-class Ion(Channel):
-  """Base class for ions."""
-
-  '''The type of the master object.'''
-  master_type = CondNeuGroup
-
-  def update(self, tdi, V):
-    raise NotImplementedError('Must be implemented by the subclass.')
-
-  def reset(self, V, batch_size=None):
-    self.reset_state(V, batch_size)
-
-  def reset_state(self, V, batch_size=None):
-    raise NotImplementedError('Must be implemented by the subclass.')
-
-  def current(self, V):
-    raise NotImplementedError('Must be implemented by the subclass.')
-
-  def __repr__(self):
-    return f'{self.__class__.__name__}(size={self.size})'
-
-
-class IonChannel(Channel):
+class IonChannel(IonChaDyn, TreeNode):
   """Base class for ion channels."""
 
   '''The type of the master object.'''
-  master_type = CondNeuGroup
+  master_type = HHTypedNeuron
 
-  def update(self, tdi, V):
+  def update(self, *args, **kwargs):
     raise NotImplementedError('Must be implemented by the subclass.')
 
-  def current(self, V):
+  def current(self, *args, **kwargs):
     raise NotImplementedError('Must be implemented by the subclass.')
 
-  def reset(self, V, batch_size=None):
-    self.reset_state(V, batch_size)
-
-  def reset_state(self, V, batch_size=None):
+  def reset_state(self, *args, **kwargs):
     raise NotImplementedError('Must be implemented by the subclass.')
 
-  def __repr__(self):
-    return f'{self.__class__.__name__}(size={self.size})'
-
-
-class Calcium(Ion, Container):
-  """The brainpy_object calcium dynamics.
-
-  Parameters
-  ----------
-  size: int, sequence of int
-    The size of the simulation target.
-  method: str
-    The numerical integration method.
-  name: str
-    The name of the object.
-  **channels
-    The calcium dependent channels.
-  """
-
-  '''The type of the master object.'''
-  master_type = CondNeuGroup
+  def clear_input(self):
+    pass
 
-  """Reversal potential."""
-  E: Union[float, bm.Variable, bm.Array]
-
-  """Calcium concentration."""
-  C: Union[float, bm.Variable, bm.Array]
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-      **channels
-  ):
-    Ion.__init__(self, size, keep_size=keep_size, mode=mode)
-    Container.__init__(self, name=name, mode=mode, **channels)
-    self.method = method
-
-  def current(self, V, C_Ca=None, E_Ca=None):
-    C_Ca = self.C if (C_Ca is None) else C_Ca
-    E_Ca = self.E if (E_Ca is None) else E_Ca
-    nodes = tuple(self.nodes(level=1, include_self=False).unique().subset(Channel).values())
-    check_master(type(self), *nodes)
-
-    if len(nodes) == 0:
-      return 0.
-    else:
-      current = nodes[0].current(V, C_Ca, E_Ca)
-      for node in nodes[1:]:
-        current += node.current(V, C_Ca, E_Ca)
-      return current
-
-  def register_implicit_nodes(self, *channels, **named_channels):
-    check_master(type(self), *channels, **named_channels)
-    super(Calcium, self).register_implicit_nodes(*channels, **named_channels)
-
-
-class CalciumChannel(IonChannel):
-  """Base class for Calcium ion channels."""
-
-  '''The type of the master object.'''
-  master_type = Calcium
-
-  def update(self, tdi, V, C_Ca, E_Ca):
-    raise NotImplementedError
-
-  def current(self, V, C_Ca, E_Ca):
-    raise NotImplementedError
-
-  def reset(self, V, C_Ca, E_Ca, batch_size=None):
-    self.reset_state(V, C_Ca, E_Ca, batch_size)
-
-  def reset_state(self, V, C_Ca, E_Ca, batch_size=None):
-    raise NotImplementedError('Must be implemented by the subclass.')
-
-
-class IhChannel(IonChannel):
-  """Base class for Ih channel models."""
-  master_type = CondNeuGroup
-
-
-class PotassiumChannel(IonChannel):
-  """Base class for potassium channel."""
-
-  '''The type of the master object.'''
-  master_type = CondNeuGroup
-
-
-class LeakyChannel(IonChannel):
-  """Base class for leaky channel."""
-  master_type = CondNeuGroup
-
-
-class SodiumChannel(IonChannel):
-  """Base class for sodium channel."""
-  master_type = CondNeuGroup
+  def __repr__(self):
+    return f'{self.name}(size={self.size})'
```

## brainpy/_src/dyn/channels/leaky.py

```diff
@@ -1,42 +1,51 @@
 # -*- coding: utf-8 -*-
 
 """
 This module implements leakage channels.
 
 """
 
-from typing import Union, Callable
+from typing import Union, Callable, Sequence
 
 import brainpy.math as bm
+from brainpy._src.dyn.neurons.hh import HHTypedNeuron
 from brainpy._src.initialize import Initializer, parameter
-from brainpy.types import ArrayType, Shape
-
-from .base import LeakyChannel
+from brainpy.types import ArrayType
+from .base import IonChannel
 
 __all__ = [
+  'LeakyChannel',
   'IL',
-  'IKL',
 ]
 
 
+class LeakyChannel(IonChannel):
+  """Base class for leaky channel dynamics."""
+
+  master_type = HHTypedNeuron
+
+  def reset_state(self, V, batch_size=None):
+    pass
+
+
 class IL(LeakyChannel):
   """The leakage channel current.
 
   Parameters
   ----------
   g_max : float
     The leakage conductance.
   E : float
     The reversal potential.
   """
 
   def __init__(
       self,
-      size,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       g_max: Union[int, float, ArrayType, Initializer, Callable] = 0.1,
       E: Union[int, float, ArrayType, Initializer, Callable] = -70.,
       method: str = None,
       name: str = None,
       mode: bm.Mode = None,
   ):
@@ -48,43 +57,12 @@
     self.E = parameter(E, self.varshape, allow_none=False)
     self.g_max = parameter(g_max, self.varshape, allow_none=False)
     self.method = method
 
   def reset_state(self, V, batch_size=None):
     pass
 
-  def update(self, tdi, V):
+  def update(self, V):
     pass
 
   def current(self, V):
     return self.g_max * (self.E - V)
-
-
-class IKL(IL):
-  """The potassium leak channel current.
-
-  Parameters
-  ----------
-  g_max : float
-    The potassium leakage conductance which is modulated by both
-    acetylcholine and norepinephrine.
-  E : float
-    The reversal potential.
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      g_max: Union[int, float, ArrayType, Initializer, Callable] = 0.005,
-      E: Union[int, float, ArrayType, Initializer, Callable] = -90.,
-      method: str = None,
-      name: str = None,
-      mode: bm.Mode = None,
-  ):
-    super(IKL, self).__init__(size=size,
-                              keep_size=keep_size,
-                              g_max=g_max,
-                              E=E,
-                              method=method,
-                              name=name,
-                              mode=mode)
```

## brainpy/_src/dyn/neurons/hh.py

```diff
@@ -1,30 +1,192 @@
 from functools import partial
-from typing import Union, Callable, Optional, Any, Sequence
+from typing import Any, Sequence
+from typing import Union, Callable, Optional
 
 import brainpy.math as bm
 from brainpy._src.context import share
-from brainpy._src.initialize import ZeroInit, OneInit, Uniform
-from brainpy._src.integrators import odeint, JointEq
+from brainpy._src.dyn.base import NeuDyn, IonChaDyn
+from brainpy._src.initialize import OneInit
+from brainpy._src.initialize import Uniform, variable_, noise as init_noise
+from brainpy._src.integrators import JointEq
+from brainpy._src.integrators import odeint, sdeint
+from brainpy._src.mixin import Container, TreeNode
+from brainpy._src.types import ArrayType
 from brainpy.check import is_initializer
-from brainpy.types import Shape, ArrayType, Sharding
-from brainpy._src.dyn.base import HHTypeNeuLTC
-
+from brainpy.types import Shape
 
 __all__ = [
+  'HHTypedNeuron',
+  'CondNeuGroupLTC',
+  'CondNeuGroup',
   'HHLTC',
   'HH',
   'MorrisLecarLTC',
   'MorrisLecar',
-  'WangBuzsakiModelLTC',
-  'WangBuzsakiModel'
+  'WangBuzsakiHHLTC',
+  'WangBuzsakiHH'
 ]
 
 
-class HHLTC(HHTypeNeuLTC):
+class HHTypedNeuron(NeuDyn):
+  pass
+
+
+class CondNeuGroupLTC(HHTypedNeuron, Container, TreeNode):
+  r"""Base class to model conductance-based neuron group.
+
+  The standard formulation for a conductance-based model is given as
+
+  .. math::
+
+      C_m {dV \over dt} = \sum_jg_j(E - V) + I_{ext}
+
+  where :math:`g_j=\bar{g}_{j} M^x N^y` is the channel conductance, :math:`E` is the
+  reversal potential, :math:`M` is the activation variable, and :math:`N` is the
+  inactivation variable.
+
+  :math:`M` and :math:`N` have the dynamics of
+
+  .. math::
+
+      {dx \over dt} = \phi_x {x_\infty (V) - x \over \tau_x(V)}
+
+  where :math:`x \in [M, N]`, :math:`\phi_x` is a temperature-dependent factor,
+  :math:`x_\infty` is the steady state, and :math:`\tau_x` is the time constant.
+  Equivalently, the above equation can be written as:
+
+  .. math::
+
+      \frac{d x}{d t}=\phi_{x}\left(\alpha_{x}(1-x)-\beta_{x} x\right)
+
+  where :math:`\alpha_{x}` and :math:`\beta_{x}` are rate constants.
+
+  .. versionadded:: 2.1.9
+     Model the conductance-based neuron model.
+
+  Parameters
+  ----------
+  size : int, sequence of int
+    The network size of this neuron group.
+  method: str
+    The numerical integration method.
+  name : optional, str
+    The neuron group name.
+
+  """
+
+  def __init__(
+      self,
+      size: Shape,
+      keep_size: bool = False,
+      C: Union[float, ArrayType, Callable] = 1.,
+      A: Union[float, ArrayType, Callable] = 1e-3,
+      V_th: Union[float, ArrayType, Callable] = 0.,
+      V_initializer: Union[Callable, ArrayType] = Uniform(-70, -60.),
+      noise: Optional[Union[float, ArrayType, Callable]] = None,
+      method: str = 'exp_auto',
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
+      init_var: bool = True,
+      input_var: bool = True,
+      spk_type: Optional[type] = None,
+      **channels
+  ):
+    super().__init__(size, keep_size=keep_size, mode=mode, name=name, )
+
+    # attribute for ``Container``
+    self.children = bm.node_dict(self.format_elements(IonChaDyn, **channels))
+
+    # parameters for neurons
+    self.input_var = input_var
+    self.C = C
+    self.A = A
+    self.V_th = V_th
+    self.noise = init_noise(noise, self.varshape, num_vars=1)
+    self._V_initializer = V_initializer
+    self.spk_type = ((bm.float_ if isinstance(self.mode, bm.TrainingMode) else bm.bool)
+                     if (spk_type is None) else spk_type)
+
+    # function
+    if self.noise is None:
+      self.integral = odeint(f=self.derivative, method=method)
+    else:
+      self.integral = sdeint(f=self.derivative, g=self.noise, method=method)
+
+    if init_var:
+      self.reset_state(self.mode)
+
+  def derivative(self, V, t, I):
+    # synapses
+    for out in self.cur_inputs.values():
+      I = I + out(V)
+    # channels
+    for ch in self.nodes(level=1, include_self=False).subset(IonChaDyn).unique().values():
+      I = I + ch.current(V)
+    return I / self.C
+
+  def reset_state(self, batch_size=None):
+    self.V = variable_(self._V_initializer, self.varshape, batch_size)
+    self.spike = variable_(partial(bm.zeros, dtype=self.spk_type), self.varshape, batch_size)
+    if self.input_var:
+      self.input = variable_(bm.zeros, self.varshape, batch_size)
+    for channel in self.nodes(level=1, include_self=False).subset(IonChaDyn).unique().values():
+      channel.reset_state(self.V.value, batch_size=batch_size)
+
+  def update(self, x=None):
+    # inputs
+    x = 0. if x is None else x
+    if self.input_var:
+      self.input += x
+      x = self.input.value
+    x = x * (1e-3 / self.A)
+
+    # integral
+    V = self.integral(self.V.value, share['t'], x, share['dt'])
+
+    # check whether the children channels have the correct parents.
+    channels = self.nodes(level=1, include_self=False).subset(IonChaDyn).unique()
+    self.check_hierarchies(self.__class__, **channels)
+
+    # update channels
+    for node in channels.values():
+      node(self.V.value)
+
+    # update variables
+    if self.spike.dtype == bool:
+      self.spike.value = bm.logical_and(V >= self.V_th, self.V < self.V_th)
+    else:
+      self.spike.value = bm.logical_and(V >= self.V_th, self.V < self.V_th).astype(self.spike.dtype)
+    self.V.value = V
+    return self.spike.value
+
+  def clear_input(self):
+    """Useful for monitoring inputs. """
+    if self.input_var:
+      self.input.value = bm.zeros_like(self.input)
+
+  def return_info(self):
+    return self.spike
+
+
+class CondNeuGroup(CondNeuGroupLTC):
+  def derivative(self, V, t, I):
+    for ch in self.nodes(level=1, include_self=False).subset(IonChaDyn).unique().values():
+      I = I + ch.current(V)
+    return I / self.C
+
+  def update(self, x=None):
+    # inputs
+    x = 0. if x is None else x
+    for out in self.cur_inputs.values():
+      x = x + out(self.V.value)
+    return super().update(x)
+
+
+class HHLTC(NeuDyn):
   r"""HodgkinHuxley neuron model with liquid time constant.
 
   **Model Descriptions**
 
   The Hodgkin-Huxley (HH; Hodgkin & Huxley, 1952) model [1]_ for the generation of
   the nerve action potential is one of the most successful mathematical models of
   a complex biological process that has ever been formulated. The basic concepts
@@ -187,14 +349,15 @@
          of membrane current and its application to conduction and excitation
          in nerve." The Journal of physiology 117.4 (1952): 500.
   .. [2] https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model
   .. [3] Ashwin, Peter, Stephen Coombes, and Rachel Nicks. "Mathematical
          frameworks for oscillatory network dynamics in neuroscience."
          The Journal of Mathematical Neuroscience 6, no. 1 (2016): 1-92.
   """
+
   def __init__(
       self,
       size: Union[int, Sequence[int]],
       sharding: Any = None,
       keep_size: bool = False,
       mode: bm.Mode = None,
       name: str = None,
@@ -302,15 +465,15 @@
     self.spike.value = bm.logical_and(self.V < self.V_th, V >= self.V_th)
     self.V.value = V
     self.m.value = m
     self.h.value = h
     self.n.value = n
     return self.spike.value
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
 
 class HH(HHLTC):
   r"""HodgkinHuxley neuron model.
 
     **Model Descriptions**
@@ -477,14 +640,15 @@
            of membrane current and its application to conduction and excitation
            in nerve." The Journal of physiology 117.4 (1952): 500.
     .. [2] https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model
     .. [3] Ashwin, Peter, Stephen Coombes, and Rachel Nicks. "Mathematical
            frameworks for oscillatory network dynamics in neuroscience."
            The Journal of Mathematical Neuroscience 6, no. 1 (2016): 1-92.
     """
+
   def dV(self, V, t, m, h, n, I):
     I_Na = (self.gNa * m ** 3.0 * h) * (V - self.ENa)
     I_K = (self.gK * n ** 4.0) * (V - self.EK)
     I_leak = self.gL * (V - self.EL)
     dVdt = (- I_Na - I_K - I_leak + I) / self.C
     return dVdt
 
@@ -492,18 +656,18 @@
   def derivative(self):
     return JointEq(self.dV, self.dm, self.dh, self.dn)
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
       x += out(self.V.value)
-    super().update(x)
+    return super().update(x)
 
 
-class MorrisLecarLTC(HHTypeNeuLTC):
+class MorrisLecarLTC(NeuDyn):
   r"""The Morris-Lecar neuron model with liquid time constant.
 
     **Model Descriptions**
 
     The Morris-Lecar model [4]_ (Also known as :math:`I_{Ca}+I_K`-model)
     is a two-dimensional "reduced" excitation model applicable to
     systems having two non-inactivating voltage-sensitive conductances.
@@ -568,14 +732,17 @@
     References
     ----------
 
     .. [4] Lecar, Harold. "Morris-lecar model." Scholarpedia 2.10 (2007): 1333.
     .. [5] http://www.scholarpedia.org/article/Morris-Lecar_model
     .. [6] https://en.wikipedia.org/wiki/Morris%E2%80%93Lecar_model
     """
+
+  supported_modes = (bm.NonBatchingMode, bm.BatchingMode)
+
   def __init__(
       self,
       size: Union[int, Sequence[int]],
       sharding: Any = None,
       keep_size: bool = False,
       mode: bm.Mode = None,
       name: str = None,
@@ -667,15 +834,15 @@
 
     spike = bm.logical_and(self.V < self.V_th, V >= self.V_th)
     self.V.value = V
     self.W.value = W
     self.spike.value = spike
     return spike
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
 
 class MorrisLecar(MorrisLecarLTC):
   r"""The Morris-Lecar neuron model.
 
       **Model Descriptions**
@@ -744,14 +911,15 @@
       References
       ----------
 
       .. [4] Lecar, Harold. "Morris-lecar model." Scholarpedia 2.10 (2007): 1333.
       .. [5] http://www.scholarpedia.org/article/Morris-Lecar_model
       .. [6] https://en.wikipedia.org/wiki/Morris%E2%80%93Lecar_model
       """
+
   def dV(self, V, t, W, I):
     M_inf = (1 / 2) * (1 + bm.tanh((V - self.V1) / self.V2))
     I_Ca = self.g_Ca * M_inf * (V - self.V_Ca)
     I_K = self.g_K * W * (V - self.V_K)
     I_Leak = self.g_leak * (V - self.V_leak)
     dVdt = (- I_Ca - I_K - I_Leak + I) / self.C
     return dVdt
@@ -766,18 +934,18 @@
   def derivative(self):
     return JointEq(self.dV, self.dW)
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
       x += out(self.V.value)
-    super().update(x)
+    return super().update(x)
 
 
-class WangBuzsakiModelLTC(HHTypeNeuLTC):
+class WangBuzsakiHHLTC(NeuDyn):
   r"""Wang-Buzsaki model [9]_, an implementation of a modified Hodgkin-Huxley model with liquid time constant.
 
     Each model is described by a single compartment and obeys the current balance equation:
 
     .. math::
 
         C_{m} \frac{d V}{d t}=-I_{\mathrm{Na}}-I_{\mathrm{K}}-I_{\mathrm{L}}-I_{\mathrm{syn}}+I_{\mathrm{app}}
@@ -853,14 +1021,15 @@
     References
     ----------
     .. [9] Wang, X.J. and Buzsaki, G., (1996) Gamma oscillation by synaptic
            inhibition in a hippocampal interneuronal network model. Journal of
            neuroscience, 16(20), pp.6402-6413.
 
     """
+
   def __init__(
       self,
       size: Union[int, Sequence[int]],
       sharding: Any = None,
       keep_size: bool = False,
       mode: bm.Mode = None,
       name: str = None,
@@ -956,18 +1125,19 @@
     V, h, n = self.integral(self.V, self.h, self.n, t, x, dt)
     self.spike.value = bm.logical_and(self.V < self.V_th, V >= self.V_th)
     self.V.value = V
     self.h.value = h
     self.n.value = n
     return self.spike.value
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
-class WangBuzsakiModel(WangBuzsakiModelLTC):
+
+class WangBuzsakiHH(WangBuzsakiHHLTC):
   r"""Wang-Buzsaki model [9]_, an implementation of a modified Hodgkin-Huxley model.
 
     Each model is described by a single compartment and obeys the current balance equation:
 
     .. math::
 
         C_{m} \frac{d V}{d t}=-I_{\mathrm{Na}}-I_{\mathrm{K}}-I_{\mathrm{L}}-I_{\mathrm{syn}}+I_{\mathrm{app}}
@@ -1043,14 +1213,15 @@
     References
     ----------
     .. [9] Wang, X.J. and Buzsaki, G., (1996) Gamma oscillation by synaptic
            inhibition in a hippocampal interneuronal network model. Journal of
            neuroscience, 16(20), pp.6402-6413.
 
   """
+
   def m_inf(self, V):
     alpha = -0.1 * (V + 35) / (bm.exp(-0.1 * (V + 35)) - 1)
     beta = 4. * bm.exp(-(V + 60.) / 18.)
     return alpha / (alpha + beta)
 
   def dh(self, h, t, V):
     alpha = 0.07 * bm.exp(-(V + 58) / 20)
@@ -1075,8 +1246,8 @@
   def derivative(self):
     return JointEq(self.dV, self.dh, self.dn)
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
       x += out(self.V.value)
-    super().update(x)
+    return super().update(x)
```

## brainpy/_src/dyn/neurons/lif.py

```diff
@@ -6,15 +6,15 @@
 import brainpy.math as bm
 from brainpy._src.context import share
 from brainpy._src.initialize import ZeroInit, OneInit
 from brainpy._src.integrators import odeint, JointEq
 from brainpy.check import is_initializer
 from brainpy.types import Shape, ArrayType, Sharding
 from brainpy._src.dyn._docs import ref_doc, lif_doc, pneu_doc, dpneu_doc, ltc_doc, if_doc
-from brainpy._src.dyn.base import GradNeuDyn
+from .base import GradNeuDyn
 
 __all__ = [
   'IF',
   'IFLTC',
   'Lif',
   'LifLTC',
   'LifRef',
@@ -63,23 +63,25 @@
   resistance.
 
   Args:
     %s
     %s
     %s
   """
+
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sequence[str]] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       init_var: bool = True,
 
       # neuron parameters
       V_rest: Union[float, ArrayType, Callable] = 0.,
       R: Union[float, ArrayType, Callable] = 1.,
@@ -91,15 +93,16 @@
                      name=name,
                      keep_size=keep_size,
                      mode=mode,
                      sharding=sharding,
                      spk_fun=spk_fun,
                      detach_spk=detach_spk,
                      method=method,
-                     spk_type=spk_type)
+                     spk_type=spk_type,
+                     spk_reset=spk_reset)
 
     # parameters
     self.V_rest = self.init_param(V_rest)
     self.tau = self.init_param(tau)
     self.R = self.init_param(R)
 
     # initializers
@@ -110,42 +113,44 @@
 
     # variables
     if init_var:
       self.reset_state(self.mode)
 
   def derivative(self, V, t, I):
     for out in self.cur_inputs.values():
-      I += out(V)
+      I = I + out(V)
     return (-V + self.V_rest + self.R * I) / self.tau
 
   def reset_state(self, batch_size=None):
     self.V = self.init_variable(self._V_initializer, batch_size)
+    self.spike = self.init_variable(partial(bm.zeros, dtype=self.spk_type), batch_size)
 
   def update(self, x=None):
     t = share.load('t')
     dt = share.load('dt')
     x = 0. if x is None else x
 
     # integrate membrane potential
     self.V.value = self.integral(self.V.value, t, x, dt)
+
     return self.V.value
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.V
 
 
 class IF(IFLTC):
   def derivative(self, V, t, I):
     return (-V + self.V_rest + self.R * I) / self.tau
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 IF.__doc__ = IFLTC.__doc__ % ('', if_doc, pneu_doc, dpneu_doc)
 IFLTC.__doc__ = IFLTC.__doc__ % (ltc_doc, if_doc, pneu_doc, dpneu_doc)
 
 
 class LifLTC(GradNeuDyn):
@@ -178,14 +183,15 @@
       size: Shape,
       sharding: Optional[Sequence[str]] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       init_var: bool = True,
 
       # neuron parameters
       V_rest: Union[float, ArrayType, Callable] = 0.,
       V_reset: Union[float, ArrayType, Callable] = -5.,
@@ -199,15 +205,16 @@
                      name=name,
                      keep_size=keep_size,
                      mode=mode,
                      sharding=sharding,
                      spk_fun=spk_fun,
                      detach_spk=detach_spk,
                      method=method,
-                     spk_type=spk_type)
+                     spk_type=spk_type,
+                     spk_reset=spk_reset)
 
     # parameters
     self.V_rest = self.init_param(V_rest)
     self.V_reset = self.init_param(V_reset)
     self.V_th = self.init_param(V_th)
     self.tau = self.init_param(tau)
     self.R = self.init_param(R)
@@ -220,15 +227,15 @@
 
     # variables
     if init_var:
       self.reset_state(self.mode)
 
   def derivative(self, V, t, I):
     for out in self.cur_inputs.values():
-      I += out(V)
+      I = I + out(V)
     return (-V + self.V_rest + self.R * I) / self.tau
 
   def reset_state(self, batch_size=None):
     self.V = self.init_variable(self._V_initializer, batch_size)
     self.spike = self.init_variable(partial(bm.zeros, dtype=self.spk_type), batch_size)
 
   def update(self, x=None):
@@ -239,37 +246,42 @@
     # integrate membrane potential
     V = self.integral(self.V.value, t, x, dt)
 
     # spike, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike
+      else:
+        raise ValueError
 
     else:
       spike = V >= self.V_th
       V = bm.where(spike, self.V_reset, V)
 
     self.V.value = V
     self.spike.value = spike
     return spike
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
 
 class Lif(LifLTC):
   def derivative(self, V, t, I):
     return (-V + self.V_rest + self.R * I) / self.tau
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 Lif.__doc__ = LifLTC.__doc__ % ('', lif_doc, pneu_doc, dpneu_doc)
 LifLTC.__doc__ = LifLTC.__doc__ % (ltc_doc, lif_doc, pneu_doc, dpneu_doc)
 
 
 class LifRefLTC(LifLTC):
@@ -305,14 +317,15 @@
       size: Shape,
       sharding: Optional[Sharding] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
       detach_spk: bool = False,
+      spk_reset: str = 'soft',
       method: str = 'exp_auto',
       name: Optional[str] = None,
       init_var: bool = True,
 
       # old neuron parameter
       V_rest: Union[float, ArrayType, Callable] = 0.,
       V_reset: Union[float, ArrayType, Callable] = -5.,
@@ -332,14 +345,15 @@
       keep_size=keep_size,
       mode=mode,
       method=method,
       sharding=sharding,
       spk_fun=spk_fun,
       detach_spk=detach_spk,
       spk_type=spk_type,
+      spk_reset=spk_reset,
 
       init_var=False,
 
       V_rest=V_rest,
       V_reset=V_reset,
       V_th=V_th,
       R=R,
@@ -347,20 +361,14 @@
       V_initializer=V_initializer,
     )
 
     # parameters
     self.ref_var = ref_var
     self.tau_ref = self.init_param(tau_ref)
 
-    # initializers
-    self._V_initializer = is_initializer(V_initializer)
-
-    # integral
-    self.integral = odeint(method=method, f=self.derivative)
-
     # variables
     if init_var:
       self.reset_state(self.mode)
 
   def reset_state(self, batch_size=None):
     super().reset_state(batch_size)
     self.t_last_spike = self.init_variable(bm.ones, batch_size)
@@ -382,15 +390,20 @@
       refractory = stop_gradient(refractory)
     V = bm.where(refractory, self.V.value, V)
 
     # spike, refractory, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike_no_grad = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike_no_grad
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike_no_grad
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike_no_grad
+      else:
+        raise ValueError
       spike_ = spike_no_grad > 0.
       # will be used in other place, like Delta Synapse, so stop its gradient
       if self.ref_var:
         self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
       t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
 
     else:
@@ -408,16 +421,16 @@
 class LifRef(LifRefLTC):
   def derivative(self, V, t, I):
     return (-V + self.V_rest + self.R * I) / self.tau
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 LifRef.__doc__ = LifRefLTC.__doc__ % ('', lif_doc, pneu_doc, dpneu_doc, ref_doc)
 LifRefLTC.__doc__ = LifRefLTC.__doc__ % (ltc_doc, lif_doc, pneu_doc, dpneu_doc, ref_doc)
 
 
 class ExpIFLTC(GradNeuDyn):
@@ -513,23 +526,25 @@
            are reliable predictors of naturalistic pyramidal-neuron voltage
            traces." Journal of Neurophysiology 99, no. 2 (2008): 656-666.
     .. [4] Richardson, Magnus JE. "Firing-rate response of linear and nonlinear
            integrate-and-fire neurons to modulated current-based and
            conductance-based synaptic drive." Physical Review E 76, no. 2 (2007): 021919.
     .. [5] https://en.wikipedia.org/wiki/Exponential_integrate-and-fire
     """
+
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sequence[str]] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       init_var: bool = True,
 
       # neuron parameters
       V_rest: Union[float, ArrayType, Callable] = -65.,
       V_reset: Union[float, ArrayType, Callable] = -68.,
@@ -545,15 +560,17 @@
                      name=name,
                      keep_size=keep_size,
                      mode=mode,
                      sharding=sharding,
                      spk_fun=spk_fun,
                      detach_spk=detach_spk,
                      method=method,
-                     spk_type=spk_type)
+                     spk_type=spk_type,
+                     spk_reset=spk_reset)
+
     # parameters
     self.V_rest = self.init_param(V_rest)
     self.V_reset = self.init_param(V_reset)
     self.V_th = self.init_param(V_th)
     self.V_T = self.init_param(V_T)
     self.delta_T = self.init_param(delta_T)
     self.tau = self.init_param(tau)
@@ -567,15 +584,15 @@
 
     # variables
     if init_var:
       self.reset_state(self.mode)
 
   def derivative(self, V, t, I):
     for out in self.cur_inputs.values():
-      I += out(V)
+      I = I + out(V)
     exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T)
     dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau
     return dvdt
 
   def reset_state(self, batch_size=None):
     self.V = self.init_variable(self._V_initializer, batch_size)
     self.spike = self.init_variable(partial(bm.zeros, dtype=self.spk_type), batch_size)
@@ -588,52 +605,57 @@
     # integrate membrane potential
     V = self.integral(self.V.value, t, x, dt)
 
     # spike, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike
+      else:
+        raise ValueError
 
     else:
       spike = V >= self.V_th
       V = bm.where(spike, self.V_reset, V)
 
     self.V.value = V
     self.spike.value = spike
     return spike
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
 
 class ExpIF(ExpIFLTC):
   def derivative(self, V, t, I):
     exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T)
     dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau
     return dvdt
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
-
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 class ExpIFRefLTC(ExpIFLTC):
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sharding] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
       detach_spk: bool = False,
+      spk_reset: str = 'soft',
       method: str = 'exp_auto',
       name: Optional[str] = None,
       init_var: bool = True,
 
       # old neuron parameter
       V_rest: Union[float, ArrayType, Callable] = -65.,
       V_reset: Union[float, ArrayType, Callable] = -68.,
@@ -655,14 +677,15 @@
       keep_size=keep_size,
       mode=mode,
       method=method,
       sharding=sharding,
       spk_fun=spk_fun,
       detach_spk=detach_spk,
       spk_type=spk_type,
+      spk_reset=spk_reset,
 
       init_var=False,
 
       V_rest=V_rest,
       V_reset=V_reset,
       V_th=V_th,
       V_T=V_T,
@@ -707,15 +730,20 @@
       refractory = stop_gradient(refractory)
     V = bm.where(refractory, self.V.value, V)
 
     # spike, refractory, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike_no_grad = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike_no_grad
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike_no_grad
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike_no_grad
+      else:
+        raise ValueError
       spike_ = spike_no_grad > 0.
       # will be used in other place, like Delta Synapse, so stop its gradient
       if self.ref_var:
         self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
       t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
 
     else:
@@ -735,16 +763,17 @@
     exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T)
     dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau
     return dvdt
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
+
 
 ExpIF.__doc__ = ExpIFLTC.__doc__ % ('')
 ExpIFRefLTC.__doc__ = ExpIFLTC.__doc__ % (ltc_doc)
 ExpIFRef.__doc__ = ExpIFLTC.__doc__ % ('')
 ExpIFLTC.__doc__ = ExpIFLTC.__doc__ % (ltc_doc)
 
 
@@ -818,23 +847,25 @@
   **References**
 
   .. [1] Fourcaud-Trocm, Nicolas, et al. "How spike generation
          mechanisms determine the neuronal response to fluctuating
          inputs." Journal of Neuroscience 23.37 (2003): 11628-11640.
   .. [2] http://www.scholarpedia.org/article/Adaptive_exponential_integrate-and-fire_model
   """
+
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sequence[str]] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       init_var: bool = True,
 
       # neuron parameters
       V_rest: Union[float, ArrayType, Callable] = -65.,
       V_reset: Union[float, ArrayType, Callable] = -68.,
@@ -854,15 +885,16 @@
                      name=name,
                      keep_size=keep_size,
                      mode=mode,
                      sharding=sharding,
                      spk_fun=spk_fun,
                      detach_spk=detach_spk,
                      method=method,
-                     spk_type=spk_type)
+                     spk_type=spk_type,
+                     spk_reset=spk_reset)
     # parameters
     self.V_rest = self.init_param(V_rest)
     self.V_reset = self.init_param(V_reset)
     self.V_th = self.init_param(V_th)
     self.V_T = self.init_param(V_T)
     self.a = self.init_param(a)
     self.b = self.init_param(b)
@@ -880,15 +912,15 @@
 
     # variables
     if init_var:
       self.reset_state(self.mode)
 
   def dV(self, V, t, w, I):
     for out in self.cur_inputs.values():
-      I += out(V)
+      I = I + out(V)
     exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T)
     dVdt = (- V + self.V_rest + exp - self.R * w + self.R * I) / self.tau
     return dVdt
 
   def dw(self, w, t, V):
     dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w
     return dwdt
@@ -910,28 +942,33 @@
     # integrate membrane potential
     V, w = self.integral(self.V.value, self.w.value, t, x, dt)
 
     # spike, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike
+      else:
+        raise ValueError
       w += self.b * spike
 
     else:
       spike = V >= self.V_th
       V = bm.where(spike, self.V_reset, V)
       w = bm.where(spike, w + self.b, w)
 
     self.V.value = V
     self.w.value = w
     self.spike.value = spike
     return spike
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
 
 class AdExIF(AdExIFLTC):
   def dV(self, V, t, w, I):
     exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T)
     dVdt = (- V + self.V_rest + exp - self.R * w + self.R * I) / self.tau
@@ -944,27 +981,28 @@
   @property
   def derivative(self):
     return JointEq([self.dV, self.dw])
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 class AdExIFRefLTC(AdExIFLTC):
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sharding] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       init_var: bool = True,
 
       # old neuron parameter
       V_rest: Union[float, ArrayType, Callable] = -65.,
@@ -991,14 +1029,15 @@
       keep_size=keep_size,
       mode=mode,
       method=method,
       sharding=sharding,
       spk_fun=spk_fun,
       detach_spk=detach_spk,
       spk_type=spk_type,
+      spk_reset=spk_reset,
 
       init_var=False,
 
       V_rest=V_rest,
       V_reset=V_reset,
       V_th=V_th,
       V_T=V_T,
@@ -1048,15 +1087,20 @@
       refractory = stop_gradient(refractory)
     V = bm.where(refractory, self.V.value, V)
 
     # spike, refractory, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike_no_grad = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike_no_grad
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike_no_grad
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike_no_grad
+      else:
+        raise ValueError
       w += self.b * spike_no_grad
       spike_ = spike_no_grad > 0.
       # will be used in other place, like Delta Synapse, so stop its gradient
       if self.ref_var:
         self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
       t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
 
@@ -1087,22 +1131,24 @@
   @property
   def derivative(self):
     return JointEq([self.dV, self.dw])
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
+
 
 AdExIF.__doc__ = AdExIFLTC.__doc__ % ('')
 AdExIFRefLTC.__doc__ = AdExIFLTC.__doc__ % (ltc_doc)
 AdExIFRef.__doc__ = AdExIFLTC.__doc__ % ('')
 AdExIFLTC.__doc__ = AdExIFLTC.__doc__ % (ltc_doc)
 
+
 class QuaIFLTC(GradNeuDyn):
   r"""Quadratic Integrate-and-Fire neuron model %s.
 
     **Model Descriptions**
 
     In contrast to physiologically accurate but computationally expensive
     neuron models like the HodgkinHuxley model, the QIF model [1]_ seeks only
@@ -1161,23 +1207,25 @@
 
     **References**
 
     .. [1]  P. E. Latham, B.J. Richmond, P. Nelson and S. Nirenberg
             (2000) Intrinsic dynamics in neuronal networks. I. Theory.
             J. Neurophysiology 83, pp. 808827.
     """
+
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sequence[str]] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       init_var: bool = True,
 
       # neuron parameters
       V_rest: Union[float, ArrayType, Callable] = -65.,
       V_reset: Union[float, ArrayType, Callable] = -68.,
@@ -1193,15 +1241,16 @@
                      name=name,
                      keep_size=keep_size,
                      mode=mode,
                      sharding=sharding,
                      spk_fun=spk_fun,
                      detach_spk=detach_spk,
                      method=method,
-                     spk_type=spk_type)
+                     spk_type=spk_type,
+                     spk_reset=spk_reset)
     # parameters
     self.V_rest = self.init_param(V_rest)
     self.V_reset = self.init_param(V_reset)
     self.V_th = self.init_param(V_th)
     self.V_c = self.init_param(V_c)
     self.c = self.init_param(c)
     self.R = self.init_param(R)
@@ -1215,15 +1264,15 @@
 
     # variables
     if init_var:
       self.reset_state(self.mode)
 
   def derivative(self, V, t, I):
     for out in self.cur_inputs.values():
-      I += out(V)
+      I = I + out(V)
     dVdt = (self.c * (V - self.V_rest) * (V - self.V_c) + self.R * I) / self.tau
     return dVdt
 
   def reset_state(self, batch_size=None):
     self.V = self.init_variable(self._V_initializer, batch_size)
     self.spike = self.init_variable(partial(bm.zeros, dtype=self.spk_type), batch_size)
 
@@ -1235,49 +1284,55 @@
     # integrate membrane potential
     V = self.integral(self.V.value, t, x, dt)
 
     # spike, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike
+      else:
+        raise ValueError
 
     else:
       spike = V >= self.V_th
       V = bm.where(spike, self.V_reset, V)
 
     self.V.value = V
     self.spike.value = spike
     return spike
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
 
 class QuaIF(QuaIFLTC):
   def derivative(self, V, t, I):
     dVdt = (self.c * (V - self.V_rest) * (V - self.V_c) + self.R * I) / self.tau
     return dVdt
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 class QuaIFRefLTC(QuaIFLTC):
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sharding] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       init_var: bool = True,
 
       # old neuron parameter
       V_rest: Union[float, ArrayType, Callable] = -65.,
@@ -1300,14 +1355,15 @@
       keep_size=keep_size,
       mode=mode,
       method=method,
       sharding=sharding,
       spk_fun=spk_fun,
       detach_spk=detach_spk,
       spk_type=spk_type,
+      spk_reset=spk_reset,
 
       init_var=False,
 
       V_rest=V_rest,
       V_reset=V_reset,
       V_th=V_th,
       V_c=V_c,
@@ -1352,15 +1408,20 @@
       refractory = stop_gradient(refractory)
     V = bm.where(refractory, self.V.value, V)
 
     # spike, refractory, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike_no_grad = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike_no_grad
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike_no_grad
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike_no_grad
+      else:
+        raise ValueError
       spike_ = spike_no_grad > 0.
       # will be used in other place, like Delta Synapse, so stop its gradient
       if self.ref_var:
         self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
       t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
 
     else:
@@ -1379,16 +1440,16 @@
   def derivative(self, V, t, I):
     dVdt = (self.c * (V - self.V_rest) * (V - self.V_c) + self.R * I) / self.tau
     return dVdt
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 QuaIF.__doc__ = QuaIFLTC.__doc__ % ('')
 QuaIFRefLTC.__doc__ = QuaIFLTC.__doc__ % (ltc_doc)
 QuaIFRef.__doc__ = QuaIFLTC.__doc__ % ('')
 QuaIFLTC.__doc__ = QuaIFLTC.__doc__ % (ltc_doc)
 
@@ -1465,23 +1526,25 @@
 
   .. [1] Izhikevich, E. M. (2004). Which model to use for cortical spiking
          neurons?. IEEE transactions on neural networks, 15(5), 1063-1070.
   .. [2] Touboul, Jonathan. "Bifurcation analysis of a general class of
          nonlinear integrate-and-fire neurons." SIAM Journal on Applied
          Mathematics 68, no. 4 (2008): 1045-1079.
   """
+
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sequence[str]] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       init_var: bool = True,
 
       # neuron parameters
       V_rest: Union[float, ArrayType, Callable] = -65.,
       V_reset: Union[float, ArrayType, Callable] = -68.,
@@ -1500,15 +1563,16 @@
                      name=name,
                      keep_size=keep_size,
                      mode=mode,
                      sharding=sharding,
                      spk_fun=spk_fun,
                      detach_spk=detach_spk,
                      method=method,
-                     spk_type=spk_type)
+                     spk_type=spk_type,
+                     spk_reset=spk_reset)
     # parameters
     self.V_rest = self.init_param(V_rest)
     self.V_reset = self.init_param(V_reset)
     self.V_th = self.init_param(V_th)
     self.V_c = self.init_param(V_c)
     self.a = self.init_param(a)
     self.b = self.init_param(b)
@@ -1525,15 +1589,15 @@
 
     # variables
     if init_var:
       self.reset_state(self.mode)
 
   def dV(self, V, t, w, I):
     for out in self.cur_inputs.values():
-      I += out(V)
+      I = I + out(V)
     dVdt = (self.c * (V - self.V_rest) * (V - self.V_c) - w + I) / self.tau
     return dVdt
 
   def dw(self, w, t, V):
     dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w
     return dwdt
 
@@ -1554,28 +1618,33 @@
     # integrate membrane potential
     V, w = self.integral(self.V.value, self.w.value, t, x, dt)
 
     # spike, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike
+      else:
+        raise ValueError
       w += self.b * spike
 
     else:
       spike = V >= self.V_th
       V = bm.where(spike, self.V_reset, V)
       w = bm.where(spike, w + self.b, w)
 
     self.V.value = V
     self.w.value = w
     self.spike.value = spike
     return spike
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
 
 class AdQuaIF(AdQuaIFLTC):
   def dV(self, V, t, w, I):
     dVdt = (self.c * (V - self.V_rest) * (V - self.V_c) - w + I) / self.tau
     return dVdt
@@ -1587,27 +1656,28 @@
   @property
   def derivative(self):
     return JointEq([self.dV, self.dw])
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 class AdQuaIFRefLTC(AdQuaIFLTC):
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sharding] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       init_var: bool = True,
 
       # old neuron parameter
       V_rest: Union[float, ArrayType, Callable] = -65.,
@@ -1633,14 +1703,15 @@
       keep_size=keep_size,
       mode=mode,
       method=method,
       sharding=sharding,
       spk_fun=spk_fun,
       detach_spk=detach_spk,
       spk_type=spk_type,
+      spk_reset=spk_reset,
 
       init_var=False,
 
       V_rest=V_rest,
       V_reset=V_reset,
       V_th=V_th,
       V_c=V_c,
@@ -1689,15 +1760,20 @@
       refractory = stop_gradient(refractory)
     V = bm.where(refractory, self.V.value, V)
 
     # spike, refractory, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike_no_grad = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike_no_grad
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike_no_grad
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike_no_grad
+      else:
+        raise ValueError
       w += self.b * spike_no_grad
       spike_ = spike_no_grad > 0.
       # will be used in other place, like Delta Synapse, so stop its gradient
       if self.ref_var:
         self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
       t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
 
@@ -1727,16 +1803,16 @@
   @property
   def derivative(self):
     return JointEq([self.dV, self.dw])
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 AdQuaIF.__doc__ = AdQuaIFLTC.__doc__ % ('')
 AdQuaIFRefLTC.__doc__ = AdQuaIFLTC.__doc__ % (ltc_doc)
 AdQuaIFRef.__doc__ = AdQuaIFLTC.__doc__ % ('')
 AdQuaIFLTC.__doc__ = AdQuaIFLTC.__doc__ % (ltc_doc)
 
@@ -1818,23 +1894,25 @@
            integrate-and-fire neural model produces diverse spiking
            behaviors." Neural computation 21.3 (2009): 704-718.
     .. [2] Teeter, Corinne, Ramakrishnan Iyer, Vilas Menon, Nathan
            Gouwens, David Feng, Jim Berg, Aaron Szafer et al. "Generalized
            leaky integrate-and-fire models classify multiple neuron types."
            Nature communications 9, no. 1 (2018): 1-15.
   """
+
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sequence[str]] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       init_var: bool = True,
 
       # neuron parameters
       V_rest: Union[float, ArrayType, Callable] = -70.,
       V_reset: Union[float, ArrayType, Callable] = -70.,
@@ -1860,15 +1938,16 @@
                      name=name,
                      keep_size=keep_size,
                      mode=mode,
                      sharding=sharding,
                      spk_fun=spk_fun,
                      detach_spk=detach_spk,
                      method=method,
-                     spk_type=spk_type)
+                     spk_type=spk_type,
+                     spk_reset=spk_reset, )
     # parameters
     self.V_rest = self.init_param(V_rest)
     self.V_reset = self.init_param(V_reset)
     self.V_th_inf = self.init_param(V_th_inf)
     self.V_th_reset = self.init_param(V_th_reset)
     self.R = self.init_param(R)
     self.a = self.init_param(a)
@@ -1901,15 +1980,15 @@
     return - self.k2 * I2
 
   def dVth(self, V_th, t, V):
     return self.a * (V - self.V_rest) - self.b * (V_th - self.V_th_inf)
 
   def dV(self, V, t, I1, I2, I):
     for out in self.cur_inputs.values():
-      I += out(V)
+      I = I + out(V)
     return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tau
 
   @property
   def derivative(self):
     return JointEq(self.dI1, self.dI2, self.dVth, self.dV)
 
   def reset_state(self, batch_size=None):
@@ -1927,70 +2006,66 @@
     # integrate membrane potential
     I1, I2, V_th, V = self.integral(self.I1.value, self.I2.value, self.V_th.value, self.V.value, t, x, dt)
 
     # spike, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike
+      else:
+        raise ValueError
       I1 += spike * (self.R1 * I1 + self.A1 - I1)
       I2 += spike * (self.R2 * I2 + self.A2 - I2)
-      reset_th = self.spk_fun(self.V_th_reset - V_th) * spike
-      V_th += reset_th * (self.V_th_reset - V_th)
+      V_th += (bm.maximum(self.V_th_reset, V_th) - V_th) * spike
 
     else:
       spike = self.V_th <= V
       V = bm.where(spike, self.V_reset, V)
       I1 = bm.where(spike, self.R1 * I1 + self.A1, I1)
       I2 = bm.where(spike, self.R2 * I2 + self.A2, I2)
       V_th = bm.where(spike, bm.maximum(self.V_th_reset, V_th), V_th)
     self.spike.value = spike
     self.I1.value = I1
     self.I2.value = I2
     self.V_th.value = V_th
     self.V.value = V
     return spike
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
 
 class Gif(GifLTC):
-  def dI1(self, I1, t):
-    return - self.k1 * I1
-
-  def dI2(self, I2, t):
-    return - self.k2 * I2
-
-  def dVth(self, V_th, t, V):
-    return self.a * (V - self.V_rest) - self.b * (V_th - self.V_th_inf)
-
   def dV(self, V, t, I1, I2, I):
     return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tau
 
   @property
   def derivative(self):
     return JointEq(self.dI1, self.dI2, self.dVth, self.dV)
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 class GifRefLTC(GifLTC):
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sharding] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       init_var: bool = True,
 
       # old neuron parameter
       V_rest: Union[float, ArrayType, Callable] = -70.,
@@ -2023,14 +2098,15 @@
       keep_size=keep_size,
       mode=mode,
       method=method,
       sharding=sharding,
       spk_fun=spk_fun,
       detach_spk=detach_spk,
       spk_type=spk_type,
+      spk_reset=spk_reset,
 
       init_var=False,
 
       V_rest=V_rest,
       V_reset=V_reset,
       V_th_inf=V_th_inf,
       V_th_reset=V_th_reset,
@@ -2088,19 +2164,23 @@
       refractory = stop_gradient(refractory)
     V = bm.where(refractory, self.V.value, V)
 
     # spike, refractory, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike_no_grad = stop_gradient(spike) if self.detach_spk else spike
-      V += (self.V_reset - V) * spike
+      if self.spk_reset == 'soft':
+        V -= (self.V_th - self.V_reset) * spike_no_grad
+      elif self.spk_reset == 'hard':
+        V += (self.V_reset - V) * spike_no_grad
+      else:
+        raise ValueError
       I1 += spike * (self.R1 * I1 + self.A1 - I1)
       I2 += spike * (self.R2 * I2 + self.A2 - I2)
-      reset_th = self.spk_fun(self.V_th_reset - V_th) * spike
-      V_th += reset_th * (self.V_th_reset - V_th)
+      V_th += (bm.maximum(self.V_th_reset, V_th) - V_th) * spike_no_grad
       spike_ = spike_no_grad > 0.
       # will be used in other place, like Delta Synapse, so stop its gradient
       if self.ref_var:
         self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
       t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
 
     else:
@@ -2118,41 +2198,28 @@
     self.V_th.value = V_th
     self.spike.value = spike
     self.t_last_spike.value = t_last_spike
     return spike
 
 
 class GifRef(GifRefLTC):
-  def dI1(self, I1, t):
-    return - self.k1 * I1
-
-  def dI2(self, I2, t):
-    return - self.k2 * I2
-
-  def dVth(self, V_th, t, V):
-    return self.a * (V - self.V_rest) - self.b * (V_th - self.V_th_inf)
-
   def dV(self, V, t, I1, I2, I):
     return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tau
 
-  @property
-  def derivative(self):
-    return JointEq(self.dI1, self.dI2, self.dVth, self.dV)
-
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
-Gif.__doc__ = GifLTC.__doc__ % ('')
-GifRefLTC.__doc__ = GifLTC.__doc__ % (ltc_doc)
-GifRef.__doc__ = GifLTC.__doc__ % ('')
-GifLTC.__doc__ = GifLTC.__doc__ % (ltc_doc)
+Gif.__doc__ = GifLTC.__doc__ % ''
+GifRefLTC.__doc__ = GifLTC.__doc__ % ltc_doc
+GifRef.__doc__ = GifLTC.__doc__ % ''
+GifLTC.__doc__ = GifLTC.__doc__ % ltc_doc
 
 
 class IzhikevichLTC(GradNeuDyn):
   r"""The Izhikevich neuron model %s.
 
     **Model Descriptions**
 
@@ -2214,23 +2281,25 @@
 
     .. [1] Izhikevich, Eugene M. "Simple model of spiking neurons." IEEE
            Transactions on neural networks 14.6 (2003): 1569-1572.
 
     .. [2] Izhikevich, Eugene M. "Which model to use for cortical spiking neurons?."
            IEEE transactions on neural networks 15.5 (2004): 1063-1070.
     """
+
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sequence[str]] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       name: Optional[str] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       init_var: bool = True,
 
       # neuron parameters
       V_th: Union[float, ArrayType, Callable] = 30.,
       a: Union[float, ArrayType, Callable] = 0.02,
@@ -2247,15 +2316,16 @@
                      name=name,
                      keep_size=keep_size,
                      mode=mode,
                      sharding=sharding,
                      spk_fun=spk_fun,
                      detach_spk=detach_spk,
                      method=method,
-                     spk_type=spk_type)
+                     spk_type=spk_type,
+                     spk_reset=spk_reset, )
     # parameters
     self.V_th = self.init_param(V_th)
     self.a = self.init_param(a)
     self.b = self.init_param(b)
     self.c = self.init_param(c)
     self.d = self.init_param(d)
     self.R = self.init_param(R)
@@ -2270,15 +2340,15 @@
 
     # variables
     if init_var:
       self.reset_state(self.mode)
 
   def dV(self, V, t, u, I):
     for out in self.cur_inputs.values():
-      I += out(V)
+      I = I + out(V)
     dVdt = 0.04 * V * V + 5 * V + 140 - u + I
     return dVdt
 
   def du(self, u, t, V):
     dudt = self.a * (self.b * V - u)
     return dudt
 
@@ -2301,28 +2371,28 @@
     # integrate membrane potential
     V, u = self.integral(self.V.value, self.u.value, t, x, dt)
 
     # spike, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike = stop_gradient(spike) if self.detach_spk else spike
-      V += spike * (self.c - self.V_th)
+      V += spike * (self.c - V)
       u += spike * self.d
 
     else:
       spike = V >= self.V_th
       V = bm.where(spike, self.c, V)
       u = bm.where(spike, u + self.d, u)
 
     self.V.value = V
     self.u.value = u
     self.spike.value = spike
     return spike
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.spike
 
 
 class Izhikevich(IzhikevichLTC):
   def dV(self, V, t, u, I):
     dVdt = 0.04 * V * V + 5 * V + 140 - u + I
     return dVdt
@@ -2334,27 +2404,28 @@
   @property
   def derivative(self):
     return JointEq([self.dV, self.du])
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
 class IzhikevichRefLTC(IzhikevichLTC):
   def __init__(
       self,
       size: Shape,
       sharding: Optional[Sharding] = None,
       keep_size: bool = False,
       mode: Optional[bm.Mode] = None,
       spk_fun: Callable = bm.surrogate.InvSquareGrad(),
       spk_type: Any = None,
+      spk_reset: str = 'soft',
       detach_spk: bool = False,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       init_var: bool = True,
 
       # old neuron parameter
       V_th: Union[float, ArrayType, Callable] = 30.,
@@ -2378,14 +2449,15 @@
       keep_size=keep_size,
       mode=mode,
       method=method,
       sharding=sharding,
       spk_fun=spk_fun,
       detach_spk=detach_spk,
       spk_type=spk_type,
+      spk_reset=spk_reset,
 
       init_var=False,
 
       V_th=V_th,
       a=a,
       b=b,
       c=c,
@@ -2432,15 +2504,15 @@
       refractory = stop_gradient(refractory)
     V = bm.where(refractory, self.V.value, V)
 
     # spike, refractory, spiking time, and membrane potential reset
     if isinstance(self.mode, bm.TrainingMode):
       spike = self.spk_fun(V - self.V_th)
       spike_no_grad = stop_gradient(spike) if self.detach_spk else spike
-      V += spike * (self.c - self.V_th)
+      V += spike * (self.c - V)
       u += spike * self.d
       spike_ = spike_no_grad > 0.
       # will be used in other place, like Delta Synapse, so stop its gradient
       if self.ref_var:
         self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
       t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
 
@@ -2470,15 +2542,15 @@
   @property
   def derivative(self):
     return JointEq([self.dV, self.du])
 
   def update(self, x=None):
     x = 0. if x is None else x
     for out in self.cur_inputs.values():
-      x += out(self.V.value)
-    super().update(x)
+      x = x + out(self.V.value)
+    return super().update(x)
 
 
-Izhikevich.__doc__ = IzhikevichLTC.__doc__ % ('')
-IzhikevichRefLTC.__doc__ = IzhikevichLTC.__doc__ % (ltc_doc)
-IzhikevichRef.__doc__ = IzhikevichLTC.__doc__ % ('')
-IzhikevichLTC.__doc__ = IzhikevichLTC.__doc__ % (ltc_doc)
+Izhikevich.__doc__ = IzhikevichLTC.__doc__ % ''
+IzhikevichRefLTC.__doc__ = IzhikevichLTC.__doc__ % ltc_doc
+IzhikevichRef.__doc__ = IzhikevichLTC.__doc__ % ''
+IzhikevichLTC.__doc__ = IzhikevichLTC.__doc__ % ltc_doc
```

## brainpy/_src/dyn/others/common.py

```diff
@@ -76,15 +76,15 @@
     t = share.load('t')
     dt = share.load('dt')
     self.x.value = self.integral(self.x.value, t, dt)
     if inp is not None:
       self.x += inp
     return self.x.value
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.x
 
 
 Leaky.__doc__ = Leaky.__doc__ % pneu_doc
 
 
 class Integrator(NeuDyn):
@@ -151,12 +151,12 @@
   def update(self, x=None):
     t = share.load('t')
     dt = share.load('dt')
     x = 0. if x is None else x
     self.x.value = self.integral(self.x.value, t, I_ext=x, dt=dt)
     return self.x.value
 
-  def return_for_delay(self):
+  def return_info(self):
     return self.x
 
 
 Integrator.__doc__ = Integrator.__doc__ % pneu_doc
```

## brainpy/_src/dyn/synapses/__init__.py

```diff
@@ -0,0 +1,4 @@
+00000000: 0a66 726f 6d20 2e61 6273 7472 6163 745f  .from .abstract_
+00000010: 6d6f 6465 6c73 2069 6d70 6f72 7420 2a0a  models import *.
+00000020: 6672 6f6d 202e 6269 6f5f 6d6f 6465 6c73  from .bio_models
+00000030: 2069 6d70 6f72 7420 2a0a                  import *.
```

## brainpy/_src/integrators/ode/exponential.py

```diff
@@ -134,15 +134,15 @@
 
   .. plot::
     :include-source: True
 
     >>> import brainpy as bp
     >>> import brainpy.math as bm
     >>>
-    >>> class HH(bp.NeuGroup):
+    >>> class HH(bp.dyn.NeuDyn):
     >>>   def __init__(self, size, ENa=55., EK=-90., EL=-65, C=1.0, gNa=35., gK=9.,
     >>>                gL=0.1, V_th=20., phi=5.0, name=None):
     >>>     super(HH, self).__init__(size=size, name=name)
     >>>
     >>>     # parameters
     >>>     self.ENa = ENa
     >>>     self.EK = EK
@@ -207,15 +207,15 @@
 
   .. plot::
     :include-source: True
 
     >>> import brainpy as bp
     >>> import brainpy.math as bm
     >>>
-    >>> class HH(bp.NeuGroup):
+    >>> class HH(bp.dyn.NeuDyn):
     >>>   def __init__(self, size, ENa=55., EK=-90., EL=-65, C=1.0, gNa=35., gK=9.,
     >>>                gL=0.1, V_th=20., phi=5.0, name=None):
     >>>     super(HH, self).__init__(size=size, name=name)
     >>>
     >>>     # parameters
     >>>     self.ENa = ENa
     >>>     self.EK = EK
```

## brainpy/_src/math/compat_numpy.py

```diff
@@ -1,8 +1,9 @@
 # -*- coding: utf-8 -*-
+
 import jax
 import jax.numpy as jnp
 import numpy as np
 from jax.tree_util import tree_flatten, tree_unflatten
 from jax.tree_util import tree_map
 
 from ._utils import _compatible_with_brainpy_array, _as_jax_array_
```

## brainpy/_src/math/compat_pytorch.py

```diff
@@ -30,15 +30,14 @@
   'atan',
   'arctan',
   'atan2',
   'atanh',
 ]
 
 
-
 Tensor = Array
 cat = concatenate
 
 
 def flatten(input: Union[jax.Array, Array],
             start_dim: Optional[int] = None,
             end_dim: Optional[int] = None) -> jax.Array:
```

## brainpy/_src/math/delayvars.py

```diff
@@ -337,14 +337,18 @@
     assert update_method in [ROTATE_UPDATE, CONCAT_UPDATE]
     self.update_method = update_method
     # attributes and variables
     self.data: Variable = None
     self.num_delay_step: int = 0
     self.idx: Variable = None
 
+    self.delay_target = None
+    if isinstance(delay_target, Variable):
+      self.delay_target = delay_target
+
     # initialization
     self.reset(delay_target, delay_len, initial_delay_data, batch_axis)
 
   @property
   def delay_shape(self):
     """The data shape of this delay variable."""
     return self.data.shape
@@ -444,22 +448,28 @@
       pass
     elif hasattr(delay_idx, 'dtype') and not jnp.issubdtype(delay_idx.dtype, jnp.integer):
       raise ValueError(f'"delay_len" must be integer, but we got {delay_idx}')
     indices = (delay_idx,) + tuple(indices)
     # the delay data
     return self.data[indices]
 
-  def update(self, value: Union[numbers.Number, Array, jax.Array]):
+  def update(self, value: Union[numbers.Number, Array, jax.Array] = None):
     """Update delay variable with the new data.
 
     Parameters
     ----------
     value: Any
       The value of the latest data, used to update this delay variable.
     """
+    if value is None:
+      if self.delay_target is None:
+        raise ValueError('Must provide value.')
+      else:
+        value = self.delay_target.value
+
     if self.update_method == ROTATE_UPDATE:
       self.idx.value = stop_gradient(as_jax((self.idx - 1) % self.num_delay_step))
       self.data[self.idx[0]] = value
 
     elif self.update_method == CONCAT_UPDATE:
       if self.num_delay_step >= 2:
         self.data.value = vstack([broadcast_to(value, self.data.shape[1:]), self.data[1:]])
```

## brainpy/_src/math/ndarray.py

```diff
@@ -76,22 +76,30 @@
       value = jnp.asarray(value)
     if dtype is not None:
       value = jnp.asarray(value, dtype=dtype)
     self._value = value
 
   def _check_tracer(self):
     self_value = self.value
-    if hasattr(self_value, '_trace'):
+    if hasattr(self_value, '_trace') and hasattr(self_value._trace.main, 'jaxpr_stack'):
       if len(self_value._trace.main.jaxpr_stack) == 0:
         raise RuntimeError('This Array is modified during the transformation. '
                            'BrainPy only supports transformations for Variable. '
                            'Please declare it as a Variable.') from jax.core.escaped_tracer_error(self_value, None)
     return self_value
 
   @property
+  def sharding(self):
+    return self._value.sharding
+
+  @property
+  def addressable_shards(self):
+    return self._value.addressable_shards
+
+  @property
   def value(self):
     return self._value
 
   @value.setter
   def value(self, value):
     self_value = self._check_tracer()
 
@@ -736,15 +744,15 @@
       The axis along which to split, default is 0.
 
     Returns
     -------
     sub-arrays : list of ndarrays
       A list of sub-arrays as views into `ary`.
     """
-    return [_return(a) for a in self.value.split(indices_or_sections, axis=axis)]
+    return [_return(a) for a in jnp.split(self.value, indices_or_sections, axis=axis)]
 
   def take(self, indices, axis=None, mode=None):
     """Return an array formed from the elements of a at the given indices."""
     return _return(self.value.take(indices=_as_jax_array_(indices), axis=axis, mode=mode))
 
   def tobytes(self):
     """Construct Python bytes containing the raw data bytes in the array.
```

## brainpy/_src/math/sharding.py

```diff
@@ -127,23 +127,30 @@
     sharding: Optional[Sharding] = None,
 ):
   """Partition inputs with the given sharding strategy."""
   if sharding is None:
     return x
   else:
     assert isinstance(sharding, Sharding)
-    f = partial(_device_put, device=sharding)
-    return jax.tree_util.tree_map(f, x, is_leaf=lambda a: isinstance(a, Array))
+    if isinstance(x, (Array, jax.Array)):
+      return _device_put(x, device=sharding)
+    return jax.tree_util.tree_map(partial(_device_put, device=sharding),
+                                  x,
+                                  is_leaf=lambda a: isinstance(a, Array))
 
 
 def partition(
     x: Any,
     sharding: Optional[Union[Sequence[str], jax.Device, Sharding]] = None,
 ):
   if sharding is None:
     return x
   elif isinstance(sharding, (jax.Device, Sharding)):
-    return jax.tree_util.tree_map(partial(_device_put, device=sharding), x, is_leaf=lambda a: isinstance(a, Array))
+    if isinstance(x, (Array, jax.Array)):
+      return _device_put(x, device=sharding)
+    return jax.tree_util.tree_map(partial(_device_put, device=sharding),
+                                  x,
+                                  is_leaf=lambda a: isinstance(a, Array))
   elif isinstance(sharding, (tuple, list)) and any([isinstance(s, str) for s in sharding]):
     return partition_by_axname(x, sharding)
   else:
     raise TypeError
```

## brainpy/_src/math/jitconn/_event_matvec.py

```diff
@@ -6,24 +6,24 @@
 import jax
 import numpy as np
 from jax import numpy as jnp, dtypes
 from jax.core import ShapedArray, Primitive
 from jax.interpreters import xla, ad
 from jax.lib import xla_client
 
-from brainpy._src.math.ndarray import _get_dtype
 from brainpy._src.math.interoperability import as_jax
 from brainpy._src.math.jitconn._matvec import (mv_prob_homo_p,
                                                mv_prob_uniform_p,
                                                mv_prob_normal_p,
                                                mv_prob_homo,
                                                mv_prob_uniform,
                                                mv_prob_normal)
+from brainpy._src.math.ndarray import _get_dtype
 from brainpy._src.math.op_registers import register_general_batching
-from brainpy.errors import GPUOperatorNotFound, MathError
+from brainpy.errors import GPUOperatorNotFound
 
 try:
   from brainpylib import gpu_ops
 except ImportError:
   gpu_ops = None
 
 __all__ = [
@@ -164,23 +164,15 @@
   return [out]
 
 
 def _event_matvec_prob_homo_cpu_translation(
     c, events, weight, clen, seed, *, shape, transpose, outdim_parallel
 ):
   n_row, n_col = (shape[1], shape[0]) if transpose else shape
-  event_shape = c.get_shape(events)
-  if event_shape.element_type() == jnp.bool_:
-    event_type = b'_bool'
-    out_dtype = dtypes.canonicalize_dtype(float)
-    type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
-  else:
-    out_dtype = event_shape.element_type()
-    event_type = b'_float' if out_dtype == jnp.float32 else b'_double'
-    type_name = event_type
+  out_dtype, event_type, type_name = _get_types(c.get_shape(events))
 
   if outdim_parallel:
     fn = b'cpu_event_matvec_prob_homo' + type_name + event_type
   else:
     fn = b'cpu_event_matvec_atomic_prob_homo' + type_name + event_type
 
   return xla_client.ops.CustomCallWithLayout(
@@ -208,23 +200,15 @@
 
 def _event_matvec_prob_homo_gpu_translation(
     c, events, weight, clen, seed, *, shape, transpose, outdim_parallel
 ):
   if gpu_ops is None:
     raise GPUOperatorNotFound(event_mv_prob_homo_p.name)
 
-  event_shape = c.get_shape(events)
-  if event_shape.element_type() == jnp.bool_:
-    event_type = b'_bool'
-    out_dtype = dtypes.canonicalize_dtype(float)
-    type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
-  else:
-    out_dtype = event_shape.element_type()
-    event_type = b'_float' if out_dtype == jnp.float32 else b'_double'
-    type_name = event_type
+  out_dtype, event_type, type_name = _get_types(c.get_shape(events))
 
   opaque = gpu_ops.build_double_size_descriptor(shape[1] if transpose else shape[0],
                                                 shape[0] if transpose else shape[1], )
 
   if outdim_parallel:
     fn = b'gpu_event_matvec_prob_homo_v2' + type_name + event_type
   else:
@@ -363,23 +347,15 @@
 
 
 def _event_matvec_prob_uniform_cpu_translation(
     c, events, w_low, w_high, clen, seed, *, shape, transpose, outdim_parallel
 ):
   n_row, n_col = (shape[1], shape[0]) if transpose else shape
 
-  event_shape = c.get_shape(events)
-  if event_shape.element_type() == jnp.bool_:
-    event_type = b'_bool'
-    out_dtype = dtypes.canonicalize_dtype(float)
-    type_name = b'_float' if (out_dtype == jnp.float32) else b'_double'
-  else:
-    out_dtype = event_shape.element_type()
-    event_type = b'_float' if (out_dtype == jnp.float32) else b'_double'
-    type_name = event_type
+  out_dtype, event_type, type_name = _get_types(c.get_shape(events))
 
   if outdim_parallel:
     fn = b'cpu_event_matvec_prob_uniform' + type_name + event_type
   else:
     fn = b'cpu_event_matvec_atomic_prob_uniform' + type_name + event_type
   return xla_client.ops.CustomCallWithLayout(
     c,
@@ -408,23 +384,15 @@
 
 def _event_matvec_prob_uniform_gpu_translation(
     c, events, w_low, w_high, clen, seed, *, shape, transpose, outdim_parallel
 ):
   if gpu_ops is None:
     raise GPUOperatorNotFound(event_mv_prob_uniform_p.name)
 
-  event_shape = c.get_shape(events)
-  if event_shape.element_type() == jnp.bool_:
-    event_type = b'_bool'
-    out_dtype = dtypes.canonicalize_dtype(float)
-    type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
-  else:
-    out_dtype = event_shape.element_type()
-    event_type = b'_float' if out_dtype == jnp.float32 else b'_double'
-    type_name = event_type
+  out_dtype, event_type, type_name = _get_types(c.get_shape(events))
 
   opaque = gpu_ops.build_double_size_descriptor(shape[1] if transpose else shape[0],
                                                 shape[0] if transpose else shape[1])
   if outdim_parallel:
     fn = b'gpu_event_matvec_prob_uniform_v2' + type_name + event_type
   else:
     fn = b'gpu_event_matvec_atomic_prob_uniform_v2' + type_name + event_type
@@ -509,15 +477,14 @@
     events, w_mu, w_sigma, clen, seed, *, shape, transpose, outdim_parallel
 ):
   assert _get_dtype(events) in [jnp.bool_, jnp.float32, jnp.float64]
   _w_mu_dtype = _get_dtype(w_mu)
   _w_sigma_dtype = _get_dtype(w_sigma)
   assert _w_mu_dtype == _w_sigma_dtype, '"w_mu" and "w_sigma" must be same typed.'
   assert _w_mu_dtype in [jnp.float32, jnp.float64], '"w_mu" must be float valued.'
-  assert _w_sigma_dtype in [jnp.float32, jnp.float64], '"w_sigma" must be float valued.'
   assert _get_dtype(clen) in [jnp.int32, jnp.int64, jnp.uint32, jnp.uint64]
   assert _get_dtype(seed) in [jnp.int32, jnp.int64, jnp.uint32, jnp.uint64]
 
   if w_mu.ndim != 1:
     raise ValueError('w_mu should be a 1D scalar.')
   if w_sigma.ndim != 1:
     raise ValueError('w_sigma should be a 1D scalar.')
@@ -543,28 +510,44 @@
     if events.shape[0] != shape[1]:
       raise ValueError(f'Shape mismatch, mat {shape} @ vec ({events.shape[0]},).')
 
   out = ShapedArray(dtype=w_mu.dtype, shape=(shape[1] if transpose else shape[0],))
   return [out]
 
 
+def _get_types(event_shape):
+  event_type = event_shape.element_type()
+  if event_type == jnp.bool_:
+    event_type = b'_bool'
+    out_dtype = dtypes.canonicalize_dtype(float)
+  elif event_type == jnp.float32:
+    event_type = b'_float'
+    out_dtype = event_shape.element_type()
+  elif event_type == jnp.float64:
+    event_type = b'_double'
+    out_dtype = event_shape.element_type()
+  else:
+    raise TypeError
+
+  if out_dtype == jnp.float32:
+    type_name = b'_float'
+  elif out_dtype == jnp.float64:
+    type_name = b'_double'
+  else:
+    raise TypeError
+
+  return out_dtype, event_type, type_name
+
+
 def _event_matvec_prob_normal_cpu_translation(
     c, events, w_mu, w_sigma, clen, seed, *, shape, transpose, outdim_parallel
 ):
   n_row, n_col = (shape[1], shape[0]) if transpose else shape
 
-  event_shape = c.get_shape(events)
-  if event_shape.element_type() == jnp.bool_:
-    event_type = b'_bool'
-    out_dtype = dtypes.canonicalize_dtype(float)
-    type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
-  else:
-    out_dtype = event_shape.element_type()
-    event_type = b'_float' if out_dtype == jnp.float32 else b'_double'
-    type_name = event_type
+  out_dtype, event_type, type_name = _get_types(c.get_shape(events))
 
   if outdim_parallel:
     fn = b'cpu_event_matvec_prob_normal' + type_name + event_type
   else:
     fn = b'cpu_event_matvec_atomic_prob_normal' + type_name + event_type
   return xla_client.ops.CustomCallWithLayout(
     c,
@@ -593,23 +576,16 @@
 
 def _event_matvec_prob_normal_gpu_translation(
     c, events, w_mu, w_sigma, clen, seed, *, shape, transpose, outdim_parallel
 ):
   if gpu_ops is None:
     raise GPUOperatorNotFound(event_mv_prob_normal_p.name)
 
-  event_shape = c.get_shape(events)
-  if event_shape.element_type() == jnp.bool_:
-    event_type = b'_bool'
-    out_dtype = dtypes.canonicalize_dtype(float)
-    type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
-  else:
-    out_dtype = event_shape.element_type()
-    event_type = b'_float' if out_dtype == jnp.float32 else b'_double'
-    type_name = event_type
+  out_dtype, event_type, type_name = _get_types(c.get_shape(events))
+
   opaque = gpu_ops.build_double_size_descriptor(shape[1] if transpose else shape[0],
                                                 shape[0] if transpose else shape[1])
   if outdim_parallel:
     fn = b'gpu_event_matvec_prob_normal_v2' + type_name + event_type
   else:
     fn = b'gpu_event_matvec_atomic_prob_normal_v2' + type_name + event_type
   return xla_client.ops.CustomCallWithLayout(
```

## brainpy/_src/math/object_transform/autograd.py

```diff
@@ -220,21 +220,25 @@
                 [v.value for v in self._grad_vars],  # variables for gradients
                 {},  # dynamical variables
                 *args,
                 **kwargs
               )
           cache_stack(self.target, stack)
 
-      self._dyn_vars = stack
-      self._dyn_vars.remove_var_by_id(*[id(v) for v in self._grad_vars])
-      self._eval_dyn_vars = True
+        self._dyn_vars = stack
+        self._dyn_vars.remove_var_by_id(*[id(v) for v in self._grad_vars])
+        self._eval_dyn_vars = True
 
-      # if not the outermost transformation
-      if current_transform_number():
-        return self._return(rets)
+        # if not the outermost transformation
+        if current_transform_number():
+          return self._return(rets)
+      else:
+        self._dyn_vars = stack
+        self._dyn_vars.remove_var_by_id(*[id(v) for v in self._grad_vars])
+        self._eval_dyn_vars = True
 
     rets = self._transform(
       [v.value for v in self._grad_vars],  # variables for gradients
       self._dyn_vars.dict_data(),  # dynamical variables
       *args,
       **kwargs
     )
```

## brainpy/_src/math/object_transform/base.py

```diff
@@ -645,16 +645,16 @@
   """
 
   def __init__(self, seq=()):
     super().__init__()
     self.extend(seq)
 
   def append(self, element) -> 'NodeList':
-    if not isinstance(element, BrainPyObject):
-      raise TypeError(f'element must be an instance of {BrainPyObject.__name__}.')
+    # if not isinstance(element, BrainPyObject):
+    #   raise TypeError(f'element must be an instance of {BrainPyObject.__name__}.')
     super().append(element)
     return self
 
   def extend(self, iterable) -> 'NodeList':
     for element in iterable:
       self.append(element)
     return self
@@ -664,18 +664,18 @@
 
 
 class NodeDict(dict):
   """A dictionary of :py:class:`~.BrainPyObject`, which is compatible with
   :py:func:`.vars()` operation in a :py:class:`~.BrainPyObject`.
   """
 
-  def _check_elem(self, elem):
-    if not isinstance(elem, BrainPyObject):
-      raise TypeError(f'Element should be {BrainPyObject.__name__}, but got {type(elem)}.')
-    return elem
+  # def _check_elem(self, elem):
+  #   if not isinstance(elem, BrainPyObject):
+  #     raise TypeError(f'Element should be {BrainPyObject.__name__}, but got {type(elem)}.')
+  #   return elem
 
   def __init__(self, *args, **kwargs):
     super().__init__()
     self.update(*args, **kwargs)
 
   def update(self, *args, **kwargs) -> 'VarDict':
     for arg in args:
@@ -686,13 +686,13 @@
         assert len(arg) == 2
         self[arg[0]] = args[1]
     for k, v in kwargs.items():
       self[k] = v
     return self
 
   def __setitem__(self, key, value) -> 'VarDict':
-    super().__setitem__(key, self._check_elem(value))
+    super().__setitem__(key, value)
     return self
 
 
 node_dict = NodeDict
```

## brainpy/_src/math/object_transform/controls.py

```diff
@@ -718,20 +718,21 @@
 def _get_for_loop_transform(
     body_fun,
     dyn_vars,
     bar: tqdm,
     progress_bar: bool,
     remat: bool,
     reverse: bool,
-    unroll: int
+    unroll: int,
+    unroll_kwargs: tools.DotDict
 ):
   def fun2scan(carry, x):
     for k in dyn_vars.keys():
       dyn_vars[k]._value = carry[k]
-    results = body_fun(*x)
+    results = body_fun(*x, **unroll_kwargs)
     if progress_bar:
       id_tap(lambda *arg: bar.update(), ())
     return dyn_vars.dict_data(), results
 
   if remat:
     fun2scan = jax.checkpoint(fun2scan)
 
@@ -749,14 +750,15 @@
     body_fun: Callable,
     operands: Any,
     reverse: bool = False,
     unroll: int = 1,
     remat: bool = False,
     jit: Optional[bool] = None,
     progress_bar: bool = False,
+    unroll_kwargs: Optional[Dict] = None,
 
     # deprecated
     dyn_vars: Union[Variable, Sequence[Variable], Dict[str, Variable]] = None,
     child_objs: Optional[Union[BrainPyObject, Sequence[BrainPyObject], Dict[str, BrainPyObject]]] = None,
 ):
   """``for-loop`` control flow with :py:class:`~.Variable`.
 
@@ -841,55 +843,64 @@
     The children objects used in the target function.
 
     .. versionadded:: 2.3.1
 
     .. deprecated:: 2.4.0
        No longer need to provide ``child_objs``. This function is capable of automatically
        collecting the children objects used in the target ``func``.
+  unroll_kwargs: dict
+    The keyword arguments without unrolling.
 
   Returns
   -------
   outs: Any
     The stacked outputs of ``body_fun`` when scanned over the leading axis of the inputs.
   """
 
   dynvar_deprecation(dyn_vars)
   node_deprecation(child_objs)
 
+  if unroll_kwargs is None:
+    unroll_kwargs = dict()
+  unroll_kwargs = tools.DotDict(unroll_kwargs)
+
   if not isinstance(operands, (list, tuple)):
     operands = (operands,)
 
   num_total = min([op.shape[0] for op in jax.tree_util.tree_flatten(operands)[0]])
   bar = None
   if progress_bar:
     bar = tqdm(total=num_total)
 
   if jit is None:  # jax disable jit
     jit = not jax.config.jax_disable_jit
-  dyn_vars = get_stack_cache(body_fun)
+  dyn_vars = get_stack_cache((body_fun, unroll_kwargs))
   if jit:
     if dyn_vars is None:
       # TODO: better cache mechanism?
       with new_transform('for_loop'):
         with VariableStack() as dyn_vars:
           transform = _get_for_loop_transform(body_fun, VariableStack(), bar,
-                                              progress_bar, remat, reverse, unroll)
+                                              progress_bar, remat, reverse, unroll,
+                                              unroll_kwargs)
           if current_transform_number() > 1:
             rets = transform(operands)
           else:
             rets = jax.eval_shape(transform, operands)
-      cache_stack(body_fun, dyn_vars)  # cache
+      cache_stack((body_fun, unroll_kwargs), dyn_vars)  # cache
       if current_transform_number():
         return rets[1]
       del rets
   else:
     dyn_vars = VariableStack()
 
   # TODO: cache mechanism?
-  transform = _get_for_loop_transform(body_fun, dyn_vars, bar, progress_bar, remat, reverse, unroll)
+  transform = _get_for_loop_transform(body_fun, dyn_vars, bar,
+                                      progress_bar, remat, reverse,
+                                      unroll, unroll_kwargs)
   if jit:
     dyn_vals, out_vals = transform(operands)
   else:
     with jax.disable_jit():
       dyn_vals, out_vals = transform(operands)
   for key in dyn_vars.keys():
     dyn_vars[key]._value = dyn_vals[key]
```

## brainpy/_src/math/object_transform/jit.py

```diff
@@ -73,20 +73,18 @@
 
   def __init__(
       self,
       fun: Callable,
       static_argnums: Union[int, Iterable[int], None] = None,
       static_argnames: Union[str, Iterable[str], None] = None,
       donate_argnums: Union[int, Iterable[int]] = (),
-      device: Optional[Any] = None,
       inline: bool = False,
       keep_unused: bool = False,
       abstracted_axes: Optional[Any] = None,
       name: Optional[str] = None,
-      backend: Optional[str] = None,
       in_shardings: Union[Sharding, UnspecifiedValue] = UNSPECIFIED,
       out_shardings: Union[Sharding, UnspecifiedValue] = UNSPECIFIED,
 
       # deprecated
       dyn_vars: Dict[str, Variable] = None,
       child_objs: Dict[str, BrainPyObject] = None,
   ):
@@ -100,19 +98,17 @@
 
     # target
     if hasattr(fun, '__self__') and isinstance(getattr(fun, '__self__'), BrainPyObject):
       self.register_implicit_nodes(getattr(fun, '__self__'))
     self.fun = fun
 
     # parameters
-    self._backend = backend
     self._static_argnums = _seq_of_int(static_argnums)
     self._static_argnames = _seq_of_str(static_argnames)
     self._donate_argnums = donate_argnums
-    self._device = device
     self._inline = inline
     self._keep_unused = keep_unused
     self._abstracted_axes = abstracted_axes
     self._in_shardings = in_shardings
     self._out_shardings = out_shardings
     # if isinstance(in_shardings, UnspecifiedValue):
     #   pass
@@ -149,19 +145,17 @@
           **kwargs
         )
         self._transform = jax.jit(
           self._transform_function,
           static_argnums=jax.tree_util.tree_map(lambda a: a + 1, self._static_argnums),
           static_argnames=self._static_argnames,
           donate_argnums=self._donate_argnums,
-          device=self._device,
           inline=self._inline,
           keep_unused=self._keep_unused,
           abstracted_axes=self._abstracted_axes,
-          backend=self._backend,
           in_shardings=self._in_shardings,
           out_shardings=self._out_shardings,
         )
 
       # if not the outermost transformation
       if current_transform_number():
         return rets
@@ -230,27 +224,25 @@
 def jit(
     func: Callable = None,
 
     # original jax.jit parameters
     static_argnums: Union[int, Iterable[int], None] = None,
     static_argnames: Union[str, Iterable[str], None] = None,
     donate_argnums: Union[int, Sequence[int]] = (),
-    device: Optional[Any] = None,
     inline: bool = False,
     keep_unused: bool = False,
-    backend: Optional[str] = None,
     abstracted_axes: Optional[Any] = None,
 
     # deprecated
     dyn_vars: Optional[Union[Variable, Sequence[Variable], Dict[str, Variable]]] = None,
     child_objs: Optional[Union[BrainPyObject, Sequence[BrainPyObject], Dict[str, BrainPyObject]]] = None,
 
     # others
     **kwargs,
-) -> JITTransform:
+) -> Union[Callable, JITTransform]:
   """
   JIT (Just-In-Time) compilation for BrainPy computation.
 
   This function has the same ability to just-in-time compile a pure function,
   but it can also JIT compile a :py:class:`brainpy.DynamicalSystem`, or a
   :py:class:`brainpy.BrainPyObject` object.
 
@@ -310,43 +302,38 @@
   if func is None:
     return lambda f: JITTransform(fun=f,
                                   dyn_vars=dyn_vars,
                                   child_objs=child_objs,
                                   static_argnums=static_argnums,
                                   static_argnames=static_argnames,
                                   donate_argnums=donate_argnums,
-                                  device=device,
                                   inline=inline,
                                   keep_unused=keep_unused,
-                                  backend=backend,
                                   abstracted_axes=abstracted_axes,
                                   **kwargs)
   else:
     return JITTransform(fun=func,
                         dyn_vars=dyn_vars,
                         child_objs=child_objs,
                         static_argnums=static_argnums,
                         static_argnames=static_argnames,
                         donate_argnums=donate_argnums,
-                        device=device,
                         inline=inline,
                         keep_unused=keep_unused,
-                        backend=backend,
                         abstracted_axes=abstracted_axes,
                         **kwargs)
 
 
 jit.__doc__ = jit.__doc__.format(jit_par=_jit_par.strip())
 
 
 def cls_jit(
     func: Callable = None,
     static_argnums: Union[int, Iterable[int], None] = None,
     static_argnames: Union[str, Iterable[str], None] = None,
-    device: Optional[Any] = None,
     inline: bool = False,
     keep_unused: bool = False,
     abstracted_axes: Optional[Any] = None,
     **kwargs
 ) -> Callable:
   """Just-in-time compile a function and then the jitted function as the bound method for a class.
   
@@ -382,24 +369,22 @@
   func : JITTransform
     A callable jitted function, set up for just-in-time compilation.
   """
   if func is None:
     return lambda f: _make_jit_fun(fun=f,
                                    static_argnums=static_argnums,
                                    static_argnames=static_argnames,
-                                   device=device,
                                    inline=inline,
                                    keep_unused=keep_unused,
                                    abstracted_axes=abstracted_axes,
                                    **kwargs)
   else:
     return _make_jit_fun(fun=func,
                          static_argnums=static_argnums,
                          static_argnames=static_argnames,
-                         device=device,
                          inline=inline,
                          keep_unused=keep_unused,
                          abstracted_axes=abstracted_axes,
                          **kwargs)
 
 
 cls_jit.__doc__ = cls_jit.__doc__.format(jit_pars=_jit_par)
@@ -416,21 +401,22 @@
     **jit_kwargs
 ):
   static_argnums = _seq_of_int(static_argnums)
   static_argnames = _seq_of_int(static_argnames)
 
   @wraps(fun)
   def call_fun(self, *args, **kwargs):
-    fun2 = partial(fun, self)
     if jax.config.jax_disable_jit:
-      return fun2(*args, **kwargs)
+      return fun(self, *args, **kwargs)
 
     hash_v = hash(fun) + hash(self)
     cache = get_stack_cache(hash_v)  # TODO: better cache mechanism
     if cache is None:
+      fun2 = partial(fun, self)
+      
       with jax.ensure_compile_time_eval():
         if len(static_argnums) or len(static_argnames):
           fun3, args_, kwargs_ = _partial_fun(fun2, args, kwargs, static_argnums, static_argnames)
         else:
           args_, kwargs_, fun3 = args, kwargs, fun2
         with VariableStack() as stack:
           _ = jax.eval_shape(fun3, *args_, **kwargs_)
```

## brainpy/_src/math/object_transform/naming.py

```diff
@@ -28,15 +28,15 @@
         f'>>> brainpy.brainpy_object.clear_name_cache() \n\n'
         f'to clear all cached names. '
       )
   else:
     _name2id[name] = id(obj)
 
 
-def get_unique_name(type_):
+def get_unique_name(type_: str):
   """Get the unique name for the given object type."""
   if type_ not in _typed_names:
     _typed_names[type_] = 0
   name = f'{type_}{_typed_names[type_]}'
   _typed_names[type_] += 1
   return name
```

## brainpy/_src/math/op_registers/numba_approach/__init__.py

```diff
@@ -3,15 +3,14 @@
 import warnings
 from functools import partial
 from typing import Callable
 from typing import Union, Sequence
 
 import numba
 from jax import core
-from jax.abstract_arrays import ShapedArray
 from jax.interpreters import xla, batching, ad
 from jax.tree_util import tree_map
 from numba.core.dispatcher import Dispatcher
 
 from brainpy._src.math.ndarray import Array
 from brainpy._src.math.object_transform.base import BrainPyObject
 from .cpu_translation import _cpu_translation, compile_cpu_signature_with_numba
@@ -108,15 +107,15 @@
     res = self.op.bind(*args, **kwargs)
     return res
 
 
 def register_op_with_numba(
     op_name: str,
     cpu_func: Callable,
-    out_shapes: Union[Callable, ShapedArray, Sequence[ShapedArray]],
+    out_shapes: Union[Callable, core.ShapedArray, Sequence[core.ShapedArray]],
     gpu_func_translation: Callable = None,
     batching_translation: Callable = None,
     jvp_translation: Callable = None,
     transpose_translation: Callable = None,
     multiple_results: bool = False,
 ):
   """
@@ -171,19 +170,19 @@
   # output shape evaluation function
   def abs_eval_rule(*input_shapes, **info):
     if callable(out_shapes):
       shapes = out_shapes(*input_shapes, **info)
     else:
       shapes = out_shapes
 
-    if isinstance(shapes, ShapedArray):
+    if isinstance(shapes, core.ShapedArray):
       pass
     elif isinstance(shapes, (tuple, list)):
       for elem in shapes:
-        if not isinstance(elem, ShapedArray):
+        if not isinstance(elem, core.ShapedArray):
           raise ValueError(f'Elements in "out_shapes" must be instances of '
                            f'jax.abstract_arrays.ShapedArray, but we got '
                            f'{type(elem)}: {elem}')
     else:
       raise ValueError(f'Unknown type {type(shapes)}, only '
                        f'supports function, ShapedArray or '
                        f'list/tuple of ShapedArray.')
```

## brainpy/_src/measure/correlation.py

```diff
@@ -103,14 +103,18 @@
     res = _cc(*indices)
   else:
     raise UnsupportedError(f'Do not support {method}. We only support "loop" or "vmap".')
 
   return np.mean(np.asarray(res))
 
 
+def _f_signal(signal):
+  return jnp.mean(signal * signal) - jnp.mean(signal) ** 2
+
+
 def voltage_fluctuation(potentials, numpy=True, method='loop'):
   r"""Calculate neuronal synchronization via voltage variance.
 
   The method comes from [1]_ [2]_ [3]_.
 
   First, average over the membrane potential :math:`V`
 
@@ -139,57 +143,46 @@
   of :math:`N` neurons by:
 
   .. math::
 
       \chi^2 \left( N \right) = \frac{\sigma_V^2}{ \frac{1}{N} \sum_{i=1}^N
       \sigma_{V_i}^2}
 
-  Parameters
-  ----------
-  potentials : ndarray
-    The membrane potential matrix of the neuron group.
-  numpy: bool
-    Whether we use numpy array as the functional output.
-    If ``False``, this function can be JIT compiled.
-  method: str
-    The method to calculate all pairs of cross correlation.
-    Supports two kinds of methods: `loop` and `vmap`.
-    `vmap` method will consume much more memory.
-
-    .. versionadded:: 2.2.3.4
-
-
-  Returns
-  -------
-  sync_index : float
-      The synchronization index.
-
-  References
-  ----------
   .. [1] Golomb, D. and Rinzel J. (1993) Dynamics of globally coupled
          inhibitory neurons with heterogeneity. Phys. Rev. E 48:4810-4814.
   .. [2] Golomb D. and Rinzel J. (1994) Clustering in globally coupled
          inhibitory neurons. Physica D 72:259-282.
   .. [3] David Golomb (2007) Neuronal synchrony measures. Scholarpedia, 2(1):1347.
+
+  Args:
+    potentials: The membrane potential matrix of the neuron group.
+    numpy: Whether we use numpy array as the functional output. If ``False``, this function can be JIT compiled.
+    method: The method to calculate all pairs of cross correlation.
+       Supports two kinds of methods: `loop` and `vmap`.
+      `vmap` method will consume much more memory.
+
+      .. versionadded:: 2.2.3.4
+
+  Returns:
+    sync_index: The synchronization index.
   """
 
   potentials = bm.as_jax(potentials)
   avg = jnp.mean(potentials, axis=1)
   avg_var = jnp.mean(avg * avg) - jnp.mean(avg) ** 2
 
   if method == 'loop':
-    _var = lambda aa: bm.for_loop(lambda signal: jnp.mean(signal * signal) - jnp.mean(signal) ** 2,
-                                  operands=jnp.moveaxis(aa, 0, 1))
+    _var = bm.for_loop(_f_signal, operands=jnp.moveaxis(potentials, 0, 1))
 
   elif method == 'vmap':
-    _var = vmap(lambda signal: jnp.mean(signal * signal) - jnp.mean(signal) ** 2, in_axes=1)
+    _var = vmap(_f_signal, in_axes=1)(potentials)
   else:
     raise UnsupportedError(f'Do not support {method}. We only support "loop" or "vmap".')
 
-  var_mean = jnp.mean(_var(potentials))
+  var_mean = jnp.mean(_var)
   r = jnp.where(var_mean == 0., 1., avg_var / var_mean)
   return bm.as_numpy(r) if numpy else r
 
 
 def matrix_correlation(x, y, numpy=True):
   """Pearson correlation of the lower triagonal of two matrices.
```

## brainpy/_src/running/jax_multiprocessing.py

```diff
@@ -56,16 +56,18 @@
   res_tree = None
   results = None
   vmap_func = vmap(func)
   for i in range(0, num_pars[0], num_parallel):
     run_f = vmap(func) if clear_buffer else vmap_func
     if isinstance(arguments, dict):
       r = run_f(**tree_unflatten(tree, [ele[i: i + num_parallel] for ele in elements]))
-    else:
+    elif isinstance(arguments, (tuple, list)):
       r = run_f(*tree_unflatten(tree, [ele[i: i + num_parallel] for ele in elements]))
+    else:
+      raise TypeError
     res_values, res_tree = tree_flatten(r, is_leaf=lambda a: isinstance(a, bm.Array))
     if results is None:
       results = tuple([np.asarray(val) if clear_buffer else val] for val in res_values)
     else:
       for j, val in enumerate(res_values):
         results[j].append(np.asarray(val) if clear_buffer else val)
     if clear_buffer:
```

## brainpy/_src/running/runner.py

```diff
@@ -114,24 +114,22 @@
         warnings.warn("`fun_monitors` is deprecated since version 2.3.1. "
                       "Define `func_monitors` in `monitors`")
       check.is_dict_data(fun_monitors, key_type=str, val_type=types.FunctionType)
       self._monitors.update(fun_monitors)
 
     # monitor for user access
     self.mon = DotDict()
-    self.mon['var_names'] = tuple(self._monitors.keys())
 
     # progress bar
     assert isinstance(progress_bar, bool), 'Must be a boolean variable.'
     self.progress_bar = progress_bar
     self._pbar = None
 
     # dynamical changed variables
     self._dyn_vars = check.is_all_vars(dyn_vars, out_as='dict')
-    self.register_implicit_vars(self._dyn_vars)
 
     # numpy mon after run
     self.numpy_mon_after_run = numpy_mon_after_run
 
   def _format_seq_monitors(self, monitors):
     if not isinstance(monitors, (tuple, list)):
       raise TypeError(f'Must be a tuple/list, but we got {type(monitors)}')
```

## brainpy/_src/tools/dicts.py

```diff
@@ -38,72 +38,28 @@
   a PyTree.
 
   >>> from jax import jit
   >>> f = jit(lambda x: x)
   >>> f(d)
   TypeError: Argument 'a' of type <class 'str'> is not a valid JAX type.
 
-  At this moment, you can label this attribute `names` as not a key in the dictionary
-  by using the syntax::
-
-  >>> d.add_attr_not_key('names')
-  >>> f(d)
-  {'a': DeviceArray(10, dtype=int32, weak_type=True),
-   'b': DeviceArray(20, dtype=int32, weak_type=True),
-   'c': DeviceArray(30, dtype=int32, weak_type=True)}
-
   """
 
-  '''Used to exclude variables that '''
-  attrs_not_keys = ('attrs_not_keys', 'var_names')
-
   def __init__(self, *args, **kwargs):
     super().__init__(*args, **kwargs)
     self.__dict__ = self
-    self.var_names = ()
 
   def copy(self) -> 'DotDict':
     return type(self)(super().copy())
 
-  def keys(self):
-    """Retrieve all keys in the dict, excluding ignored keys."""
-    keys = []
-    for k in super(DotDict, self).keys():
-      if k not in self.attrs_not_keys:
-        keys.append(k)
-    return tuple(keys)
-
-  def values(self):
-    """Retrieve all values in the dict, excluding values of ignored keys."""
-    values = []
-    for k, v in super(DotDict, self).items():
-      if k not in self.attrs_not_keys:
-        values.append(v)
-    return tuple(values)
-
-  def items(self):
-    """Retrieve all items in the dict, excluding ignored items."""
-    items = []
-    for k, v in super(DotDict, self).items():
-      if k not in self.attrs_not_keys:
-        items.append((k, v))
-    return items
-
   def to_numpy(self):
     """Change all values to numpy arrays."""
     for key in tuple(self.keys()):
       self[key] = np.asarray(self[key])
 
-  def add_attr_not_key(self, *args):
-    """Add excluded attribute when retrieving dictionary keys. """
-    for arg in args:
-      if not isinstance(arg, str):
-        raise TypeError('Only support string.')
-    self.attrs_not_keys += args
-
   def update(self, *args, **kwargs):
     super().update(*args, **kwargs)
     return self
 
   def __add__(self, other):
     """Merging two dicts.
 
@@ -175,15 +131,15 @@
   def subset(self, var_type):
     """Get the subset of the (key, value) pair.
 
     ``subset()`` can be used to get a subset of some class:
 
     >>> import brainpy as bp
     >>>
-    >>> some_collector = Collector()
+    >>> some_collector = DotDict()
     >>>
     >>> # get all trainable variables
     >>> some_collector.subset(bp.math.TrainVar)
     >>>
     >>> # get all Variable
     >>> some_collector.subset(bp.math.Variable)
 
@@ -213,13 +169,16 @@
     seen = set()
     for k, v in self.items():
       if id(v) not in seen:
         seen.add(id(v))
         gather[k] = v
     return gather
 
+  def __hash__(self):
+    return hash(tuple(sorted(self.items())))
+
 
 register_pytree_node(
   DotDict,
   lambda x: (x.values(), x.keys()),
   lambda keys, values: DotDict(safe_zip(keys, values))
 )
```

## brainpy/_src/train/__init__.py

```diff
@@ -17,9 +17,8 @@
 
 - reservoir computing networks,
 - artificial recurrent neural networks,
 - spiking neural networks,
 - and others.
 """
 
-
-
+from . import base, back_propagation, online, offline
```

## brainpy/_src/train/back_propagation.py

```diff
@@ -1,27 +1,25 @@
 # -*- coding: utf-8 -*-
 
 import time
 from collections.abc import Iterable
-from functools import partial
 from typing import Union, Dict, Callable, Sequence, Optional
 
 import jax.numpy as jnp
 import numpy as np
 from jax.tree_util import tree_map
 from tqdm import tqdm
 
+from brainpy import tools
 import brainpy.losses as losses
 import brainpy.math as bm
-from brainpy import tools, optim
-from brainpy._src.dynsys import DynamicalSystem
+from brainpy import optim
 from brainpy._src.context import share
-from brainpy._src.math.object_transform.base import BrainPyObject
+from brainpy._src.dynsys import DynamicalSystem
 from brainpy._src.running import constants as c
-from brainpy.check import serialize_kwargs
 from brainpy.errors import UnsupportedError, NoLongerSupportError
 from brainpy.types import ArrayType, Output
 from ._utils import msg
 from .base import DSTrainer
 
 __all__ = [
   'BPTT',
@@ -79,16 +77,15 @@
       # -------------
       # API deprecated
       seed: int = None,  # deprecated
       shuffle_data: bool = None,  # deprecated
 
       **kwargs,
   ):
-    super(BPTrainer, self).__init__(target=target,
-                                    **kwargs)
+    super().__init__(target=target, **kwargs)
 
     if shuffle_data is not None:
       raise NoLongerSupportError(
         f'''
         "shuffle_data" is no longer supported. '
         To be general, users should shuffle their data by themself.
         
@@ -133,16 +130,17 @@
     # loss data
     self._report_train_metrics = dict()
     self._report_test_metrics = dict()
     self._detailed_train_metrics = dict()
     self._detailed_test_metrics = dict()
 
     # functions
-    self._f_loss_compiled = dict()
-    self._f_grad_compiled = dict()
+    self._jit_step_func_grad = bm.jit(self._step_func_grad, static_argnums=(0,))
+    self._jit_step_func_loss = bm.jit(self._step_func_loss, static_argnums=(0,))
+    self._jit_step_func_fit = bm.jit(self._step_func_fit, static_argnums=(0,))
 
   def __repr__(self):
     name = self.__class__.__name__
     prefix = ' ' * len(name)
     return (f'{name}(target={self.target}, \n\t'
             f'{prefix}jit={self.jit}, \n\t'
             f'{prefix}loss={self._loss_func}, \n\t'
@@ -226,14 +224,19 @@
       .. versionadded:: 2.3.1
     batch_size: int
 
       .. deprecated:: 2.2.4.1
          Please set batch size in your dataset.
 
     """
+    if shared_args is None:
+      shared_args = dict()
+    shared_args['fit'] = shared_args.get('fit', True)
+    shared_args = tools.DotDict(shared_args)
+
     if batch_size is not None:
       raise NoLongerSupportError('Please set batch size in your data. '
                                  'Specifically, make an iterable dataset '
                                  'which return a batch of (X, Y) data.')
     if isinstance(train_data, (tuple, list)):
       if len(train_data) == 2:
         raise UnsupportedError(msg)
@@ -242,15 +245,15 @@
       assert callable(fun_after_report), ('\n'
                                           'Unknown "fun_after_report", '
                                           'it should be a callable function receiving '
                                           'three arguments: idx, metrics, phase')
 
     if shared_args is None:
       shared_args = dict()
-    shared_args['fit'] = shared_args.get('fit', False)
+    shared_args['fit'] = shared_args.get('fit', True)
 
     true_progress_bar = self.progress_bar
     self.progress_bar = False
 
     # training the model
     detailed_train_metric = dict()
     report_train_metric = dict()
@@ -273,15 +276,15 @@
       for x, y in _training_data:
         # reset state
         if reset_state:
           self.target.reset_state(self._get_input_batch_size(x))
           self.reset_state()
 
         # training
-        res = self._get_f_train(shared_args)(x, y)
+        res = self.f_train(shared_args, x, y)
 
         # loss
         fit_epoch_metric['loss'].append(res[0])
         if self.loss_has_aux:
           if not isinstance(res[1], dict):
             raise TypeError(f'Auxiliary data in loss function should be a dict. But we got {type(res)}')
           for k, v in res[1].items():
@@ -351,15 +354,15 @@
         for x, y in _testing_data:
           # reset state
           if reset_state:
             self.target.reset_state(self._get_input_batch_size(x))
             self.reset_state()
 
           # testing
-          res = self._get_f_loss(shared_args)(x, y)
+          res = self.f_loss(shared_args, x, y)
 
           # loss
           if self.loss_has_aux:
             test_epoch_metric['loss'].append(res[0])
             if not isinstance(res[1], dict):
               raise TypeError(f'Auxiliary data in loss function should be a dict. But we got {type(res)}')
             for k, v in res[1].items():
@@ -422,69 +425,40 @@
     # finally
     self._report_train_metrics = {k: np.asarray(v) for k, v in report_train_metric.items()}
     self._detailed_train_metrics = {k: np.asarray(v) for k, v in detailed_train_metric.items()}
     self._report_test_metrics = {k: np.asarray(v) for k, v in report_test_metric.items()}
     self._detailed_test_metrics = {k: np.asarray(v) for k, v in detailed_test_metric.items()}
     self.progress_bar = true_progress_bar
 
-  def _get_f_loss(self, shared_args=None, jit=True) -> Callable:
-    """Get loss function."""
-    if shared_args is None:
-      shared_args = dict()
-    shared_args2 = {k: v for k, v in shared_args.items()}
-    shared_args2['_local_jit_'] = jit
-    shared_args_str = serialize_kwargs(shared_args2)
-    if shared_args_str not in self._f_loss_compiled:
-      self._f_loss_compiled[shared_args_str] = partial(self._step_func_loss, shared_args)
-      if self.jit[c.LOSS_PHASE] and jit:
-        self._f_loss_compiled[shared_args_str] = bm.jit(self._f_loss_compiled[shared_args_str])
-    return self._f_loss_compiled[shared_args_str]
-
-  def _get_f_grad(self, shared_args=None) -> Callable:
-    """Get gradient function."""
-    shared_args_str = serialize_kwargs(shared_args)
-    if shared_args_str not in self._f_grad_compiled:
-      _f_loss_internal = self._get_f_loss(shared_args, jit=False)
-      dyn_vars = self.target.vars()
-      dyn_vars.update(self._dyn_vars)
-      tran_vars = dyn_vars.subset(bm.TrainVar).unique()
-      grad_f = bm.grad(_f_loss_internal,
-                       grad_vars=tran_vars,
-                       return_value=True,
-                       has_aux=self.loss_has_aux)
-      self._f_grad_compiled[shared_args_str] = grad_f
-    return self._f_grad_compiled[shared_args_str]
-
-  def _get_f_train(self, shared_args=None) -> Callable:
-    """Get training function."""
-    if shared_args is None: shared_args = dict()
-    if not isinstance(shared_args, dict):
-      raise ValueError(f'Only supports dict for "shared_args". '
-                       f'But got {type(shared_args)}: {shared_args}')
-
-    shared_args_str = serialize_kwargs(shared_args)
-    if shared_args_str not in self._f_fit_compiled:
-      self._f_fit_compiled[shared_args_str] = partial(self._step_func_fit, shared_args)
-      if self.jit[c.FIT_PHASE]:
-        dyn_vars = self.target.vars()
-        dyn_vars.update(self.optimizer.vars())
-        if isinstance(self._loss_func, BrainPyObject):
-          dyn_vars.update(self._loss_func)
-        dyn_vars.update(self._dyn_vars)
-        dyn_vars.update(self.vars(level=0))
-        dyn_vars = dyn_vars.unique()
-        self._f_fit_compiled[shared_args_str] = bm.jit(self._f_fit_compiled[shared_args_str])
-    return self._f_fit_compiled[shared_args_str]
+  def _step_func_grad(self, shared_args, inputs, targets):
+    tran_vars = self.target.train_vars().unique()
+    grad_f = bm.grad(self._step_func_loss,
+                     grad_vars=tran_vars,
+                     return_value=True,
+                     has_aux=self.loss_has_aux)
+    return grad_f(shared_args, inputs, targets)
 
   def _step_func_loss(self, shared_args, inputs, targets):
     raise NotImplementedError
 
+  @property
+  def f_loss(self):
+    return self._jit_step_func_loss if self.jit[c.LOSS_PHASE] else self._step_func_loss
+
   def _step_func_fit(self, shared_args, inputs, targets):
     raise NotImplementedError
 
+  @property
+  def f_train(self):
+    return self._jit_step_func_fit if self.jit[c.FIT_PHASE] else self._step_func_fit
+
+  @property
+  def f_grad(self):
+    return self._jit_step_func_grad if self.jit[c.FIT_PHASE] else self._step_func_grad
+
 
 class BPTT(BPTrainer):
   """The trainer implementing the back-propagation through time (BPTT)
   algorithm for training dyamical systems.
 
   For more parameters, users should refer to :py:class:`~.DSRunner`.
 
@@ -524,83 +498,76 @@
   data_first_axis: str
     To indicate whether the first axis is the batch size (``data_first_axis='B'``) or the
     time length (``data_first_axis='T'``).
   """
 
   def _step_func_loss(self, shared_args, inputs, targets):
     num_step = self._get_input_time_step(xs=inputs)
-    indices = jnp.arange(num_step, dtype=bm.int_)
-    times = indices * self.dt + self.t0
-    indices = indices + self.i0
+    indices = np.arange(self.i0, self.i0 + num_step, dtype=np.int_)
     if isinstance(self.target.mode, bm.BatchingMode) and self.data_first_axis == 'B':
       inputs = tree_map(lambda x: bm.moveaxis(x, 0, 1), inputs, is_leaf=lambda x: isinstance(x, bm.Array))
-    inputs = (times, indices, inputs)
-    outs, mons = self._predict(xs=inputs, shared_args=shared_args)
+    if not isinstance(inputs, (tuple, list)):
+      inputs = (inputs,)
+    outs, mons = self._predict(indices, *inputs, shared_args=shared_args)
     predicts = (outs, mons) if len(mons) > 0 else outs
     return self._loss_func(predicts, targets)
 
   def _step_func_fit(self, shared_args, inputs, targets):
-    res = self._get_f_grad(shared_args)(inputs, targets)
+    res = self.f_grad(shared_args, inputs, targets)
     self.optimizer.update(res[0])
     return res[1:]
 
 
 class BPFF(BPTrainer):
   """
   The trainer implementing back propagation algorithm
   for feedforward neural networks.
 
   For more parameters, users should refer to :py:class:`~.DSRunner`.
 
   """
 
   def _step_func_loss(self, shared_args, inputs, targets):
-    outputs, mon = self._get_f_predict(shared_args)(inputs)
+    if not isinstance(inputs, (tuple, list)):
+      inputs = (inputs,)
+    outputs, mon = self._step_func_predict(*inputs, shared_args=shared_args)
     outs = (outputs, mon) if len(mon) > 0 else outputs
     loss = self._loss_func(outs, targets)
     return loss
 
   def _step_func_fit(self, shared_args, inputs, targets):
-    res = self._get_f_grad(shared_args)(inputs, targets)
+    res = self.f_grad(shared_args, inputs, targets)
     self.optimizer.update(res[0])
     return res[1:]
 
-  def _step_func_predict(self, shared, x=None):
-    assert self.data_first_axis == 'B', f'There is no time dimension when using the trainer {self.__class__.__name__}.'
-    for k, v in shared.items():
-      share.save(k, v)
+  def _step_func_predict(self, *x, shared_args=None):
+    assert self.data_first_axis == 'B', (f'There is no time dimension when '
+                                         f'using the trainer {self.__class__.__name__}.')
+    if shared_args is not None:
+      assert isinstance(shared_args, dict)
+      share.save(**shared_args)
+    share.save(dt=self.dt)
 
     # input step
     self.target.clear_input()
-    self._step_func_input(shared)
+    self._step_func_input()
 
     # dynamics update step
-    args = () if x is None else (x, )
-    out = self.target(*args)
+    out = self.target(*x)
 
     # monitor step
-    mon = self._step_func_monitor(shared)
-    share.clear_shargs()
+    mon = self._step_func_monitor()
+    # share.clear_shargs()
     return out, mon
 
-  def _get_f_predict(self, shared_args: Dict = None, jit: bool = True):
-    if shared_args is None:
-      shared_args = tools.DotDict()
-    if not isinstance(shared_args, dict):
-      raise ValueError(f'"shared_args" must be a dict, but got {type(shared_args)}')
-
-    shared_args2 = {k: v for k, v in shared_args.items()}
-    shared_args2['_local_jit_'] = jit
-    shared_args_str = serialize_kwargs(shared_args)
-    if shared_args_str not in self._f_predict_compiled:
-
-      self._f_predict_compiled[shared_args_str] = partial(self._step_func_predict, shared_args)
-      if self.jit[c.PREDICT_PHASE] and jit:
-        self._f_predict_compiled[shared_args_str] = bm.jit(self._f_predict_compiled[shared_args_str])
-    return self._f_predict_compiled[shared_args_str]
+  def _fun_predict(self, *inputs, shared_args=None):
+    if self.jit['predict']:
+      return self._jit_step_func_predict(*inputs, shared_args=shared_args)
+    else:
+      return self._step_func_predict(*inputs, shared_args=shared_args)
 
   def predict(
       self,
       inputs: Union[ArrayType, Sequence[ArrayType], Dict[str, ArrayType]],
       reset_state: bool = True,
       shared_args: Dict = None,
       eval_time: bool = False
@@ -624,30 +591,32 @@
       Evaluate the time used for running.
 
     Returns
     -------
     output: ArrayType, dict
       The model output.
     """
-    if shared_args is None: shared_args = dict()
+    if shared_args is None:
+      shared_args = dict()
     shared_args['fit'] = shared_args.get('fit', False)
+    shared_args = tools.DotDict(shared_args)
 
     # reset the model states
     if reset_state:
       self.target.reset_state(self._get_input_batch_size(xs=inputs))
       self.reset_state()
     # init monitor
-    for key in self.mon.var_names:
+    for key in self._monitors.keys():
       self.mon[key] = []  # reshape the monitor items
     # prediction
+    if not isinstance(inputs, (tuple, list)):
+      inputs = (inputs,)
     if eval_time: t0 = time.time()
-    outs, hists = self._predict(xs=inputs, shared_args=shared_args)
+    outs, hists = self._fun_predict(*inputs, shared_args=shared_args)
     if eval_time: t1 = time.time()
     # post-running for monitors
     for key in hists.keys():
       self.mon[key] = bm.asarray(hists[key])
     if self.numpy_mon_after_run:
       for key in hists.keys():
         self.mon[key] = np.asarray(self.mon[key])
     return (t1 - t0, outs) if eval_time else outs
-
-
```

## brainpy/_src/train/base.py

```diff
@@ -36,15 +36,15 @@
   '''All children nodes in this training target.'''
 
   def __init__(
       self,
       target: DynamicalSystem,
       **kwargs
   ):
-    super(DSTrainer, self).__init__(target=target, **kwargs)
+    super().__init__(target=target, **kwargs)
 
     if not isinstance(self.target.mode, bm.BatchingMode):
       raise NoLongerSupportError(f'''
       From version 2.3.1, DSTrainer must receive a DynamicalSystem instance with 
       the computing mode of {bm.batching_mode} or {bm.training_mode}. 
       
       See https://github.com/brainpy/BrainPy/releases/tag/V2.3.1
@@ -55,46 +55,42 @@
     if isinstance(self._origin_jit, bool):
       self.jit[c.PREDICT_PHASE] = self._origin_jit
       self.jit[c.FIT_PHASE] = self._origin_jit
     else:
       self.jit[c.PREDICT_PHASE] = self._origin_jit.get(c.PREDICT_PHASE, True)
       self.jit[c.FIT_PHASE] = self._origin_jit.get(c.FIT_PHASE, True)
 
-    # training function
-    self._f_fit_compiled = dict()
-
   def predict(
       self,
-      inputs: Union[ArrayType, Sequence[ArrayType], Dict[str, ArrayType]],
+      inputs: Any,
       reset_state: bool = False,
       shared_args: Optional[Dict] = None,
       eval_time: bool = False
   ) -> Output:
     """Prediction function.
 
     Parameters
     ----------
     inputs: ArrayType, sequence of ArrayType, dict of ArrayType
       The input values.
     reset_state: bool
       Reset the target state before running.
-    shared_args: dict
-      The shared arguments across nodes.
     eval_time: bool
       Whether we evaluate the running time or not?
+    shared_args: dict
+      The shared arguments across nodes.
 
     Returns
     -------
     output: ArrayType, sequence of ArrayType, dict of ArrayType
       The running output.
     """
     if shared_args is None:
       shared_args = dict()
     shared_args['fit'] = shared_args.get('fit', False)
-
     return super().predict(inputs=inputs,
                            reset_state=reset_state,
                            shared_args=shared_args,
                            eval_time=eval_time)
 
   def fit(
       self,
```

## brainpy/_src/train/offline.py

```diff
@@ -1,19 +1,21 @@
 # -*- coding: utf-8 -*-
 
-from typing import Dict, Sequence, Union, Callable
+from typing import Dict, Sequence, Union, Callable, Any
 
 import numpy as np
 import tqdm.auto
 from jax.experimental.host_callback import id_tap
 
 import brainpy.math as bm
+from brainpy import tools
+from brainpy._src.context import share
 from brainpy._src.dynsys import DynamicalSystem
+from brainpy._src.runners import _call_fun_with_share
 from brainpy.algorithms.offline import get, RidgeRegression, OfflineAlgorithm
-from brainpy.check import serialize_kwargs
 from brainpy.errors import NoImplementationError
 from brainpy.types import ArrayType, Output
 from ._utils import format_ys
 from .base import DSTrainer
 
 __all__ = [
   'OfflineTrainer',
@@ -52,15 +54,15 @@
       self,
       target: DynamicalSystem,
       fit_method: Union[OfflineAlgorithm, Callable, Dict, str] = None,
       **kwargs
   ):
     self._true_numpy_mon_after_run = kwargs.get('numpy_mon_after_run', True)
     kwargs['numpy_mon_after_run'] = False
-    super(OfflineTrainer, self).__init__(target=target, **kwargs)
+    super().__init__(target=target, **kwargs)
 
     # get all trainable nodes
     nodes = self.target.nodes(level=-1, include_self=True).subset(DynamicalSystem).unique()
     self.train_nodes = tuple([node for node in nodes.values() if isinstance(node.mode, bm.TrainingMode)])
     if len(self.train_nodes) == 0:
       raise ValueError('Found no trainable nodes.')
 
@@ -79,24 +81,26 @@
       raise ValueError(f'"train_method" must be an instance of callable function, '
                        f'but we got {type(fit_method)}.')
     self.fit_method = fit_method
 
     # set the training method
     for node in self.train_nodes:
       node.offline_fit_by = fit_method
+    # training function
+    self._jit_fun_train = bm.jit(self._fun_train, static_argnames=['shared_args'])
 
   def __repr__(self):
     name = self.__class__.__name__
     prefix = ' ' * len(name)
     return (f'{name}(target={self.target}, \n\t'
             f'{prefix}fit_method={self.fit_method})')
 
   def predict(
       self,
-      inputs: Union[ArrayType, Sequence[ArrayType], Dict[str, ArrayType]],
+      inputs: Any,
       reset_state: bool = False,
       shared_args: Dict = None,
       eval_time: bool = False
   ) -> Output:
     """Prediction function.
 
     What's different from `predict()` function in :py:class:`~.DynamicalSystem` is that
@@ -104,28 +108,26 @@
 
     Parameters
     ----------
     inputs: ArrayType
       The input values.
     reset_state: bool
       Reset the target state before running.
-    shared_args: dict
-      The shared arguments across nodes.
     eval_time: bool
       Whether we evaluate the running time or not?
+    shared_args: dict
+      The shared arguments across nodes.
 
     Returns
     -------
     output: ArrayType
       The running output.
     """
-    outs = super(OfflineTrainer, self).predict(inputs=inputs,
-                                               reset_state=reset_state,
-                                               shared_args=shared_args,
-                                               eval_time=eval_time)
+    outs = super().predict(inputs=inputs, reset_state=reset_state,
+                           eval_time=eval_time, shared_args=shared_args)
     for node in self.train_nodes:
       node.fit_record.clear()
     return outs
 
   def fit(
       self,
       train_data: Sequence,
@@ -148,29 +150,32 @@
         - If the shape of each tensor is `(num_sample, num_time, num_feature)`,
           then the fitting happens on the whole data series.
     reset_state: bool
       Whether reset the initial states of the target model.
     shared_args: dict
       The shared keyword arguments for the target models.
     """
-    if shared_args is None: shared_args = dict()
+    if shared_args is None:
+      shared_args = dict()
     shared_args['fit'] = shared_args.get('fit', True)
+    shared_args = tools.DotDict(shared_args)
 
     # checking training and testing data
     if not isinstance(train_data, (list, tuple)):
       raise ValueError(f"{self.__class__.__name__} only support "
                        f"training data with the format of (X, Y) pair, "
                        f"but we got a {type(train_data)}.")
     if len(train_data) != 2:
       raise ValueError(f"{self.__class__.__name__} only support "
                        f"training data with the format of (X, Y) pair, "
                        f"but we got a sequence with length {len(train_data)}")
     xs, ys = train_data
 
     # prediction, get all needed data
+    shared_args['fit'] = shared_args.get('fit', False)
     outs = self.predict(inputs=xs, reset_state=reset_state, shared_args=shared_args)
 
     # check target data
     ys = format_ys(self, ys)
 
     # init progress bar
     if self.progress_bar:
@@ -178,15 +183,17 @@
       self._pbar.set_description(f"Train {len(self.train_nodes)} nodes: ", refresh=True)
 
     # training
     monitor_data = dict()
     for node in self.train_nodes:
       key = f'{node.name}-fit_record'
       monitor_data[key] = self.mon.get(key)
-    self._get_f_train(shared_args)(monitor_data, ys)
+    run_fun = self._jit_fun_train if self.jit['fit'] else self._fun_train
+    shared_args['fit'] = True
+    run_fun(monitor_data, ys, shared_args=shared_args)
     del monitor_data
 
     # close the progress bar
     if self.progress_bar:
       self._pbar.close()
 
     # final things
@@ -195,77 +202,59 @@
       node.fit_record.clear()  # clear fit records
     if self._true_numpy_mon_after_run:
       for key in self.mon.keys():
         self.mon[key] = np.asarray(self.mon[key])
 
     return outs
 
-  def _get_f_train(self, shared_args: Dict = None) -> Callable:
-    """Get training function."""
-    shared_args = dict() if shared_args is None else shared_args
-    shared_kwargs_str = serialize_kwargs(shared_args)
-    if shared_kwargs_str not in self._f_fit_compiled:
-      self._f_fit_compiled[shared_kwargs_str] = (
-        self._fun_train
-        if self.jit['fit'] else
-        bm.jit(self._fun_train)
-      )
-    return self._f_fit_compiled[shared_kwargs_str]
+  def _fun_train(self,
+                 monitor_data: Dict[str, ArrayType],
+                 target_data: Dict[str, ArrayType],
+                 shared_args: Dict = None):
+    if shared_args is None:
+      shared_args = dict()
+    share.save(**shared_args)
 
-  def _fun_train(self, monitor_data: Dict[str, ArrayType], target_data: Dict[str, ArrayType]):
     for node in self.train_nodes:
       fit_record = monitor_data[f'{node.name}-fit_record']
       targets = target_data[node.name]
       node.offline_fit(targets, fit_record)
       if self.progress_bar:
         id_tap(lambda *args: self._pbar.update(), ())
 
-  def _step_func_monitor(self, shared):
+  def _step_func_monitor(self):
     res = dict()
     for key, val in self._monitors.items():
       if callable(val):
-        res[key] = val(shared)
+        res[key] = _call_fun_with_share(val)
       else:
         (variable, idx) = val
         if idx is None:
           res[key] = variable.value
         else:
           res[key] = variable[bm.asarray(idx)]
-    if shared.get('fit', False):
+    if share.load('fit'):
       for node in self.train_nodes:
         res[f'{node.name}-fit_record'] = node.fit_record
     return res
 
   def _check_interface(self):
     for node in self.train_nodes:
       if not hasattr(node, 'offline_fit'):
-          raise NoImplementationError(
-            f'''
+        raise NoImplementationError(
+          f'''
             The node
             
             {node}
             
             is set to be computing mode of {bm.training_mode} with {self.__class__.__name__}. 
             However, it does not implement the required training 
             interface "offline_fit()" function. 
             '''
-          )
-      # if hasattr(node.offline_init, 'not_customized'):
-      #   if node.offline_init.not_customized:
-      #     raise NoImplementationError(
-      #       f'''
-      #       The node
-      #
-      #       {node}
-      #
-      #       is set to be computing mode of {bm.training_mode} with {self.__class__.__name__}.
-      #       However, it does not implement the required training
-      #       interface "offline_init()" function.
-      #       '''
-      #     )
+        )
 
 
 class RidgeTrainer(OfflineTrainer):
   """Trainer of ridge regression, also known as regression with Tikhonov regularization.
 
   For more parameters, users should refer to :py:class:`~.DSRunner`.
 
@@ -274,10 +263,8 @@
   target: TrainingSystem, DynamicalSystem
     The target model.
   alpha: float
     The regularization coefficient.
   """
 
   def __init__(self, target, alpha=1e-7, **kwargs):
-    super(RidgeTrainer, self).__init__(target=target,
-                                       fit_method=dict(name='ridge', alpha=alpha),
-                                       **kwargs)
+    super().__init__(target=target, fit_method=dict(name='ridge', alpha=alpha), **kwargs)
```

## brainpy/_src/train/online.py

```diff
@@ -1,22 +1,21 @@
 # -*- coding: utf-8 -*-
-
-from functools import partial
-from typing import Dict, Sequence, Union, Callable, Tuple
+import functools
+from typing import Dict, Sequence, Union, Callable
 
 import numpy as np
 import tqdm.auto
 from jax.experimental.host_callback import id_tap
 from jax.tree_util import tree_map
 
 from brainpy import math as bm, tools
-from brainpy._src.dynsys import DynamicalSystem
 from brainpy._src.context import share
+from brainpy._src.dynsys import DynamicalSystem
+from brainpy._src.runners import _call_fun_with_share
 from brainpy.algorithms.online import get, OnlineAlgorithm, RLS
-from brainpy.check import serialize_kwargs
 from brainpy.errors import NoImplementationError
 from brainpy.types import ArrayType, Output
 from ._utils import format_ys
 from .base import DSTrainer
 
 __all__ = [
   'OnlineTrainer',
@@ -54,15 +53,15 @@
 
   def __init__(
       self,
       target: DynamicalSystem,
       fit_method: Union[OnlineAlgorithm, Callable, Dict, str] = None,
       **kwargs
   ):
-    super(OnlineTrainer, self).__init__(target=target, **kwargs)
+    super().__init__(target=target, **kwargs)
 
     # get all trainable nodes
     nodes = self.target.nodes(level=-1, include_self=True).subset(DynamicalSystem).unique()
     self.train_nodes = tuple([node for node in nodes.values() if isinstance(node.mode, bm.TrainingMode)])
     if len(self.train_nodes) == 0:
       raise ValueError('Found no trainable nodes.')
 
@@ -141,14 +140,15 @@
       self,
       train_data: Sequence,
       reset_state: bool = False,
       shared_args: Dict = None,
   ) -> Output:
     if shared_args is None: shared_args = dict()
     shared_args['fit'] = shared_args.get('fit', True)
+    shared_args = tools.DotDict(shared_args)
 
     # checking training and testing data
     if not isinstance(train_data, (list, tuple)):
       raise ValueError(f"{self.__class__.__name__} only support "
                        f"training data with the format of (X, Y) pair, "
                        f"but we got a {type(train_data)}.")
     if len(train_data) != 2:
@@ -162,149 +162,131 @@
       num_batch = self._get_input_batch_size(xs)
       self.target.reset_state(num_batch)
       self.reset_state()
 
     # format input/target data
     ys = format_ys(self, ys)
     num_step = self._get_input_time_step(xs=xs)
-    shared = tools.DotDict(i=bm.arange(num_step, dtype=bm.int_).value)
-    shared['t'] = shared['i'] * self.dt
-    shared['t'] += self.t0
-    shared['i'] += self.i0
 
+    indices = np.arange(self.i0, num_step + self.i0, dtype=np.int_)
     if self.data_first_axis == 'B':
       xs = tree_map(lambda x: bm.moveaxis(x, 0, 1),
                     xs,
                     is_leaf=lambda x: isinstance(x, bm.Array))
       ys = tree_map(lambda y: bm.moveaxis(y, 0, 1),
                     ys,
                     is_leaf=lambda y: isinstance(y, bm.Array))
 
     # init monitor
-    for key in self.mon.var_names:
+    for key in self._monitors.keys():
       self.mon[key] = []  # reshape the monitor items
 
     # init progress bar
     if self.progress_bar:
       self._pbar = tqdm.auto.tqdm(total=num_step)
       self._pbar.set_description(f"Train {num_step} steps: ", refresh=True)
 
     # prediction
-    outs, hists = self._fit(tix=(shared['t'], shared['i'], xs), ys=ys, shared_args=shared_args)
+    xs = (xs, ) if not isinstance(xs, (tuple, list)) else xs
+    outs, hists = self._fit(indices, xs=xs, ys=ys, shared_args=shared_args)
 
     # close the progress bar
     if self.progress_bar:
       self._pbar.close()
 
     # post-running for monitors
-    hists['ts'] = shared['t'] + self.dt
     if self.numpy_mon_after_run:
       hists = tree_map(lambda a: np.asarray(a), hists, is_leaf=lambda a: isinstance(a, bm.Array))
     for key in hists.keys():
       self.mon[key] = hists[key]
-    self.i0 += shared['t'].shape[0]
-    self.t0 += num_step * self.dt
+    self.i0 += num_step
     return outs
 
-  def _fit(
-      self,
-      tix: Tuple,
-      ys: Union[ArrayType, Sequence[ArrayType], Dict[str, ArrayType]],
-      shared_args: Dict = None,
-  ):
+  def _fit(self,
+           indices: ArrayType,
+           xs: Sequence,
+           ys: Dict[str, ArrayType],
+           shared_args: Dict = None):
     """Predict the output according to the inputs.
 
     Parameters
     ----------
-    tix: tuple
-      Each tensor should have the shape of `(num_time, num_batch, num_feature)`.
-    ys: ArrayType
+    indices: ArrayType
+      The running indices.
+    ys: dict
       Each tensor should have the shape of `(num_time, num_batch, num_feature)`.
     shared_args: optional, dict
       The shared keyword arguments.
 
     Returns
     -------
     outputs, hists
       A tuple of pair of (outputs, hists).
     """
-    _fit_func = self._get_fit_func(shared_args)
-    hists = _fit_func(tix + (ys,))
+    hists = bm.for_loop(functools.partial(self._step_func_fit, shared_args=shared_args),
+                        (indices, xs, ys),
+                        jit=self.jit['fit'])
     hists = tree_map(lambda x: bm.moveaxis(x, 0, 1),
                      hists,
                      is_leaf=lambda x: isinstance(x, bm.Array))
     return hists
 
-  def _get_fit_func(self, shared_args: Dict = None):
-    if shared_args is None: shared_args = dict()
-    shared_kwargs_str = serialize_kwargs(shared_args)
-    if shared_kwargs_str not in self._f_fit_compiled:
-      @bm.jit
-      def run_func(all_inputs):
-        return bm.for_loop(partial(self._step_func_fit, shared_args),
-                           all_inputs,
-                           jit=self.jit['fit'])
-
-      self._f_fit_compiled[shared_kwargs_str] = run_func
-    return self._f_fit_compiled[shared_kwargs_str]
-
-  def _step_func_fit(self, shared_args, t, i, x, ys):
-    shared = tools.DotDict(t=t, dt=self.dt, i=i)
-    shared.update(shared_args)
-    share.save(**shared)
+  def _step_func_fit(self, i, xs: Sequence, ys: Dict, shared_args=None):
+    if shared_args is None:
+      shared_args = dict()
+    share.save(t=i * self.dt, dt=self.dt, i=i, **shared_args)
 
     # input step
     self.target.clear_input()
-    self._step_func_input(shared)
+    self._step_func_input()
 
     # update step
-    args = () if x is None else (x, )
-    out = self.target(*args)
+    out = self.target(*xs)
 
     # monitor step
-    monitors = self._step_func_monitor(shared)
+    monitors = self._step_func_monitor()
     for node in self.train_nodes:
       fit_record = monitors.pop(f'{node.name}-fit_record')
       target = ys[node.name]
       node.online_fit(target, fit_record)
 
     # finally
     if self.progress_bar:
       id_tap(lambda *arg: self._pbar.update(), ())
     return out, monitors
 
   def _check_interface(self):
     for node in self.train_nodes:
       if not hasattr(node, 'online_fit'):
-          raise NoImplementationError(
-            f'The node \n\n{node}\n\n'
-            f'is set to be trainable with {self.__class__.__name__} method. '
-            f'However, it does not implement the required training '
-            f'interface "online_fit()" function. '
-          )
+        raise NoImplementationError(
+          f'The node \n\n{node}\n\n'
+          f'is set to be trainable with {self.__class__.__name__} method. '
+          f'However, it does not implement the required training '
+          f'interface "online_fit()" function. '
+        )
       if not hasattr(node, 'online_init'):
-          raise NoImplementationError(
-            f'The node \n\n{node}\n\n'
-            f'is set to be trainable with {self.__class__.__name__} method. '
-            f'However, it does not implement the required training '
-            f'interface "online_init()" function. '
-          )
+        raise NoImplementationError(
+          f'The node \n\n{node}\n\n'
+          f'is set to be trainable with {self.__class__.__name__} method. '
+          f'However, it does not implement the required training '
+          f'interface "online_init()" function. '
+        )
 
-  def _step_func_monitor(self, shared):
+  def _step_func_monitor(self):
     res = dict()
     for key, val in self._monitors.items():
       if callable(val):
-        res[key] = val(shared)
+        res[key] = _call_fun_with_share(val)
       else:
         (variable, idx) = val
         if idx is None:
           res[key] = variable.value
         else:
           res[key] = variable[bm.asarray(idx)]
-    if shared.get('fit', False):
+    if share.load('fit'):
       for node in self.train_nodes:
         res[f'{node.name}-fit_record'] = node.fit_record
     return res
 
 
 class ForceTrainer(OnlineTrainer):
   """FORCE learning."""
```

## brainpy/dyn/__init__.py

```diff
@@ -1,6 +1,11 @@
 
+from .base import *
+from .ions import *
 from .channels import *
 from .neurons import *
 from .synapses import *
 from .projections import *
 from .others import *
+from .outs import *
+from .rates import *
+from .compat import NeuGroup
```

## brainpy/dyn/channels.py

```diff
@@ -1,54 +1,74 @@
 from brainpy._src.dyn.channels.base import (
-  Ion,
-  IonChannel,
-  Calcium,
-  IhChannel,
-  CalciumChannel,
-  SodiumChannel,
-  PotassiumChannel,
-  LeakyChannel,
+  IonChannel as IonChannel,
 )
 
-from brainpy._src.dyn.channels.Ca import (
-  CalciumFixed,
-  CalciumChannel,
-  CalciumDetailed,
-  CalciumFirstOrder,
-  CalciumDyna,
-  ICaN_IS2008,
-  ICaT_HM1992,
-  ICaT_HP1992,
-  ICaHT_HM1992,
-  ICaL_IS2008,
+from brainpy._src.dyn.channels.calcium import (
+  CalciumChannel as CalciumChannel,
+  ICaN_IS2008 as ICaN_IS2008,
+  ICaT_HM1992 as ICaT_HM1992,
+  ICaT_HP1992 as ICaT_HP1992,
+  ICaHT_HM1992 as ICaHT_HM1992,
+  ICaHT_Re1993 as ICaHT_Re1993,
+  ICaL_IS2008 as ICaL_IS2008,
 )
 
-from brainpy._src.dyn.channels.K import (
+
+from brainpy._src.dyn.channels.potassium import (
+  PotassiumChannel as PotassiumChannel,
+  IKDR_Ba2002v2 as IKDR_Ba2002v2,
+  IK_TM1991v2 as IK_TM1991v2,
+  IK_HH1952v2 as IK_HH1952v2,
+  IKA1_HM1992v2 as IKA1_HM1992v2,
+  IKA2_HM1992v2 as IKA2_HM1992v2,
+  IKK2A_HM1992v2 as IKK2A_HM1992v2,
+  IKK2B_HM1992v2 as IKK2B_HM1992v2,
+  IKNI_Ya1989v2 as IKNI_Ya1989v2,
+  IK_Leak as IK_Leak,
+)
+from brainpy._src.dyn.channels.potassium_compatible import (
   IKDR_Ba2002,
   IK_TM1991,
   IK_HH1952,
   IKA1_HM1992,
   IKA2_HM1992,
   IKK2A_HM1992,
   IKK2B_HM1992,
   IKNI_Ya1989,
+  IKL,
 )
 
-from brainpy._src.dyn.channels.IH import (
+
+from brainpy._src.dyn.channels.hyperpolarization_activated import (
   Ih_HM1992,
   Ih_De1996,
 )
 
-from brainpy._src.dyn.channels.KCa import (
+
+from brainpy._src.dyn.channels.potassium_calcium import (
+  IAHP_De1994v2
+)
+from brainpy._src.dyn.channels.potassium_calcium_compatible import (
   IAHP_De1994
 )
 
-from brainpy._src.dyn.channels.Na import (
+
+from brainpy._src.dyn.channels.sodium import (
+  SodiumChannel,
+)
+from brainpy._src.dyn.channels.sodium_compatible import (
   INa_Ba2002,
   INa_TM1991,
   INa_HH1952,
 )
+from brainpy._src.dyn.channels.sodium import (
+  INa_Ba2002v2,
+  INa_TM1991v2,
+  INa_HH1952v2,
+)
+
 
 from brainpy._src.dyn.channels.leaky import (
+  LeakyChannel,
   IL,
-  IKL,
 )
+
```

## brainpy/dyn/neurons.py

```diff
@@ -1,14 +1,8 @@
 
-from brainpy._src.dyn.base import (
-  NeuDyn,
-  GradNeuDyn,
-  HHTypeNeu,
-  HHTypeNeuLTC
-)
 
 from brainpy._src.dyn.neurons.lif import (
   Lif,
   LifLTC,
   LifRefLTC,
   LifRef,
   ExpIF,
@@ -34,23 +28,21 @@
   Izhikevich,
   IzhikevichLTC,
   IzhikevichRefLTC,
   IzhikevichRef
 )
 
 from brainpy._src.dyn.neurons.hh import (
+  HHTypedNeuron,
+  CondNeuGroupLTC,
+  CondNeuGroup,
   HH,
   HHLTC,
   MorrisLecar,
   MorrisLecarLTC,
-  WangBuzsakiModel,
-  WangBuzsakiModelLTC,
+  WangBuzsakiHH,
+  WangBuzsakiHHLTC,
 )
 
-from brainpy._src.dyn.neurons.input import (
-  InputGroup,
-  OutputGroup,
-  SpikeTimeGroup,
-  PoissonGroup,
-)
+
```

## brainpy/dyn/others.py

```diff
@@ -1,4 +1,17 @@
 from brainpy._src.dyn.others.common import (
-  Leaky,
-  Integrator,
-)
+  Leaky as Leaky,
+  Integrator as Integrator,
+)
+
+from brainpy._src.dyn.others.input import (
+  InputGroup as InputGroup,
+  OutputGroup as OutputGroup,
+  SpikeTimeGroup as SpikeTimeGroup,
+  PoissonGroup as PoissonGroup,
+)
+
+
+from brainpy._src.dyn.others.noise import (
+  OUProcess as OUProcess,
+)
+
```

## brainpy/dyn/projections.py

```diff
@@ -1,7 +1,20 @@
 
 
-from brainpy._src.dyn.projections import (
-  ProjAlignPost,
-  ProjAlignPre,
+from brainpy._src.dyn.projections.aligns import (
+  VanillaProj,
+  ProjAlignPostMg1,
+  ProjAlignPostMg2,
+  ProjAlignPost1,
+  ProjAlignPost2,
+  ProjAlignPreMg1,
+  ProjAlignPreMg2,
+)
+
+from brainpy._src.dyn.projections.conn import (
+  SynConn as SynConn,
+)
+
+from brainpy._src.dyn.projections.others import (
+  PoissonInput as PoissonInput,
 )
```

## brainpy/dyn/synapses.py

```diff
@@ -1,24 +1,22 @@
 
-from brainpy._src.dyn.base import (
-  SynDyn,
-  SynOut,
-)
-
-from brainpy._src.dyn.synapses.dynamics import (
+from brainpy._src.dyn.synapses.abstract_models import (
+  Delta,
   Expon,
   DualExpon,
-  Alpha,
+  DualExponV2,
   NMDA,
   STD,
   STP,
+)
+from brainpy._src.dyn.synapses.bio_models import (
   AMPA,
   GABAa,
   BioNMDA,
 )
-
-from brainpy._src.dyn.synapses.outputs import (
-  COBA,
-  CUBA,
-  MgBlock,
+from brainpy._src.dyn.synapses.delay_couplings import (
+  DiffusiveCoupling,
+  AdditiveCoupling,
 )
 
+
+
```

## Comparing `brainpy/_src/dyn/channels/Ca.py` & `brainpy/_src/dynold/neurons/biological_models.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,1092 +1,844 @@
 # -*- coding: utf-8 -*-
 
-"""
-This module implements voltage-dependent calcium channels.
-
-"""
-
 from typing import Union, Callable
 
 import brainpy.math as bm
-from brainpy._src.dynsys import Channel
-from brainpy._src.initialize import OneInit, Initializer, parameter, variable
+from brainpy import check
+from brainpy._src.context import share
+from brainpy._src.dyn.neurons import hh
+from brainpy._src.dyn.base import NeuDyn
+from brainpy._src.initialize import (OneInit,
+                                     Initializer,
+                                     parameter,
+                                     noise as init_noise,
+                                     variable_)
 from brainpy._src.integrators.joint_eq import JointEq
 from brainpy._src.integrators.ode.generic import odeint
+from brainpy._src.integrators.sde.generic import sdeint
 from brainpy.types import Shape, ArrayType
-from .base import Calcium, CalciumChannel
 
 __all__ = [
-  'CalciumFixed',
-  'CalciumDyna',
-  'CalciumDetailed',
-  'CalciumFirstOrder',
-
-  '_ICa_p2q_ss', '_ICa_p2q_markov',
-
-  'ICaN_IS2008',
-
-  'ICaT_HM1992',
-  'ICaT_HP1992',
-
-  'ICaHT_HM1992',
-
-  'ICaL_IS2008',
+  'HH',
+  'MorrisLecar',
+  'PinskyRinzelModel',
+  'WangBuzsakiModel',
 ]
 
 
-class CalciumFixed(Calcium):
-  """Fixed Calcium dynamics.
-
-  This calcium model has no dynamics. It holds fixed reversal
-  potential :math:`E` and concentration :math:`C`.
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      E: Union[float, ArrayType, Initializer, Callable] = 120.,
-      C: Union[float, ArrayType, Initializer, Callable] = 2.4e-4,
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-      **channels
-  ):
-    super(CalciumFixed, self).__init__(size,
-                                       keep_size=keep_size,
-                                       method=method,
-                                       name=name,
-                                       mode=mode,
-                                       **channels)
-    self.E = parameter(E, self.varshape, allow_none=False)
-    self.C = parameter(C, self.varshape, allow_none=False)
-
-  def update(self, tdi, V):
-    for node in self.implicit_nodes.values():
-      node.update(tdi, V, self.C, self.E)
-
-  def reset_state(self, V, C_Ca=None, E_Ca=None, batch_size=None):
-    C_Ca = self.C if C_Ca is None else C_Ca
-    E_Ca = self.E if E_Ca is None else E_Ca
-    for node in self.nodes(level=1, include_self=False).unique().subset(Channel).values():
-      node.reset_state(V, C_Ca, E_Ca, batch_size=batch_size)
+class HH(hh.HH):
+  r"""HodgkinHuxley neuron model.
 
+  **Model Descriptions**
 
-class CalciumDyna(Calcium):
-  """Calcium ion flow with dynamics.
+  The Hodgkin-Huxley (HH; Hodgkin & Huxley, 1952) model [1]_ for the generation of
+  the nerve action potential is one of the most successful mathematical models of
+  a complex biological process that has ever been formulated. The basic concepts
+  expressed in the model have proved a valid approach to the study of bio-electrical
+  activity from the most primitive single-celled organisms such as *Paramecium*,
+  right through to the neurons within our own brains.
+
+  Mathematically, the model is given by,
+
+  .. math::
+
+      C \frac {dV} {dt} = -(\bar{g}_{Na} m^3 h (V &-E_{Na})
+      + \bar{g}_K n^4 (V-E_K) + g_{leak} (V - E_{leak})) + I(t)
+
+      \frac {dx} {dt} &= \alpha_x (1-x)  - \beta_x, \quad x\in {\rm{\{m, h, n\}}}
+
+      &\alpha_m(V) = \frac {0.1(V+40)}{1-\exp(\frac{-(V + 40)} {10})}
+
+      &\beta_m(V) = 4.0 \exp(\frac{-(V + 65)} {18})
+
+      &\alpha_h(V) = 0.07 \exp(\frac{-(V+65)}{20})
+
+      &\beta_h(V) = \frac 1 {1 + \exp(\frac{-(V + 35)} {10})}
+
+      &\alpha_n(V) = \frac {0.01(V+55)}{1-\exp(-(V+55)/10)}
+
+      &\beta_n(V) = 0.125 \exp(\frac{-(V + 65)} {80})
+
+  The illustrated example of HH neuron model please see `this notebook <../neurons/HH_model.ipynb>`_.
+
+  The HodgkinHuxley model can be thought of as a differential equation system with
+  four state variables, :math:`V_{m}(t),n(t),m(t)`, and :math:`h(t)`, that change
+  with respect to time :math:`t`. The system is difficult to study because it is a
+  nonlinear system and cannot be solved analytically. However, there are many numeric
+  methods available to analyze the system. Certain properties and general behaviors,
+  such as limit cycles, can be proven to exist.
+
+  *1. Center manifold*
+
+  Because there are four state variables, visualizing the path in phase space can
+  be difficult. Usually two variables are chosen, voltage :math:`V_{m}(t)` and the
+  potassium gating variable :math:`n(t)`, allowing one to visualize the limit cycle.
+  However, one must be careful because this is an ad-hoc method of visualizing the
+  4-dimensional system. This does not prove the existence of the limit cycle.
+
+  .. image:: ../../../_static/Hodgkin_Huxley_Limit_Cycle.png
+      :align: center
+
+  A better projection can be constructed from a careful analysis of the Jacobian of
+  the system, evaluated at the equilibrium point. Specifically, the eigenvalues of
+  the Jacobian are indicative of the center manifold's existence. Likewise, the
+  eigenvectors of the Jacobian reveal the center manifold's orientation. The
+  HodgkinHuxley model has two negative eigenvalues and two complex eigenvalues
+  with slightly positive real parts. The eigenvectors associated with the two
+  negative eigenvalues will reduce to zero as time :math:`t` increases. The remaining
+  two complex eigenvectors define the center manifold. In other words, the
+  4-dimensional system collapses onto a 2-dimensional plane. Any solution
+  starting off the center manifold will decay towards the *center manifold*.
+  Furthermore, the limit cycle is contained on the center manifold.
+
+  *2. Bifurcations*
+
+  If the injected current :math:`I` were used as a bifurcation parameter, then the
+  HodgkinHuxley model undergoes a Hopf bifurcation. As with most neuronal models,
+  increasing the injected current will increase the firing rate of the neuron.
+  One consequence of the Hopf bifurcation is that there is a minimum firing rate.
+  This means that either the neuron is not firing at all (corresponding to zero
+  frequency), or firing at the minimum firing rate. Because of the all-or-none
+  principle, there is no smooth increase in action potential amplitude, but
+  rather there is a sudden "jump" in amplitude. The resulting transition is
+  known as a `canard <http://www.scholarpedia.org/article/Canards>`_.
+
+  .. image:: ../../../_static/Hodgkins_Huxley_bifurcation_by_I.gif
+     :align: center
+
+  The following image shows the bifurcation diagram of the HodgkinHuxley model
+  as a function of the external drive :math:`I` [3]_. The green lines show the amplitude
+  of a stable limit cycle and the blue lines indicate unstable limit-cycle behaviour,
+  both born from Hopf bifurcations. The solid red line shows the stable fixed point
+  and the black line shows the unstable fixed point.
+
+  .. image:: ../../../_static/Hodgkin_Huxley_bifurcation.png
+     :align: center
+
+  **Model Examples**
+
+  .. plot::
+    :include-source: True
+
+    >>> import brainpy as bp
+    >>> group = bp.neurons.HH(2)
+    >>> runner = bp.DSRunner(group, monitors=['V'], inputs=('input', 10.))
+    >>> runner.run(200.)
+    >>> bp.visualize.line_plot(runner.mon.ts, runner.mon.V, show=True)
+
+  .. plot::
+    :include-source: True
+
+    >>> import brainpy as bp
+    >>> import brainpy.math as bm
+    >>> import matplotlib.pyplot as plt
+    >>>
+    >>> group = bp.neurons.HH(2)
+    >>>
+    >>> I1 = bp.inputs.spike_input(sp_times=[500., 550., 1000, 1030, 1060, 1100, 1200], sp_lens=5, sp_sizes=5., duration=2000, )
+    >>> I2 = bp.inputs.spike_input(sp_times=[600., 900, 950, 1500], sp_lens=5, sp_sizes=5., duration=2000, )
+    >>> I1 += bp.math.random.normal(0, 3, size=I1.shape)
+    >>> I2 += bp.math.random.normal(0, 3, size=I2.shape)
+    >>> I = bm.stack((I1, I2), axis=-1)
+    >>>
+    >>> runner = bp.DSRunner(group, monitors=['V'], inputs=('input', I, 'iter'))
+    >>> runner.run(2000.)
+    >>>
+    >>> fig, gs = bp.visualize.get_figure(1, 1, 3, 8)
+    >>> fig.add_subplot(gs[0, 0])
+    >>> plt.plot(runner.mon.ts, runner.mon.V[:, 0])
+    >>> plt.plot(runner.mon.ts, runner.mon.V[:, 1] + 130)
+    >>> plt.xlim(10, 2000)
+    >>> plt.xticks([])
+    >>> plt.yticks([])
+    >>> plt.show()
 
   Parameters
   ----------
-  size: int, tuple of int
-    The ion size.
-  keep_size: bool
-    Keep the geometry size.
-  C0: float, ArrayType, Initializer, Callable
-    The Calcium concentration outside of membrane.
-  T: float, ArrayType, Initializer, Callable
-    The temperature.
-  C_initializer: Initializer, Callable, ArrayType
-    The initializer for Calcium concentration.
+  size: sequence of int, int
+    The size of the neuron group.
+  ENa: float, ArrayType, Initializer, callable
+    The reversal potential of sodium. Default is 50 mV.
+  gNa: float, ArrayType, Initializer, callable
+    The maximum conductance of sodium channel. Default is 120 msiemens.
+  EK: float, ArrayType, Initializer, callable
+    The reversal potential of potassium. Default is -77 mV.
+  gK: float, ArrayType, Initializer, callable
+    The maximum conductance of potassium channel. Default is 36 msiemens.
+  EL: float, ArrayType, Initializer, callable
+    The reversal potential of learky channel. Default is -54.387 mV.
+  gL: float, ArrayType, Initializer, callable
+    The conductance of learky channel. Default is 0.03 msiemens.
+  V_th: float, ArrayType, Initializer, callable
+    The threshold of the membrane spike. Default is 20 mV.
+  C: float, ArrayType, Initializer, callable
+    The membrane capacitance. Default is 1 ufarad.
+  V_initializer: ArrayType, Initializer, callable
+    The initializer of membrane potential.
+  m_initializer: ArrayType, Initializer, callable
+    The initializer of m channel.
+  h_initializer: ArrayType, Initializer, callable
+    The initializer of h channel.
+  n_initializer: ArrayType, Initializer, callable
+    The initializer of n channel.
   method: str
-    The numerical method.
+    The numerical integration method.
   name: str
-    The ion name.
-  """
-  R = 8.31441  # gas constant, J*mol-1*K-1
-  F = 96.489  # the Faraday constant
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      C0: Union[float, ArrayType, Initializer, Callable] = 2.,
-      T: Union[float, ArrayType, Initializer, Callable] = 36.,
-      C_initializer: Union[Initializer, Callable, ArrayType] = OneInit(2.4e-4),
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-      **channels
-  ):
-    super(CalciumDyna, self).__init__(size,
-                                      keep_size=keep_size,
-                                      method=method,
-                                      name=name,
-                                      mode=mode,
-                                      **channels)
-
-    # parameters
-    self.C0 = parameter(C0, self.varshape, allow_none=False)
-    self.T = parameter(T, self.varshape, allow_none=False)  # temperature
-    self._C_initializer = C_initializer
-    self._constant = self.R / (2 * self.F) * (273.15 + self.T)
-
-    # variables
-    self.C = variable(C_initializer, self.mode, self.varshape)  # Calcium concentration
-    self.E = bm.Variable(self._reversal_potential(self.C),
-                         batch_axis=0 if isinstance(self.mode, bm.BatchingMode) else None)  # Reversal potential
-
-    # function
-    self.integral = odeint(self.derivative, method=method)
-
-  def derivative(self, C, t, V):
-    raise NotImplementedError
-
-  def reset_state(self, V, C_Ca=None, E_Ca=None, batch_size=None):
-    self.C.value = variable(self._C_initializer, batch_size, self.varshape) if (C_Ca is None) else C_Ca
-    self.E.value = self._reversal_potential(self.C)
-    for node in self.nodes(level=1, include_self=False).unique().subset(Channel).values():
-      node.reset(V, self.C, self.E, batch_size=batch_size)
-
-  def update(self, tdi, V):
-    for node in self.nodes(level=1, include_self=False).unique().subset(Channel).values():
-      node.update(tdi, V, self.C.value, self.E.value)
-    self.C.value = self.integral(self.C.value, tdi['t'], V, tdi['dt'])
-    self.E.value = self._reversal_potential(self.C.value)
-
-  def _reversal_potential(self, C):
-    return self._constant * bm.log(self.C0 / C)
-
-
-class CalciumDetailed(CalciumDyna):
-  r"""Dynamical Calcium model proposed.
-
-  **1. The dynamics of intracellular** :math:`Ca^{2+}`
-
-  The dynamics of intracellular :math:`Ca^{2+}` were determined by two contributions [1]_ :
-
-  *(i) Influx of* :math:`Ca^{2+}` *due to Calcium currents*
-
-  :math:`Ca^{2+}` ions enter through :math:`Ca^{2+}` channels and diffuse into the
-  interior of the cell. Only the :math:`Ca^{2+}` concentration in a thin shell beneath
-  the membrane was modeled. The influx of :math:`Ca^{2+}` into such a thin shell followed:
-
-  .. math::
-
-      [Ca]_{i}=-\frac{k}{2 F d} I_{Ca}
-
-  where :math:`F=96489\, \mathrm{C\, mol^{-1}}` is the Faraday constant,
-  :math:`d=1\, \mathrm{\mu m}` is the depth of the shell beneath the membrane,
-  the unit conversion constant is :math:`k=0.1` for :math:`I_T` in
-  :math:`\mathrm{\mu A/cm^{2}}` and :math:`[Ca]_{i}` in millimolar,
-  and :math:`I_{Ca}` is the summation of all :math:`Ca^{2+}` currents.
-
-  *(ii) Efflux of* :math:`Ca^{2+}` *due to an active pump*
-
-  In a thin shell beneath the membrane, :math:`Ca^{2+}` retrieval usually consists of a
-  combination of several processes, such as binding to :math:`Ca^{2+}` buffers, calcium
-  efflux due to :math:`Ca^{2+}` ATPase pump activity and diffusion to neighboring shells.
-  Only the :math:`Ca^{2+}` pump was modeled here. We adopted the following kinetic scheme:
-
-  .. math::
-
-      Ca _{i}^{2+}+ P \overset{c_1}{\underset{c_2}{\rightleftharpoons}} CaP \xrightarrow{c_3} P+ Ca _{0}^{2+}
-
-  where P represents the :math:`Ca^{2+}` pump, CaP is an intermediate state,
-  :math:`Ca _{ o }^{2+}` is the extracellular :math:`Ca^{2+}` concentration,
-  and :math:`c_{1}, c_{2}` and :math:`c_{3}` are rate constants. :math:`Ca^{2+}`
-  ions have a high affinity for the pump :math:`P`, whereas extrusion of
-  :math:`Ca^{2+}` follows a slower process (Blaustein, 1988 ). Therefore,
-  :math:`c_{3}` is low compared to :math:`c_{1}` and :math:`c_{2}` and the
-  Michaelis-Menten approximation can be used for describing the kinetics of the pump.
-  According to such a scheme, the kinetic equation for the :math:`Ca^{2+}` pump is:
-
-  .. math::
-
-      \frac{[Ca^{2+}]_{i}}{dt}=-\frac{K_{T}[Ca]_{i}}{[Ca]_{i}+K_{d}}
-
-  where :math:`K_{T}=10^{-4}\, \mathrm{mM\, ms^{-1}}` is the product of :math:`c_{3}`
-  with the total concentration of :math:`P` and :math:`K_{d}=c_{2} / c_{1}=10^{-4}\, \mathrm{mM}`
-  is the dissociation constant, which can be interpreted here as the value of
-  :math:`[Ca]_{i}` at which the pump is half activated (if :math:`[Ca]_{i} \ll K_{d}`
-  then the efflux is negligible).
-
-  **2.A simple first-order model**
-
-  While, in (Bazhenov, et al., 1998) [2]_, the :math:`Ca^{2+}` dynamics is
-  described by a simple first-order model,
-
-  .. math::
-
-      \frac{d\left[Ca^{2+}\right]_{i}}{d t}=-\frac{I_{Ca}}{z F d}+\frac{\left[Ca^{2+}\right]_{rest}-\left[C a^{2+}\right]_{i}}{\tau_{Ca}}
-
-  where :math:`I_{Ca}` is the summation of all :math:`Ca ^{2+}` currents, :math:`d`
-  is the thickness of the perimembrane "shell" in which calcium is able to affect
-  membrane properties :math:`(1.\, \mathrm{\mu M})`, :math:`z=2` is the valence of the
-  :math:`Ca ^{2+}` ion, :math:`F` is the Faraday constant, and :math:`\tau_{C a}` is
-  the :math:`Ca ^{2+}` removal rate. The resting :math:`Ca ^{2+}` concentration was
-  set to be :math:`\left[ Ca ^{2+}\right]_{\text {rest}}=.05\, \mathrm{\mu M}` .
-
-  **3. The reversal potential**
-
-  The reversal potential of calcium :math:`Ca ^{2+}` is calculated according to the
-  Nernst equation:
-
-  .. math::
-
-      E = k'{RT \over 2F} log{[Ca^{2+}]_0 \over [Ca^{2+}]_i}
-
-  where :math:`R=8.31441 \, \mathrm{J} /(\mathrm{mol}^{\circ} \mathrm{K})`,
-  :math:`T=309.15^{\circ} \mathrm{K}`,
-  :math:`F=96,489 \mathrm{C} / \mathrm{mol}`,
-  and :math:`\left[\mathrm{Ca}^{2+}\right]_{0}=2 \mathrm{mM}`.
-
-  Parameters
-  ----------
-  d : float
-    The thickness of the peri-membrane "shell".
-  F : float
-    The Faraday constant. (:math:`C*mmol^{-1}`)
-  tau : float
-    The time constant of the :math:`Ca ^{2+}` removal rate. (ms)
-  C_rest : float
-    The resting :math:`Ca ^{2+}` concentration.
-  C0 : float
-    The :math:`Ca ^{2+}` concentration outside of the membrane.
-  R : float
-    The gas constant. (:math:` J*mol^{-1}*K^{-1}`)
+    The group name.
 
   References
   ----------
 
-  .. [1] Destexhe, Alain, Agnessa Babloyantz, and Terrence J. Sejnowski.
-         "Ionic mechanisms for intrinsic slow oscillations in thalamic
-         relay neurons." Biophysical journal 65, no. 4 (1993): 1538-1552.
-  .. [2] Bazhenov, Maxim, Igor Timofeev, Mircea Steriade, and Terrence J.
-         Sejnowski. "Cellular and network models for intrathalamic augmenting
-         responses during 10-Hz stimulation." Journal of neurophysiology 79,
-         no. 5 (1998): 2730-2748.
-
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      T: Union[float, ArrayType, Initializer, Callable] = 36.,
-      d: Union[float, ArrayType, Initializer, Callable] = 1.,
-      C_rest: Union[float, ArrayType, Initializer, Callable] = 2.4e-4,
-      tau: Union[float, ArrayType, Initializer, Callable] = 5.,
-      C0: Union[float, ArrayType, Initializer, Callable] = 2.,
-      C_initializer: Union[Initializer, Callable, ArrayType] = OneInit(2.4e-4),
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-      **channels
-  ):
-    super(CalciumDetailed, self).__init__(size,
-                                          keep_size=keep_size,
-                                          method=method,
-                                          name=name,
-                                          T=T,
-                                          C0=C0,
-                                          C_initializer=C_initializer,
-                                          mode=mode,
-                                          **channels)
-
-    # parameters
-    self.d = parameter(d, self.varshape, allow_none=False)
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-    self.C_rest = parameter(C_rest, self.varshape, allow_none=False)
-
-  def derivative(self, C, t, V):
-    ICa = self.current(V, C, self.E)
-    drive = bm.maximum(- ICa / (2 * self.F * self.d), 0.)
-    return drive + (self.C_rest - C) / self.tau
-
-
-class CalciumFirstOrder(CalciumDyna):
-  r"""The first-order calcium concentration model.
-
-  .. math::
-
-     Ca' = -\alpha I_{Ca} + -\beta Ca
-
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      T: Union[float, ArrayType, Initializer, Callable] = 36.,
-      alpha: Union[float, ArrayType, Initializer, Callable] = 0.13,
-      beta: Union[float, ArrayType, Initializer, Callable] = 0.075,
-      C0: Union[float, ArrayType, Initializer, Callable] = 2.,
-      C_initializer: Union[Initializer, Callable, ArrayType] = OneInit(2.4e-4),
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-      **channels
-  ):
-    super(CalciumFirstOrder, self).__init__(size,
-                                            keep_size=keep_size,
-                                            method=method,
-                                            name=name,
-                                            T=T,
-                                            C0=C0,
-                                            C_initializer=C_initializer,
-                                            mode=mode,
-                                            **channels)
-
-    # parameters
-    self.alpha = parameter(alpha, self.varshape, allow_none=False)
-    self.beta = parameter(beta, self.varshape, allow_none=False)
-
-  def derivative(self, C, t, V):
-    ICa = self.current(V, C, self.E)
-    drive = bm.maximum(- self.alpha * ICa, 0.)
-    return drive - self.beta * C
-
-
-# -------------------------
-
-
-class _ICa_p2q_ss(CalciumChannel):
-  r"""The calcium current model of :math:`p^2q` current which described with steady-state format.
-
-  The dynamics of this generalized calcium current model is given by:
-
-  .. math::
-
-      I_{CaT} &= g_{max} p^2 q(V-E_{Ca}) \\
-      {dp \over dt} &= {\phi_p \cdot (p_{\infty}-p)\over \tau_p} \\
-      {dq \over dt} &= {\phi_q \cdot (q_{\infty}-q) \over \tau_q} \\
-
-  where :math:`\phi_p` and :math:`\phi_q` are temperature-dependent factors,
-  :math:`E_{Ca}` is the reversal potential of Calcium channel.
-
-  Parameters
-  ----------
-  size: int, tuple of int
-    The size of the simulation target.
-  keep_size: bool
-    Keep size or flatten the size?
-  method: str
-    The numerical method
-  name: str
-    The name of the object.
-  g_max : float, ArrayType, Callable, Initializer
-    The maximum conductance.
-  phi_p : float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`p`.
-  phi_q : float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`q`.
-
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      phi_p: Union[float, ArrayType, Initializer, Callable] = 3.,
-      phi_q: Union[float, ArrayType, Initializer, Callable] = 3.,
-      g_max: Union[float, ArrayType, Initializer, Callable] = 2.,
-      method: str = 'exp_auto',
-      mode: bm.Mode = None,
-      name: str = None
-  ):
-    super(_ICa_p2q_ss, self).__init__(size,
-                                      keep_size=keep_size,
-                                      name=name,
-                                      mode=mode, )
-
-    # parameters
-    self.phi_p = parameter(phi_p, self.varshape, allow_none=False)
-    self.phi_q = parameter(phi_q, self.varshape, allow_none=False)
-    self.g_max = parameter(g_max, self.varshape, allow_none=False)
-
-    # variables
-    self.p = variable(bm.zeros, self.mode, self.varshape)
-    self.q = variable(bm.zeros, self.mode, self.varshape)
-
-    # functions
-    self.integral = odeint(JointEq([self.dp, self.dq]), method=method)
-
-  def dp(self, p, t, V):
-    return self.phi_p * (self.f_p_inf(V) - p) / self.f_p_tau(V)
-
-  def dq(self, q, t, V):
-    return self.phi_q * (self.f_q_inf(V) - q) / self.f_q_tau(V)
-
-  def update(self, tdi, V, C_Ca, E_Ca):
-    self.p.value, self.q.value = self.integral(self.p, self.q, tdi['t'], V, tdi['dt'])
-
-  def current(self, V, C_Ca, E_Ca):
-    return self.g_max * self.p * self.p * self.q * (E_Ca - V)
-
-  def reset_state(self, V, C_Ca, E_Ca, batch_size=None):
-    self.p.value = self.f_p_inf(V)
-    self.q.value = self.f_q_inf(V)
-    if batch_size is not None:
-      assert self.p.shape[0] == batch_size
-      assert self.q.shape[0] == batch_size
-
-  def f_p_inf(self, V):
-    raise NotImplementedError
-
-  def f_p_tau(self, V):
-    raise NotImplementedError
-
-  def f_q_inf(self, V):
-    raise NotImplementedError
-
-  def f_q_tau(self, V):
-    raise NotImplementedError
-
-
-class _ICa_p2q_markov(CalciumChannel):
-  r"""The calcium current model of :math:`p^2q` current which described with first-order Markov chain.
-
-  The dynamics of this generalized calcium current model is given by:
-
-  .. math::
-
-      I_{CaT} &= g_{max} p^2 q(V-E_{Ca}) \\
-      {dp \over dt} &= \phi_p (\alpha_p(V)(1-p) - \beta_p(V)p) \\
-      {dq \over dt} &= \phi_q (\alpha_q(V)(1-q) - \beta_q(V)q) \\
-
-  where :math:`\phi_p` and :math:`\phi_q` are temperature-dependent factors,
-  :math:`E_{Ca}` is the reversal potential of Calcium channel.
-
-  Parameters
-  ----------
-  size: int, tuple of int
-    The size of the simulation target.
-  keep_size: bool
-    Keep size or flatten the size?
-  method: str
-    The numerical method
-  name: str
-    The name of the object.
-  g_max : float, ArrayType, Callable, Initializer
-    The maximum conductance.
-  phi_p : float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`p`.
-  phi_q : float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`q`.
-
+  .. [1] Hodgkin, Alan L., and Andrew F. Huxley. "A quantitative description
+         of membrane current and its application to conduction and excitation
+         in nerve." The Journal of physiology 117.4 (1952): 500.
+  .. [2] https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model
+  .. [3] Ashwin, Peter, Stephen Coombes, and Rachel Nicks. "Mathematical
+         frameworks for oscillatory network dynamics in neuroscience."
+         The Journal of Mathematical Neuroscience 6, no. 1 (2016): 1-92.
   """
 
   def __init__(
       self,
-      size: Shape,
-      keep_size: bool = False,
-      phi_p: Union[float, ArrayType, Initializer, Callable] = 3.,
-      phi_q: Union[float, ArrayType, Initializer, Callable] = 3.,
-      g_max: Union[float, ArrayType, Initializer, Callable] = 2.,
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-  ):
-    super(_ICa_p2q_markov, self).__init__(size,
-                                          keep_size=keep_size,
-                                          name=name,
-                                          mode=mode)
-
-    # parameters
-    self.phi_p = parameter(phi_p, self.varshape, allow_none=False)
-    self.phi_q = parameter(phi_q, self.varshape, allow_none=False)
-    self.g_max = parameter(g_max, self.varshape, allow_none=False)
-
-    # variables
-    self.p = variable(bm.zeros, self.mode, self.varshape)
-    self.q = variable(bm.zeros, self.mode, self.varshape)
-
-    # functions
-    self.integral = odeint(JointEq([self.dp, self.dq]), method=method)
-
-  def dp(self, p, t, V):
-    return self.phi_p * (self.f_p_alpha(V) * (1 - p) - self.f_p_beta(V) * p)
-
-  def dq(self, q, t, V):
-    return self.phi_q * (self.f_q_alpha(V) * (1 - q) - self.f_q_beta(V) * q)
-
-  def update(self, tdi, V, C_Ca, E_Ca):
-    self.p.value, self.q.value = self.integral(self.p, self.q, tdi['t'], V, tdi['dt'])
-
-  def current(self, V, C_Ca, E_Ca):
-    return self.g_max * self.p * self.p * self.q * (E_Ca - V)
-
-  def reset_state(self, V, C_Ca, E_Ca, batch_size=None):
-    alpha, beta = self.f_p_alpha(V), self.f_p_beta(V)
-    self.p.value = alpha / (alpha + beta)
-    alpha, beta = self.f_q_alpha(V), self.f_q_beta(V)
-    self.q.value = alpha / (alpha + beta)
-    if batch_size is not None:
-      assert self.p.shape[0] == batch_size
-      assert self.q.shape[0] == batch_size
-
-  def f_p_alpha(self, V):
-    raise NotImplementedError
-
-  def f_p_beta(self, V):
-    raise NotImplementedError
-
-  def f_q_alpha(self, V):
-    raise NotImplementedError
-
-  def f_q_beta(self, V):
-    raise NotImplementedError
-
-
-class ICaN_IS2008(CalciumChannel):
-  r"""The calcium-activated non-selective cation channel model
-  proposed by (Inoue & Strowbridge, 2008) [2]_.
-
-  The dynamics of the calcium-activated non-selective cation channel model [1]_ [2]_ is given by:
+      *args,
+      input_var: bool = True,
+      noise: Union[float, ArrayType, Initializer, Callable] = None,
+      **kwargs,
+  ):
+    self.input_var = input_var
+    super().__init__(*args, **kwargs, init_var=False)
+
+    self.noise = init_noise(noise, self.varshape, num_vars=4)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
+    self.reset_state(self.mode)
+
+  def reset_state(self, batch_size=None):
+    super().reset_state(batch_size)
+    if self.input_var:
+      self.input = variable_(bm.zeros, self.varshape, batch_size)
+
+  def update(self, x=None):
+    if self.input_var:
+      if x is not None:
+        self.input += x
+      x = self.input.value
+    else:
+      x = 0. if x is None else x
+    return super().update(x)
+
+  def clear_input(self):
+    if self.input_var:
+      self.input.value = bm.zeros_like(self.input)
+
+
+class MorrisLecar(hh.MorrisLecar):
+  r"""The Morris-Lecar neuron model.
+
+  **Model Descriptions**
+
+  The Morris-Lecar model [4]_ (Also known as :math:`I_{Ca}+I_K`-model)
+  is a two-dimensional "reduced" excitation model applicable to
+  systems having two non-inactivating voltage-sensitive conductances.
+  This model was named after Cathy Morris and Harold Lecar, who
+  derived it in 1981. Because it is two-dimensional, the Morris-Lecar
+  model is one of the favorite conductance-based models in computational neuroscience.
+
+  The original form of the model employed an instantaneously
+  responding voltage-sensitive Ca2+ conductance for excitation and a delayed
+  voltage-dependent K+ conductance for recovery. The equations of the model are:
 
   .. math::
 
       \begin{aligned}
-      I_{CAN} &=g_{\mathrm{max}} M\left([Ca^{2+}]_{i}\right) p \left(V-E\right)\\
-      &M\left([Ca^{2+}]_{i}\right) ={[Ca^{2+}]_{i} \over 0.2+[Ca^{2+}]_{i}} \\
-      &{dp \over dt} = {\phi \cdot (p_{\infty}-p)\over \tau_p} \\
-      &p_{\infty} = {1.0 \over 1 + \exp(-(V + 43) / 5.2)} \\
-      &\tau_{p} = {2.7 \over \exp(-(V + 55) / 15) + \exp((V + 55) / 15)} + 1.6
+      C\frac{dV}{dt} =& -  g_{Ca} M_{\infty} (V - V_{Ca})- g_{K} W(V - V_{K}) -
+                        g_{Leak} (V - V_{Leak}) + I_{ext} \\
+      \frac{dW}{dt} =& \frac{W_{\infty}(V) - W}{ \tau_W(V)}
       \end{aligned}
 
-  where :math:`\phi` is the temperature factor.
-
-  Parameters
-  ----------
-  g_max : float
-    The maximal conductance density (:math:`mS/cm^2`).
-  E : float
-    The reversal potential (mV).
-  phi : float
-    The temperature factor.
+  Here, :math:`V` is the membrane potential, :math:`W` is the "recovery variable",
+  which is almost invariably the normalized :math:`K^+`-ion conductance, and
+  :math:`I_{ext}` is the applied current stimulus.
+
+  **Model Examples**
+
+  .. plot::
+    :include-source: True
+
+    >>> import brainpy as bp
+    >>>
+    >>> group = bp.neurons.MorrisLecar(1)
+    >>> runner = bp.DSRunner(group, monitors=['V', 'W'], inputs=('input', 100.))
+    >>> runner.run(1000)
+    >>>
+    >>> fig, gs = bp.visualize.get_figure(2, 1, 3, 8)
+    >>> fig.add_subplot(gs[0, 0])
+    >>> bp.visualize.line_plot(runner.mon.ts, runner.mon.W, ylabel='W')
+    >>> fig.add_subplot(gs[1, 0])
+    >>> bp.visualize.line_plot(runner.mon.ts, runner.mon.V, ylabel='V', show=True)
+
+
+  **Model Parameters**
+
+  ============= ============== ======== =======================================================
+  **Parameter** **Init Value** **Unit** **Explanation**
+  ------------- -------------- -------- -------------------------------------------------------
+  V_Ca          130            mV       Equilibrium potentials of Ca+.(mV)
+  g_Ca          4.4            \        Maximum conductance of corresponding Ca+.(mS/cm2)
+  V_K           -84            mV       Equilibrium potentials of K+.(mV)
+  g_K           8              \        Maximum conductance of corresponding K+.(mS/cm2)
+  V_Leak        -60            mV       Equilibrium potentials of leak current.(mV)
+  g_Leak        2              \        Maximum conductance of leak current.(mS/cm2)
+  C             20             \        Membrane capacitance.(uF/cm2)
+  V1            -1.2           \        Potential at which M_inf = 0.5.(mV)
+  V2            18             \        Reciprocal of slope of voltage dependence of M_inf.(mV)
+  V3            2              \        Potential at which W_inf = 0.5.(mV)
+  V4            30             \        Reciprocal of slope of voltage dependence of W_inf.(mV)
+  phi           0.04           \        A temperature factor. (1/s)
+  V_th          10             mV       The spike threshold.
+  ============= ============== ======== =======================================================
 
   References
   ----------
 
-  .. [1] Destexhe, Alain, et al. "A model of spindle rhythmicity in the isolated
-         thalamic reticular nucleus." Journal of neurophysiology 72.2 (1994): 803-818.
-  .. [2] Inoue T, Strowbridge BW (2008) Transient activity induces a long-lasting
-         increase in the excitability of olfactory bulb interneurons.
-         J Neurophysiol 99: 187199.
+  .. [4] Lecar, Harold. "Morris-lecar model." Scholarpedia 2.10 (2007): 1333.
+  .. [5] http://www.scholarpedia.org/article/Morris-Lecar_model
+  .. [6] https://en.wikipedia.org/wiki/Morris%E2%80%93Lecar_model
   """
 
-  '''The type of the master object.'''
-  master_type = CalciumDyna
-
   def __init__(
       self,
-      size: Shape,
-      keep_size: bool = False,
-      E: Union[float, ArrayType, Initializer, Callable] = 10.,
-      g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
-      phi: Union[float, ArrayType, Initializer, Callable] = 1.,
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-  ):
-    super(ICaN_IS2008, self).__init__(size,
-                                      keep_size=keep_size,
-                                      name=name,
-                                      mode=mode)
-
-    # parameters
-    self.E = parameter(E, self.varshape, allow_none=False)
-    self.g_max = parameter(g_max, self.varshape, allow_none=False)
-    self.phi = parameter(phi, self.varshape, allow_none=False)
-
-    # variables
-    self.p = variable(bm.zeros, self.mode, self.varshape)
+      *args,
+      input_var: bool = True,
+      noise: Union[float, ArrayType, Initializer, Callable] = None,
+      **kwargs,
+  ):
+    self.input_var = input_var
+    super().__init__(*args, **kwargs, init_var=False)
+    self.noise = init_noise(noise, self.varshape, num_vars=2)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
+    self.reset_state(self.mode)
+
+  def reset_state(self, batch_size=None):
+    super().reset_state(batch_size)
+    if self.input_var:
+      self.input = variable_(bm.zeros, self.varshape, batch_size)
+
+  def update(self, x=None):
+    if self.input_var:
+      if x is not None:
+        self.input += x
+      x = self.input.value
+    else:
+      x = 0. if x is None else x
+    return super().update(x)
+
+  def clear_input(self):
+    if self.input_var:
+      self.input.value = bm.zeros_like(self.input)
+
+
+class PinskyRinzelModel(NeuDyn):
+  r"""The Pinsky and Rinsel (1994) model.
+
+  The Pinsky and Rinsel (1994) model [7]_ is a 2-compartment (soma and dendrite),
+  conductance-based (Hodgin-Huxley type) model of a hippocampal CA3 pyramidal
+  neuron. It is a reduced version of an earlier, 19-compartment model by
+  Traub, et. al. (1991) [8]_. This model demonstrates how similar qualitative
+  and quantitative spiking behaviors can be obtained despite the reduction
+  in model complexity.
+
+  Specifically, this model demonstrates calcium bursting behavior and how
+  the 'ping-pong' interplay between somatic and dendritic currents results
+  in a complex shape of the burst.
 
-    # function
-    self.integral = odeint(self.derivative, method=method)
+  .. image:: ../../../_static/Pinsky-Rinzel-model-illustration.png
+      :align: center
 
-  def derivative(self, p, t, V):
-    phi_p = 1.0 / (1 + bm.exp(-(V + 43.) / 5.2))
-    p_inf = 2.7 / (bm.exp(-(V + 55.) / 15.) + bm.exp((V + 55.) / 15.)) + 1.6
-    return self.phi * (phi_p - p) / p_inf
-
-  def update(self, tdi, V, C_Ca, E_Ca):
-    self.p.value = self.integral(self.p.value, tdi['t'], V, tdi['dt'])
-
-  def current(self, V, C_Ca, E_Ca):
-    M = C_Ca / (C_Ca + 0.2)
-    g = self.g_max * M * self.p
-    return g * (self.E - V)
-
-  def reset_state(self, V, C_Ca, E_Ca, batch_size=None):
-    self.p.value = 1.0 / (1 + bm.exp(-(V + 43.) / 5.2))
-    if batch_size is not None:
-      assert self.p.shape[0] == batch_size
-
-
-class ICaT_HM1992(_ICa_p2q_ss):
-  r"""The low-threshold T-type calcium current model proposed by (Huguenard & McCormick, 1992) [1]_.
-
-  The dynamics of the low-threshold T-type calcium current model [1]_ is given by:
+  Mathematically, the model is given by:
 
   .. math::
 
-      I_{CaT} &= g_{max} p^2 q(V-E_{Ca}) \\
-      {dp \over dt} &= {\phi_p \cdot (p_{\infty}-p)\over \tau_p} \\
-      &p_{\infty} = {1 \over 1+\exp [-(V+59-V_{sh}) / 6.2]} \\
-      &\tau_{p} = 0.612 + {1 \over \exp [-(V+132.-V_{sh}) / 16.7]+\exp [(V+16.8-V_{sh}) / 18.2]} \\
-      {dq \over dt} &= {\phi_q \cdot (q_{\infty}-q) \over \tau_q} \\
-      &q_{\infty} = {1 \over 1+\exp [(V+83-V_{sh}) / 4]} \\
-      & \begin{array}{l} \tau_{q} = \exp \left(\frac{V+467-V_{sh}}{66.6}\right)  \quad V< (-80 +V_{sh})\, mV  \\
-          \tau_{q} = \exp \left(\frac{V+22-V_{sh}}{-10.5}\right)+28 \quad V \geq (-80 + V_{sh})\, mV \end{array}
-
-  where :math:`\phi_p = 3.55^{\frac{T-24}{10}}` and :math:`\phi_q = 3^{\frac{T-24}{10}}`
-  are temperature-dependent factors (:math:`T` is the temperature in Celsius),
-  :math:`E_{Ca}` is the reversal potential of Calcium channel.
-
-  Parameters
-  ----------
-  T : float, ArrayType
-    The temperature.
-  T_base_p : float, ArrayType
-    The brainpy_object temperature factor of :math:`p` channel.
-  T_base_q : float, ArrayType
-    The brainpy_object temperature factor of :math:`q` channel.
-  g_max : float, ArrayType, Callable, Initializer
-    The maximum conductance.
-  V_sh : float, ArrayType, Callable, Initializer
-    The membrane potential shift.
-  phi_p : optional, float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`p`.
-  phi_q : optional, float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`q`.
-
-  References
-  ----------
-
-  .. [1] Huguenard JR, McCormick DA (1992) Simulation of the currents involved in
-         rhythmic oscillations in thalamic relay neurons. J Neurophysiol 68:13731383.
-
-  See Also
-  --------
-  ICa_p2q_form
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      T: Union[float, ArrayType] = 36.,
-      T_base_p: Union[float, ArrayType] = 3.55,
-      T_base_q: Union[float, ArrayType] = 3.,
-      g_max: Union[float, ArrayType, Initializer, Callable] = 2.,
-      V_sh: Union[float, ArrayType, Initializer, Callable] = -3.,
-      phi_p: Union[float, ArrayType, Initializer, Callable] = None,
-      phi_q: Union[float, ArrayType, Initializer, Callable] = None,
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-  ):
-    phi_p = T_base_p ** ((T - 24) / 10) if phi_p is None else phi_p
-    phi_q = T_base_q ** ((T - 24) / 10) if phi_q is None else phi_q
-    super(ICaT_HM1992, self).__init__(size,
-                                      keep_size=keep_size,
-                                      name=name,
-                                      method=method,
-                                      g_max=g_max,
-                                      phi_p=phi_p,
-                                      phi_q=phi_q,
-                                      mode=mode)
-
-    # parameters
-    self.T = parameter(T, self.varshape, allow_none=False)
-    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
-    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
-    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
-
-  def f_p_inf(self, V):
-    return 1. / (1 + bm.exp(-(V + 59. - self.V_sh) / 6.2))
-
-  def f_p_tau(self, V):
-    return 1. / (bm.exp(-(V + 132. - self.V_sh) / 16.7) +
-                 bm.exp((V + 16.8 - self.V_sh) / 18.2)) + 0.612
-
-  def f_q_inf(self, V):
-    return 1. / (1. + bm.exp((V + 83. - self.V_sh) / 4.0))
-
-  def f_q_tau(self, V):
-    return bm.where(V >= (-80. + self.V_sh),
-                     bm.exp(-(V + 22. - self.V_sh) / 10.5) + 28.,
-                     bm.exp((V + 467. - self.V_sh) / 66.6))
-
-
-class ICaT_HP1992(_ICa_p2q_ss):
-  r"""The low-threshold T-type calcium current model for thalamic
-  reticular nucleus proposed by (Huguenard & Prince, 1992) [1]_.
-
-  The dynamics of the low-threshold T-type calcium current model in thalamic
-  reticular nucleus neurons [1]_ is given by:
-
-  .. math::
-
-      I_{CaT} &= g_{max} p^2 q(V-E_{Ca}) \\
-      {dp \over dt} &= {\phi_p \cdot (p_{\infty}-p)\over \tau_p} \\
-      &p_{\infty} = {1 \over 1+\exp [-(V+52-V_{sh}) / 7.4]}  \\
-      &\tau_{p} = 3+{1 \over \exp [(V+27-V_{sh}) / 10]+\exp [-(V+102-V_{sh}) / 15]} \\
-      {dq \over dt} &= {\phi_q \cdot (q_{\infty}-q) \over \tau_q} \\
-      &q_{\infty} = {1 \over 1+\exp [(V+80-V_{sh}) / 5]} \\
-      & \tau_q = 85+ {1 \over \exp [(V+48-V_{sh}) / 4]+\exp [-(V+407-V_{sh}) / 50]}
-
-  where :math:`\phi_p = 5^{\frac{T-24}{10}}` and :math:`\phi_q = 3^{\frac{T-24}{10}}`
-  are temperature-dependent factors (:math:`T` is the temperature in Celsius),
-  :math:`E_{Ca}` is the reversal potential of Calcium channel.
-
-  Parameters
-  ----------
-  T : float, ArrayType
-    The temperature.
-  T_base_p : float, ArrayType
-    The brainpy_object temperature factor of :math:`p` channel.
-  T_base_q : float, ArrayType
-    The brainpy_object temperature factor of :math:`q` channel.
-  g_max : float, ArrayType, Callable, Initializer
-    The maximum conductance.
-  V_sh : float, ArrayType, Callable, Initializer
-    The membrane potential shift.
-  phi_p : optional, float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`p`.
-  phi_q : optional, float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`q`.
-
-  References
-  ----------
-
-  .. [1] Huguenard JR, Prince DA (1992) A novel T-type current underlies
-         prolonged Ca2+- dependent burst firing in GABAergic neurons of rat
-         thalamic reticular nucleus. J Neurosci 12: 38043817.
-
-  See Also
-  --------
-  ICa_p2q_form
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      T: Union[float, ArrayType] = 36.,
-      T_base_p: Union[float, ArrayType] = 5.,
-      T_base_q: Union[float, ArrayType] = 3.,
-      g_max: Union[float, ArrayType, Initializer, Callable] = 1.75,
-      V_sh: Union[float, ArrayType, Initializer, Callable] = -3.,
-      phi_p: Union[float, ArrayType, Initializer, Callable] = None,
-      phi_q: Union[float, ArrayType, Initializer, Callable] = None,
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-  ):
-    phi_p = T_base_p ** ((T - 24) / 10) if phi_p is None else phi_p
-    phi_q = T_base_q ** ((T - 24) / 10) if phi_q is None else phi_q
-    super(ICaT_HP1992, self).__init__(size,
-                                      keep_size=keep_size,
-                                      name=name,
-                                      method=method,
-                                      g_max=g_max,
-                                      phi_p=phi_p,
-                                      phi_q=phi_q,
-                                      mode=mode)
-
-    # parameters
-    self.T = parameter(T, self.varshape, allow_none=False)
-    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
-    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
-    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
-
-  def f_p_inf(self, V):
-    return 1. / (1. + bm.exp(-(V + 52. - self.V_sh) / 7.4))
-
-  def f_p_tau(self, V):
-    return 3. + 1. / (bm.exp((V + 27. - self.V_sh) / 10.) +
-                      bm.exp(-(V + 102. - self.V_sh) / 15.))
-
-  def f_q_inf(self, V):
-    return 1. / (1. + bm.exp((V + 80. - self.V_sh) / 5.))
-
-  def f_q_tau(self, V):
-    return 85. + 1. / (bm.exp((V + 48. - self.V_sh) / 4.) +
-                       bm.exp(-(V + 407. - self.V_sh) / 50.))
-
-
-class ICaHT_HM1992(_ICa_p2q_ss):
-  r"""The high-threshold T-type calcium current model proposed by (Huguenard & McCormick, 1992) [1]_.
+     \begin{aligned}
+    &\mathrm{C}_{\mathrm{m}} \mathrm{V}_{\mathrm{s}}^{\prime}=-\mathrm{I}_{\mathrm{Leak}}-\mathrm{I}_{\mathrm{Na}}-\mathrm{I}_{\mathrm{K}_{\mathrm{DR}}}-\frac{\mathrm{I}_{\mathrm{DS}}}{\mathrm{p}}+\frac{\mathrm{I}_{\mathrm{S}_{\mathrm{app}}}}{\mathrm{p}} \\
+    &\mathrm{C}_{\mathrm{m}} \mathrm{V}_{\mathrm{d}}^{\prime}=-\mathrm{I}_{\mathrm{Leak}}-\mathrm{I}_{\mathrm{Ca}}-\mathrm{I}_{\mathrm{K}_{\mathrm{Ca}}}-\mathrm{I}_{\mathrm{K}_{\mathrm{AHP}}}+\frac{\mathrm{I}_{\mathrm{SD}}}{(1-\mathrm{p})}+\frac{\mathrm{I}_{\mathrm{D}_{\mathrm{app}}}}{(1-\mathrm{p})} \\
+    &\frac{\mathrm{dCa}}{\mathrm{dt}}=-0.13 \mathrm{I}_{\mathrm{Ca}}-0.075 \mathrm{Ca}
+    \end{aligned}
 
-  The high-threshold T-type calcium current model is adopted from [1]_.
-  Its dynamics is given by
+  The currents of the model are functions of potentials as follows:
 
   .. math::
 
-      \begin{aligned}
-      I_{\mathrm{Ca/HT}} &= g_{\mathrm{max}} p^2 q (V-E_{Ca})
-      \\
-      {dp \over dt} &= {\phi_{p} \cdot (p_{\infty} - p) \over \tau_{p}} \\
-      &\tau_{p} =\frac{1}{\exp \left(\frac{V+132-V_{sh}}{-16.7}\right)+\exp \left(\frac{V+16.8-V_{sh}}{18.2}\right)}+0.612 \\
-      & p_{\infty} = {1 \over 1+exp[-(V+59-V_{sh}) / 6.2]}
-      \\
-      {dq \over dt} &= {\phi_{q} \cdot (q_{\infty} - h) \over \tau_{q}} \\
-      & \begin{array}{l} \tau_q = \exp \left(\frac{V+467-V_{sh}}{66.6}\right)  \quad V< (-80 +V_{sh})\, mV  \\
-      \tau_q = \exp \left(\frac{V+22-V_{sh}}{-10.5}\right)+28 \quad V \geq (-80 + V_{sh})\, mV \end{array} \\
-      &q_{\infty}  = {1 \over 1+exp[(V+83 -V_{shift})/4]}
+     \begin{aligned}
+      \mathrm{I}_{\mathrm{Na}} &=\mathrm{g}_{\mathrm{Na}} m_{\infty}^{2}\left(\mathrm{~V}_{\mathrm{s}}\right) h\left(\mathrm{~V}_{\mathrm{s}}-\mathrm{V}_{\mathrm{Na}}\right) \\
+      \mathrm{I}_{\mathrm{K}_{\mathrm{DR}}} &=\mathrm{g}_{\mathrm{K}_{\mathrm{DR}}} n\left(\mathrm{~V}_{\mathrm{s}}-\mathrm{V}_{\mathrm{K}}\right) \\
+      \mathrm{I}_{\mathrm{Ca}} &=\mathrm{g}_{\mathrm{Ca}}{ }^{2}\left(\mathrm{~V}_{\mathrm{d}}-\mathrm{V}_{\mathrm{N}}\right) \\
+      \mathrm{I}_{\mathrm{K}_{\mathrm{Ca}}} &=\mathrm{g}_{\mathrm{k}_{\mathrm{Ca}}} C \chi(\mathrm{Ca})\left(\mathrm{V}_{\mathrm{d}}-\mathrm{V}_{\mathrm{Ca}}\right) \\
+      \mathrm{I}_{\mathrm{K}_{\mathrm{AHP}}} &=\mathrm{g}_{\mathrm{K}_{\mathrm{AHP}}} q\left(\mathrm{~V}_{\mathrm{d}}-\mathrm{V}_{\mathrm{K}}\right) \\
+      \mathrm{I}_{\mathrm{SD}} &=-\mathrm{I}_{\mathrm{DS}}=\mathrm{g}_{\mathrm{c}}\left(\mathrm{V}_{\mathrm{d}}-\mathrm{V}_{\mathrm{s}}\right) \\
+      \mathrm{I}_{\mathrm{Leak}} &=\mathrm{g}_{\mathrm{L}}\left(\mathrm{V}-\mathrm{V}_{\mathrm{L}}\right)
       \end{aligned}
 
-  where :math:`phi_p = 3.55^{\frac{T-24}{10}}` and :math:`phi_q = 3^{\frac{T-24}{10}}`
-  are temperature-dependent factors (:math:`T` is the temperature in Celsius),
-  :math:`E_{Ca}` is the reversal potential of Calcium channel.
-
-  Parameters
-  ----------
-  T : float, ArrayType
-    The temperature.
-  T_base_p : float, ArrayType
-    The brainpy_object temperature factor of :math:`p` channel.
-  T_base_q : float, ArrayType
-    The brainpy_object temperature factor of :math:`q` channel.
-  g_max : float, ArrayType, Initializer, Callable
-    The maximum conductance.
-  V_sh : float, ArrayType, Initializer, Callable
-    The membrane potential shift.
-
-  References
-  ----------
-  .. [1] Huguenard JR, McCormick DA (1992) Simulation of the currents involved in
-         rhythmic oscillations in thalamic relay neurons. J Neurophysiol 68:13731383.
-
-  See Also
-  --------
-  ICa_p2q_form
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      T: Union[float, ArrayType] = 36.,
-      T_base_p: Union[float, ArrayType] = 3.55,
-      T_base_q: Union[float, ArrayType] = 3.,
-      g_max: Union[float, ArrayType, Initializer, Callable] = 2.,
-      V_sh: Union[float, ArrayType, Initializer, Callable] = 25.,
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-  ):
-    super(ICaHT_HM1992, self).__init__(size,
-                                       keep_size=keep_size,
-                                       name=name,
-                                       method=method,
-                                       g_max=g_max,
-                                       phi_p=T_base_p ** ((T - 24) / 10),
-                                       phi_q=T_base_q ** ((T - 24) / 10),
-                                       mode=mode)
-
-    # parameters
-    self.T = parameter(T, self.varshape, allow_none=False)
-    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
-    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
-    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
+  The activation and inactivation variables should satisfy these equations
 
-    # variables
-    self.p = variable(bm.zeros, self.mode, self.varshape)
-    self.q = variable(bm.zeros, self.mode, self.varshape)
-
-    # function
-    self.integral = odeint(JointEq([self.dp, self.dq]), method=method)
-
-  def f_p_inf(self, V):
-    return 1. / (1. + bm.exp(-(V + 59. - self.V_sh) / 6.2))
-
-  def f_p_tau(self, V):
-    return 1. / (bm.exp(-(V + 132. - self.V_sh) / 16.7) +
-                 bm.exp((V + 16.8 - self.V_sh) / 18.2)) + 0.612
-
-  def f_q_inf(self, V):
-    return 1. / (1. + bm.exp((V + 83. - self.V_sh) / 4.))
-
-  def f_q_tau(self, V):
-    return bm.where(V >= (-80. + self.V_sh),
-                     bm.exp(-(V + 22. - self.V_sh) / 10.5) + 28.,
-                     bm.exp((V + 467. - self.V_sh) / 66.6))
+  .. math::
 
+     \begin{aligned}
+    \omega^{\prime}(\mathrm{V}) &=\frac{\omega_{\infty}(\mathrm{V})-\omega}{\tau_{\omega}(\mathrm{V})} \\
+    \omega_{\infty}(\mathrm{V}) &=\frac{\alpha_{\omega}(\mathrm{V})}{\alpha_{\omega}(\mathrm{V})+\beta_{\omega}(\mathrm{V})} \\
+    \tau_{\omega}(\mathrm{V}) &=\frac{1}{\alpha_{\omega}(\mathrm{V})+\beta_{\omega}(\mathrm{V})}
+    \end{aligned}
 
-class ICaHT_Re1993(_ICa_p2q_markov):
-  r"""The high-threshold T-type calcium current model proposed by (Reuveni, et al., 1993) [1]_.
+  where, independently, we consider :math:`\omega = h, n, s, m, c, q`.
 
-  HVA Calcium current was described for neocortical neurons by Sayer et al. (1990).
-  Its dynamics is given by (the rate functions are measured under 36 Celsius):
+  The rate functions are defined as follows
 
   .. math::
 
      \begin{aligned}
-      I_{L} &=\bar{g}_{L} q^{2} r\left(V-E_{\mathrm{Ca}}\right) \\
-      \frac{\mathrm{d} q}{\mathrm{~d} t} &= \phi_p (\alpha_{q}(V)(1-q)-\beta_{q}(V) q) \\
-      \frac{\mathrm{d} r}{\mathrm{~d} t} &= \phi_q (\alpha_{r}(V)(1-r)-\beta_{r}(V) r) \\
-      \alpha_{q} &=\frac{0.055(-27-V+V_{sh})}{\exp [(-27-V+V_{sh}) / 3.8]-1} \\
-      \beta_{q} &=0.94 \exp [(-75-V+V_{sh}) / 17] \\
-      \alpha_{r} &=0.000457 \exp [(-13-V+V_{sh}) / 50] \\
-      \beta_{r} &=\frac{0.0065}{\exp [(-15-V+V_{sh}) / 28]+1},
-      \end{aligned}
+    \alpha_{m}\left(\mathrm{~V}_{\mathrm{s}}\right) &=\frac{0.32\left(-46.9-\mathrm{V}_{\mathrm{s}}\right)}{\exp \left(\frac{-46.9-\mathrm{V}_{\mathrm{s}}}{4}\right)-1} \\
+    \beta_{m}\left(\mathrm{~V}_{\mathrm{s}}\right) &=\frac{0.28\left(\mathrm{~V}_{\mathrm{s}}+19.9\right)}{\exp \left(\frac{\mathrm{V}_{\mathrm{s}}+19.9}{5}\right)-1}, \\
+    \alpha_{n}\left(\mathrm{~V}_{\mathrm{s}}\right) &=\frac{0.016\left(-24.9-\mathrm{V}_{\mathrm{s}}\right)}{\exp \left(\frac{-24.9-\mathrm{V}_{\mathrm{s}}}{5}\right)-1} \\
+    \beta_{n}\left(\mathrm{~V}_{\mathrm{s}}\right) &=0.25 \exp \left(-1-0.025 \mathrm{~V}_{\mathrm{s}}\right) \\
+    \alpha_{h}\left(\mathrm{~V}_{\mathrm{s}}\right) &=0.128 \exp \left(\frac{-43-\mathrm{V}_{\mathrm{s}}}{18}\right) \\
+    \beta_{h}\left(\mathrm{~V}_{\mathrm{s}}\right) &=\frac{4}{1+\exp \left(\frac{\left(-20-\mathrm{V}_{\mathrm{s}}\right.}{5}\right)}, \\
+    \alpha_{s}\left(\mathrm{~V}_{\mathrm{d}}\right) &=\frac{1.6}{1+\exp \left(-0.072\left(\mathrm{~V}_{\mathrm{d}}-5\right)\right)} \\
+    \beta_{s}\left(\mathrm{~V}_{\mathrm{d}}\right) &=\frac{0.02\left(\mathrm{~V}_{\mathrm{d}}+8.9\right)}{\exp \left(\frac{\left(\mathrm{V}_{\mathrm{d}}+8.9\right)}{5}\right)-1}, \\
+    \alpha_{C}\left(\mathrm{~V}_{\mathrm{d}}\right) &=\frac{\left(1-H\left(\mathrm{~V}_{\mathrm{d}}+10\right)\right) \exp \left(\frac{\left(\mathrm{V}_{\mathrm{d}}+50\right)}{11}-\frac{\left(\mathrm{V}_{\mathrm{d}}+53.5\right)}{27}\right)}{18.975}+H\left(\mathrm{~V}_{\mathrm{d}}+10\right)\left(2 \exp \left(\frac{\left(-53.5-\mathrm{V}_{\mathrm{d}}\right.}{27}\right)\right) \\
+    \beta_{C}\left(\mathrm{~V}_{\mathrm{d}}\right) &=\left(1-H\left(\mathrm{~V}_{\mathrm{d}}+10\right)\right)\left(2 \exp \left(\frac{\left(-53.5-\mathrm{V}_{\mathrm{d}}\right)}{27}\right)-\alpha_{c}\left(\mathrm{~V}_{\mathrm{d}}\right)\right) \\
+    \alpha_{q}(\mathrm{Ca}) &=\min (0.00002 \mathrm{Ca}, 0.01) \\
+    \beta_{q}(\mathrm{Ca}) &=0.001 \\
+    \chi(\mathrm{Ca}) &=\min \left(\frac{\mathrm{Ca}}{250}, 1\right)
+    \end{aligned}
+
+  The standard values of the parameters are given below. The maximal conductances
+  (in :math:`\mathrm{mS} / \mathrm{cm}^{2}`) are
+  :math:`\bar{g}_{L}=0.1`, :math:`\bar{g}_{\mathrm{Na}}=30`,
+  :math:`\bar{g}_{\mathrm{K}-\mathrm{DR}}=15`,
+  :math:`\bar{g}_{\mathrm{Ca}}=10`,
+  :math:`\bar{g}_{\mathrm{K}-\mathrm{AHP}}=0.8`,
+  :math:`\bar{g}_{\mathrm{K}-\mathrm{C}}=15`,
+  :math:`\bar{g}_{\mathrm{NMDA}}=0.0` and
+  :math:`\bar{g}_{\mathrm{AMPA}}=0.0`.
+  The reversal potentials (in :math:`\mathrm{mV}` ) are
+  :math:`V_{\mathrm{Na}}=120, V_{\mathrm{C}}=140, V_{\mathrm{K}}=-15 \mathrm{mV})`
+  are :math:`V_{\mathrm{Na}}=120, V_{\mathrm{Ca}}=140, V_{\mathrm{K}}=-15, $V_{L}=0`
+  and :math:`V_{\text {Syn }}=60`. The applied currents
+  (in :math:`\mu \mathrm{A} / \mathrm{cm}^{2}` ) are :math:`I_{s}=-0.5` and :math:`I_{d}=0.0`.
+  The coupling parameters are :math:`g_{c}=2.1 \mathrm{mS} / \mathrm{cm}^{2}` and
+  :math:`p=0.5`. The capacitance, :math:`C_{M}`, is
+  :math:`3 \mu \mathrm{F} / \mathrm{cm}^{2}` and :math:`\chi(C a)=\min (C a / 250,1)`.
+  Values for these parameters, and these function definitions, are taken from Traub et al, 1991.
+
 
   Parameters
   ----------
-  size: int, tuple of int
-    The size of the simulation target.
-  keep_size: bool
-    Keep size or flatten the size?
+  size: sequence of int, int
+    The size of the neuron group.
+  gNa: float, ArrayType, Initializer, callable
+    The maximum conductance of sodium channel.
+  gK: float, ArrayType, Initializer, callable
+    The maximum conductance of potassium delayed-rectifier channel.
+  gCa: float, ArrayType, Initializer, callable
+    The maximum conductance of calcium channel.
+  gAHP: float, ArrayType, Initializer, callable
+    The maximum conductance of potassium after-hyper-polarization channel.
+  gC: float, ArrayType, Initializer, callable
+    The maximum conductance of calcium activated potassium channel.
+  gL: float, ArrayType, Initializer, callable
+    The conductance of leaky channel.
+  ENa: float, ArrayType, Initializer, callable
+    The reversal potential of sodium channel.
+  EK: float, ArrayType, Initializer, callable
+    The reversal potential of potassium delayed-rectifier channel.
+  ECa: float, ArrayType, Initializer, callable
+    The reversal potential of calcium channel.
+  EL: float, ArrayType, Initializer, callable
+    The reversal potential of leaky channel.
+  gc: float, ArrayType, Initializer, callable
+    The coupling strength between the soma and dendrite.
+  V_th: float, ArrayType, Initializer, callable
+    The threshold of the membrane spike.
+  Cm: float, ArrayType, Initializer, callable
+    The threshold of the membrane spike.
+  A: float, ArrayType, Initializer, callable
+    The total cell membrane area, which is normalized to 1.
+  p: float, ArrayType, Initializer, callable
+    The proportion of cell area taken up by the soma.
+  Vs_initializer: ArrayType, Initializer, callable
+    The initializer of somatic membrane potential.
+  Vd_initializer: ArrayType, Initializer, callable
+    The initializer of dendritic membrane potential.
+  Ca_initializer: ArrayType, Initializer, callable
+    The initializer of Calcium concentration.
   method: str
-    The numerical method
+    The numerical integration method.
   name: str
-    The name of the object.
-  g_max : float, ArrayType, Callable, Initializer
-    The maximum conductance.
-  V_sh : float, ArrayType, Callable, Initializer
-    The membrane potential shift.
-  T : float, ArrayType
-    The temperature.
-  T_base_p : float, ArrayType
-    The brainpy_object temperature factor of :math:`p` channel.
-  T_base_q : float, ArrayType
-    The brainpy_object temperature factor of :math:`q` channel.
-  phi_p : optional, float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`p`.
-    If `None`, :math:`\phi_p = \mathrm{T_base_p}^{\frac{T-23}{10}}`.
-  phi_q : optional, float, ArrayType, Callable, Initializer
-    The temperature factor for channel :math:`q`.
-    If `None`, :math:`\phi_q = \mathrm{T_base_q}^{\frac{T-23}{10}}`.
+    The group name.
 
   References
   ----------
-  .. [1] Reuveni, I., et al. "Stepwise repolarization from Ca2+ plateaus
-         in neocortical pyramidal cells: evidence for nonhomogeneous
-         distribution of HVA Ca2+ channels in dendrites." Journal of
-         Neuroscience 13.11 (1993): 4609-4621.
-
-  """
+  .. [7] Pinsky, Paul F., and John Rinzel. "Intrinsic and network
+         rhythmogenesis in a reduced Traub model for CA3 neurons."
+         Journal of computational neuroscience 1.1 (1994): 39-60.
+  .. [8] Traub, R. D., Wong, R. K., Miles, R., & Michelson, H. (1991).
+         A model of a CA3 hippocampal pyramidal neuron incorporating
+         voltage-clamp data on intrinsic conductances. Journal of
+         neurophysiology, 66(2), 635-650.
+  """
+
+  supported_modes = (bm.BatchingMode, bm.NonBatchingMode)
 
   def __init__(
       self,
       size: Shape,
       keep_size: bool = False,
-      T: Union[float, ArrayType] = 36.,
-      T_base_p: Union[float, ArrayType] = 2.3,
-      T_base_q: Union[float, ArrayType] = 2.3,
-      phi_p: Union[float, ArrayType, Initializer, Callable] = None,
-      phi_q: Union[float, ArrayType, Initializer, Callable] = None,
-      g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
-      V_sh: Union[float, ArrayType, Initializer, Callable] = 0.,
+      # maximum conductance
+      gNa: Union[float, ArrayType, Initializer, Callable] = 30.,
+      gK: Union[float, ArrayType, Initializer, Callable] = 15.,
+      gCa: Union[float, ArrayType, Initializer, Callable] = 10.,
+      gAHP: Union[float, ArrayType, Initializer, Callable] = 0.8,
+      gC: Union[float, ArrayType, Initializer, Callable] = 15.,
+      gL: Union[float, ArrayType, Initializer, Callable] = 0.1,
+      # reversal potential
+      ENa: Union[float, ArrayType, Initializer, Callable] = 60.,
+      EK: Union[float, ArrayType, Initializer, Callable] = -75.,
+      ECa: Union[float, ArrayType, Initializer, Callable] = 80.,
+      EL: Union[float, ArrayType, Initializer, Callable] = -60.,
+      # other parameters
+      gc: Union[float, ArrayType, Initializer, Callable] = 2.1,
+      V_th: Union[float, ArrayType, Initializer, Callable] = 20.,
+      Cm: Union[float, ArrayType, Initializer, Callable] = 3.0,
+      p: Union[float, ArrayType, Initializer, Callable] = 0.5,
+      A: Union[float, ArrayType, Initializer, Callable] = 1.,
+      # initializers
+      Vs_initializer: Union[Initializer, Callable, ArrayType] = OneInit(-64.6),
+      Vd_initializer: Union[Initializer, Callable, ArrayType] = OneInit(-64.5),
+      Ca_initializer: Union[Initializer, Callable, ArrayType] = OneInit(0.2),
+      # others
+      noise: Union[float, ArrayType, Initializer, Callable] = None,
       method: str = 'exp_auto',
       name: str = None,
       mode: bm.Mode = None,
   ):
-    phi_p = T_base_p ** ((T - 23.) / 10.) if phi_p is None else phi_p
-    phi_q = T_base_q ** ((T - 23.) / 10.) if phi_q is None else phi_q
-    super(ICaHT_Re1993, self).__init__(size,
-                                       keep_size=keep_size,
-                                       name=name,
-                                       method=method,
-                                       g_max=g_max,
-                                       phi_p=phi_p,
-                                       phi_q=phi_q,
-                                       mode=mode)
-    self.T = parameter(T, self.varshape, allow_none=False)
-    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
-    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
-    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
-
-  def f_p_alpha(self, V):
-    temp = -27 - V + self.V_sh
-    return 0.055 * temp / (bm.exp(temp / 3.8) - 1)
-
-  def f_p_beta(self, V):
-    return 0.94 * bm.exp((-75. - V + self.V_sh) / 17.)
-
-  def f_q_alpha(self, V):
-    return 0.000457 * bm.exp((-13. - V + self.V_sh) / 50.)
-
-  def f_q_beta(self, V):
-    return 0.0065 / (bm.exp((-15. - V + self.V_sh) / 28.) + 1.)
-
-
-class ICaL_IS2008(_ICa_p2q_ss):
-  r"""The L-type calcium channel model proposed by (Inoue & Strowbridge, 2008) [1]_.
+    # initialization
+    super(PinskyRinzelModel, self).__init__(size=size,
+                                            keep_size=keep_size,
+                                            name=name,
+                                            mode=mode)
 
-  The L-type calcium channel model is adopted from (Inoue, et, al., 2008) [1]_.
-  Its dynamics is given by:
+    # conductance parameters
+    self.gAHP = parameter(gAHP, self.varshape, allow_none=False)
+    self.gCa = parameter(gCa, self.varshape, allow_none=False)
+    self.gNa = parameter(gNa, self.varshape, allow_none=False)
+    self.gK = parameter(gK, self.varshape, allow_none=False)
+    self.gL = parameter(gL, self.varshape, allow_none=False)
+    self.gC = parameter(gC, self.varshape, allow_none=False)
+
+    # reversal potential parameters
+    self.ENa = parameter(ENa, self.varshape, allow_none=False)
+    self.ECa = parameter(ECa, self.varshape, allow_none=False)
+    self.EK = parameter(EK, self.varshape, allow_none=False)
+    self.EL = parameter(EL, self.varshape, allow_none=False)
+
+    # other neuronal parameters
+    self.V_th = parameter(V_th, self.varshape, allow_none=False)
+    self.Cm = parameter(Cm, self.varshape, allow_none=False)
+    self.gc = parameter(gc, self.varshape, allow_none=False)
+    self.p = parameter(p, self.varshape, allow_none=False)
+    self.A = parameter(A, self.varshape, allow_none=False)
+    self.noise = init_noise(noise, self.varshape, num_vars=8)
+
+    # initializers
+    check.is_initializer(Vs_initializer, 'Vs_initializer', allow_none=False)
+    check.is_initializer(Vd_initializer, 'Vd_initializer', allow_none=False)
+    check.is_initializer(Ca_initializer, 'Ca_initializer', allow_none=False)
+    self._Vs_initializer = Vs_initializer
+    self._Vd_initializer = Vd_initializer
+    self._Ca_initializer = Ca_initializer
 
-  .. math::
+    # variables
+    self.Vs = variable_(self._Vs_initializer, self.varshape, self.mode)
+    self.Vd = variable_(self._Vd_initializer, self.varshape, self.mode)
+    self.Ca = variable_(self._Ca_initializer, self.varshape, self.mode)
+    self.h = bm.Variable(self.inf_h(self.Vs), batch_axis=0 if isinstance(self.mode, bm.BatchingMode) else None)
+    self.n = bm.Variable(self.inf_n(self.Vs), batch_axis=0 if isinstance(self.mode, bm.BatchingMode) else None)
+    self.s = bm.Variable(self.inf_s(self.Vd), batch_axis=0 if isinstance(self.mode, bm.BatchingMode) else None)
+    self.c = bm.Variable(self.inf_c(self.Vd), batch_axis=0 if isinstance(self.mode, bm.BatchingMode) else None)
+    self.q = bm.Variable(self.inf_q(self.Ca), batch_axis=0 if isinstance(self.mode, bm.BatchingMode) else None)
+    self.Id = variable_(bm.zeros, self.varshape, self.mode)  # input to soma
+    self.Is = variable_(bm.zeros, self.varshape, self.mode)  # input to dendrite
+
+    # integral
+    if self.noise is None:
+      self.integral = odeint(method=method, f=self.derivative)
+    else:
+      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
+
+  def reset_state(self, batch_size=None):
+    self.Vd.value = variable_(self._Vd_initializer, self.varshape, batch_size)
+    self.Vs.value = variable_(self._Vs_initializer, self.varshape, batch_size)
+    self.Ca.value = variable_(self._Ca_initializer, self.varshape, batch_size)
+    batch_axis = 0 if isinstance(self.mode, bm.BatchingMode) else None
+    self.h.value = bm.Variable(self.inf_h(self.Vs), batch_axis=batch_axis)
+    self.n.value = bm.Variable(self.inf_n(self.Vs), batch_axis=batch_axis)
+    self.s.value = bm.Variable(self.inf_s(self.Vd), batch_axis=batch_axis)
+    self.c.value = bm.Variable(self.inf_c(self.Vd), batch_axis=batch_axis)
+    self.q.value = bm.Variable(self.inf_q(self.Ca), batch_axis=batch_axis)
+    self.Id.value = variable_(bm.zeros, self.varshape, batch_size)
+    self.Is.value = variable_(bm.zeros, self.varshape, batch_size)
+
+  def dCa(self, Ca, t, s, Vd):
+    ICa = self.gCa * s * s * (Vd - self.ECa)
+    return -0.13 * ICa - 0.075 * Ca
+
+  def dh(self, h, t, Vs):
+    return self.alpha_h(Vs) * (1 - h) - self.beta_h(Vs) * h
+
+  def dn(self, n, t, Vs):
+    return self.alpha_n(Vs) * (1 - n) - self.beta_n(Vs) * n
+
+  def ds(self, s, t, Vd):
+    return self.alpha_s(Vd) * (1 - s) - self.beta_s(Vd) * s
+
+  def dc(self, c, t, Vd):
+    return self.alpha_c(Vd) * (1 - c) - self.beta_c(Vd) * c
+
+  def dq(self, q, t, Ca):
+    return self.alpha_q(Ca) * (1 - q) - self.beta_q(Ca) * q
+
+  def dVs(self, Vs, t, h, n, Vd):
+    I_Na = (self.gNa * self.inf_m(Vs) ** 2 * h) * (Vs - self.ENa)
+    I_KDR = (self.gK * n) * (Vs - self.EK)
+    I_leak = self.gL * (Vs - self.EL)
+    I_gj = self.gc / self.p * (Vd - Vs)
+    dVdt = (- I_Na - I_KDR - I_leak + I_gj + self.Is / self.p) / self.Cm
+    return dVdt
+
+  def dVd(self, Vd, t, s, q, c, Ca, Vs):
+    I_leak = self.gL * (Vd - self.EL)
+    I_Ca = self.gCa * s * s * (Vd - self.ECa)
+    I_AHP = self.gAHP * q * (Vd - self.EK)
+    I_C = self.gC * bm.minimum(Ca / 250., 1.) * (Vd - self.EK)
+    p = 1 - self.p
+    I_gj = self.gc / p * (Vs - Vd)
+    dVdt = (- I_leak - I_Ca - I_AHP - I_C + I_gj + self.Id / p) / self.Cm
+    return dVdt
+
+  @property
+  def derivative(self):
+    return JointEq(self.dVs, self.dVd, self.dCa, self.dh, self.dn, self.ds, self.dc, self.dq)
+
+  def update(self, x=None):
+    assert x is None
+    t = share.load('t')
+    dt = share.load('dt')
+    Vs, Vd, Ca, h, n, s, c, q = self.integral(Vs=self.Vs.value,
+                                              Vd=self.Vd.value,
+                                              Ca=self.Ca.value,
+                                              h=self.h.value,
+                                              n=self.n.value,
+                                              s=self.s.value,
+                                              c=self.c.value,
+                                              q=self.q.value,
+                                              t=t,
+                                              dt=dt)
+    self.Vs.value = Vs
+    self.Vd.value = Vd
+    self.Ca.value = Ca
+    self.h.value = h
+    self.n.value = n
+    self.s.value = s
+    self.c.value = c
+    self.q.value = q
+
+  def clear_input(self):
+    self.Id.value = bm.zeros_like(self.Id)
+    self.Is.value = bm.zeros_like(self.Is)
+
+  def alpha_m(self, Vs):
+    return 0.32 * (13.1 - (Vs + 60.)) / (bm.exp((13.1 - (Vs + 60.)) / 4.) - 1.)
+
+  def beta_m(self, Vs):
+    return 0.28 * ((Vs + 60.) - 40.1) / (bm.exp(((Vs + 60.) - 40.1) / 5.) - 1.)
+
+  def inf_m(self, Vs):
+    alpha = self.alpha_m(Vs)
+    beta = self.beta_m(Vs)
+    return alpha / (alpha + beta)
+
+  def alpha_n(self, Vs):
+    return 0.016 * (35.1 - (Vs + 60.)) / (bm.exp((35.1 - (Vs + 60.)) / 5) - 1)
+
+  def beta_n(self, Vs):
+    return 0.25 * bm.exp(0.5 - 0.025 * (Vs + 60.))
+
+  def inf_n(self, Vs):
+    alpha = self.alpha_n(Vs)
+    beta = self.beta_n(Vs)
+    return alpha / (alpha + beta)
+
+  def alpha_h(self, Vs):
+    return 0.128 * bm.exp((17. - (Vs + 60.)) / 18.)
+
+  def beta_h(self, Vs):
+    return 4. / (1 + bm.exp((40. - (Vs + 60.)) / 5))
+
+  def inf_h(self, Vs):
+    alpha = self.alpha_h(Vs)
+    beta = self.beta_h(Vs)
+    return alpha / (alpha + beta)
+
+  def alpha_s(self, Vd):
+    return 1.6 / (1 + bm.exp(-0.072 * ((Vd + 60.) - 65.)))
+
+  def beta_s(self, Vd):
+    return 0.02 * ((Vd + 60.) - 51.1) / (bm.exp(((Vd + 60.) - 51.1) / 5.) - 1.)
+
+  def inf_s(self, Vd):
+    alpha = self.alpha_s(Vd)
+    beta = self.beta_s(Vd)
+    return alpha / (alpha + beta)
+
+  def alpha_c(self, Vd):
+    return bm.where((Vd + 60.) <= 50.,
+                    (bm.exp(((Vd + 60.) - 10.) / 11.) - bm.exp(((Vd + 60.) - 6.5) / 27.)) / 18.975,
+                    2. * bm.exp((6.5 - (Vd + 60.)) / 27.))
+
+  def beta_c(self, Vd):
+    alpha_c = (bm.exp(((Vd + 60.) - 10.) / 11.) - bm.exp(((Vd + 60.) - 6.5) / 27.)) / 18.975
+    return bm.where((Vd + 60.) <= 50., 2. * bm.exp((6.5 - (Vd + 60.)) / 27.) - alpha_c, 0.)
+
+  def inf_c(self, Vd):
+    alpha_c = self.alpha_c(Vd)
+    beta_c = self.beta_c(Vd)
+    return alpha_c / (alpha_c + beta_c)
+
+  def alpha_q(self, Ca):
+    return bm.minimum(2e-5 * Ca, 1e-2)
+
+  def beta_q(self, Ca):
+    return 1e-3
+
+  def inf_q(self, Ca):
+    alpha = self.alpha_q(Ca)
+    beta = self.beta_q(Ca)
+    return alpha / (alpha + beta)
+
+
+class WangBuzsakiModel(hh.WangBuzsakiHH):
+  r"""Wang-Buzsaki model [9]_, an implementation of a modified Hodgkin-Huxley model.
+
+  Each model is described by a single compartment and obeys the current balance equation:
+
+  .. math::
+
+      C_{m} \frac{d V}{d t}=-I_{\mathrm{Na}}-I_{\mathrm{K}}-I_{\mathrm{L}}-I_{\mathrm{syn}}+I_{\mathrm{app}}
+
+  where :math:`C_{m}=1 \mu \mathrm{F} / \mathrm{cm}^{2}` and :math:`I_{\mathrm{app}}` is the
+  injected current (in :math:`\mu \mathrm{A} / \mathrm{cm}^{2}` ). The leak current
+  :math:`I_{\mathrm{L}}=g_{\mathrm{L}}\left(V-E_{\mathrm{L}}\right)` has a conductance
+  :math:`g_{\mathrm{L}}=0.1 \mathrm{mS} / \mathrm{cm}^{2}`, so that the passive time constant
+  :math:`\tau_{0}=C_{m} / g_{\mathrm{L}}=10 \mathrm{msec} ; E_{\mathrm{L}}=-65 \mathrm{mV}`.
+
+  The spike-generating :math:`\mathrm{Na}^{+}` and :math:`\mathrm{K}^{+}` voltage-dependent ion
+  currents :math:`\left(I_{\mathrm{Na}}\right.` and :math:`I_{\mathrm{K}}` ) are of the
+  Hodgkin-Huxley type (Hodgkin and Huxley, 1952). The transient sodium current
+  :math:`I_{\mathrm{Na}}=g_{\mathrm{Na}} m_{\infty}^{3} h\left(V-E_{\mathrm{Na}}\right)`,
+  where the activation variable :math:`m` is assumed fast and substituted by its steady-state
+  function :math:`m_{\infty}=\alpha_{m} /\left(\alpha_{m}+\beta_{m}\right)` ;
+  :math:`\alpha_{m}(V)=-0.1(V+35) /(\exp (-0.1(V+35))-1), \beta_{m}(V)=4 \exp (-(V+60) / 18)`.
+  The inactivation variable :math:`h` obeys a first-order kinetics:
+
+  .. math::
+
+      \frac{d h}{d t}=\phi\left(\alpha_{h}(1-h)-\beta_{h} h\right)
+
+  where :math:`\alpha_{h}(V)=0.07 \exp (-(V+58) / 20)` and
+  :math:`\beta_{h}(V)=1 /(\exp (-0.1(V+28)) +1) \cdot g_{\mathrm{Na}}=35 \mathrm{mS} / \mathrm{cm}^{2}` ;
+  :math:`E_{\mathrm{Na}}=55 \mathrm{mV}, \phi=5 .`
+
+  The delayed rectifier :math:`I_{\mathrm{K}}=g_{\mathrm{K}} n^{4}\left(V-E_{\mathrm{K}}\right)`,
+  where the activation variable :math:`n` obeys the following equation:
+
+  .. math::
+
+     \frac{d n}{d t}=\phi\left(\alpha_{n}(1-n)-\beta_{n} n\right)
+
+  with :math:`\alpha_{n}(V)=-0.01(V+34) /(\exp (-0.1(V+34))-1)` and
+  :math:`\beta_{n}(V)=0.125\exp (-(V+44) / 80)` ; :math:`g_{\mathrm{K}}=9 \mathrm{mS} / \mathrm{cm}^{2}`, and
+  :math:`E_{\mathrm{K}}=-90 \mathrm{mV}`.
 
-      I_{CaL} &= g_{max} p^2 q(V-E_{Ca}) \\
-      {dp \over dt} &= {\phi_p \cdot (p_{\infty}-p)\over \tau_p} \\
-      & p_{\infty} = {1 \over 1+\exp [-(V+10-V_{sh}) / 4.]} \\
-      & \tau_{p} = 0.4+{0.7 \over \exp [(V+5-V_{sh}) / 15]+\exp [-(V+5-V_{sh}) / 15]} \\
-      {dq \over dt} &= {\phi_q \cdot (q_{\infty}-q) \over \tau_q} \\
-      & q_{\infty} = {1 \over 1+\exp [(V+25-V_{sh}) / 2]} \\
-      & \tau_q = 300 + {100 \over \exp [(V+40-V_{sh}) / 9.5]+\exp [-(V+40-V_{sh}) / 9.5]}
-
-  where :math:`phi_p = 3.55^{\frac{T-24}{10}}` and :math:`phi_q = 3^{\frac{T-24}{10}}`
-  are temperature-dependent factors (:math:`T` is the temperature in Celsius),
-  :math:`E_{Ca}` is the reversal potential of Calcium channel.
 
   Parameters
   ----------
-  T : float
-    The temperature.
-  T_base_p : float
-    The brainpy_object temperature factor of :math:`p` channel.
-  T_base_q : float
-    The brainpy_object temperature factor of :math:`q` channel.
-  g_max : float
-    The maximum conductance.
-  V_sh : float
-    The membrane potential shift.
+  size: sequence of int, int
+    The size of the neuron group.
+  ENa: float, ArrayType, Initializer, callable
+    The reversal potential of sodium. Default is 50 mV.
+  gNa: float, ArrayType, Initializer, callable
+    The maximum conductance of sodium channel. Default is 120 msiemens.
+  EK: float, ArrayType, Initializer, callable
+    The reversal potential of potassium. Default is -77 mV.
+  gK: float, ArrayType, Initializer, callable
+    The maximum conductance of potassium channel. Default is 36 msiemens.
+  EL: float, ArrayType, Initializer, callable
+    The reversal potential of learky channel. Default is -54.387 mV.
+  gL: float, ArrayType, Initializer, callable
+    The conductance of learky channel. Default is 0.03 msiemens.
+  V_th: float, ArrayType, Initializer, callable
+    The threshold of the membrane spike. Default is 20 mV.
+  C: float, ArrayType, Initializer, callable
+    The membrane capacitance. Default is 1 ufarad.
+  phi: float, ArrayType, Initializer, callable
+    The temperature regulator constant.
+  V_initializer: ArrayType, Initializer, callable
+    The initializer of membrane potential.
+  h_initializer: ArrayType, Initializer, callable
+    The initializer of h channel.
+  n_initializer: ArrayType, Initializer, callable
+    The initializer of n channel.
+  method: str
+    The numerical integration method.
+  name: str
+    The group name.
 
   References
   ----------
+  .. [9] Wang, X.J. and Buzsaki, G., (1996) Gamma oscillation by synaptic
+         inhibition in a hippocampal interneuronal network model. Journal of
+         neuroscience, 16(20), pp.6402-6413.
 
-  .. [1] Inoue, Tsuyoshi, and Ben W. Strowbridge. "Transient activity induces a long-lasting
-         increase in the excitability of olfactory bulb interneurons." Journal of
-         neurophysiology 99, no. 1 (2008): 187-199.
-
-  See Also
-  --------
-  ICa_p2q_form
   """
 
   def __init__(
       self,
-      size: Shape,
-      keep_size: bool = False,
-      T: Union[float, ArrayType, Initializer, Callable] = 36.,
-      T_base_p: Union[float, ArrayType, Initializer, Callable] = 3.55,
-      T_base_q: Union[float, ArrayType, Initializer, Callable] = 3.,
-      g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
-      V_sh: Union[float, ArrayType, Initializer, Callable] = 0.,
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-  ):
-    super(ICaL_IS2008, self).__init__(size,
-                                      keep_size=keep_size,
-                                      name=name,
-                                      method=method,
-                                      g_max=g_max,
-                                      phi_p=T_base_p ** ((T - 24) / 10),
-                                      phi_q=T_base_q ** ((T - 24) / 10),
-                                      mode=mode)
-
-    # parameters
-    self.T = parameter(T, self.varshape, allow_none=False)
-    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
-    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
-    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
-
-  def f_p_inf(self, V):
-    return 1. / (1 + bm.exp(-(V + 10. - self.V_sh) / 4.))
-
-  def f_p_tau(self, V):
-    return 0.4 + .7 / (bm.exp(-(V + 5. - self.V_sh) / 15.) +
-                       bm.exp((V + 5. - self.V_sh) / 15.))
-
-  def f_q_inf(self, V):
-    return 1. / (1. + bm.exp((V + 25. - self.V_sh) / 2.))
-
-  def f_q_tau(self, V):
-    return 300. + 100. / (bm.exp((V + 40 - self.V_sh) / 9.5) +
-                          bm.exp(-(V + 40 - self.V_sh) / 9.5))
+      *args,
+      input_var: bool = True,
+      noise: Union[float, ArrayType, Initializer, Callable] = None,
+      **kwargs,
+  ):
+    self.input_var = input_var
+    super().__init__(*args, **kwargs, init_var=False)
+    self.noise = init_noise(noise, self.varshape, num_vars=3)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
+    self.reset_state(self.mode)
+
+  def reset_state(self, batch_size=None):
+    super().reset_state(batch_size)
+    if self.input_var:
+      self.input = variable_(bm.zeros, self.varshape, batch_size)
+
+  def update(self, x=None):
+    if self.input_var:
+      if x is not None:
+        self.input += x
+      x = self.input.value
+    else:
+      x = 0. if x is None else x
+    return super().update(x)
+
+  def clear_input(self):
+    if self.input_var:
+      self.input.value = bm.zeros_like(self.input)
```

## Comparing `brainpy/_src/dyn/channels/IH.py` & `brainpy/_src/dyn/channels/hyperpolarization_activated.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 # -*- coding: utf-8 -*-
 
 """
 This module implements hyperpolarization-activated cation channels.
-
 """
 
-from typing import Union, Callable
+from typing import Union, Callable, Optional
 
 import brainpy.math as bm
+from brainpy._src.context import share
+from brainpy._src.dyn.ions.calcium import Calcium
+from brainpy._src.dyn.neurons.hh import HHTypedNeuron
 from brainpy._src.initialize import Initializer, parameter, variable
 from brainpy._src.integrators import odeint, JointEq
 from brainpy.types import Shape, ArrayType
-from .base import IhChannel, CalciumChannel, Calcium
+from .base import IonChannel
 
 __all__ = [
   'Ih_HM1992',
   'Ih_De1996',
 ]
 
 
-class Ih_HM1992(IhChannel):
+class Ih_HM1992(IonChannel):
   r"""The hyperpolarization-activated cation current model propsoed by (Huguenard & McCormick, 1992) [1]_.
 
   The hyperpolarization-activated cation current model is adopted from
   (Huguenard, et, al., 1992) [1]_. Its dynamics is given by:
 
   .. math::
 
@@ -49,29 +51,31 @@
   ----------
   .. [1] Huguenard, John R., and David A. McCormick. "Simulation of the currents
          involved in rhythmic oscillations in thalamic relay neurons." Journal
          of neurophysiology 68, no. 4 (1992): 1373-1383.
 
   """
 
+  master_type = HHTypedNeuron
+
   def __init__(
       self,
       size: Shape,
       keep_size: bool = False,
       g_max: Union[float, ArrayType, Initializer, Callable] = 10.,
       E: Union[float, ArrayType, Initializer, Callable] = 43.,
       phi: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
   ):
-    super(Ih_HM1992, self).__init__(size,
-                                    keep_size=keep_size,
-                                    name=name,
-                                    mode=mode)
+    super().__init__(size,
+                     keep_size=keep_size,
+                     name=name,
+                     mode=mode)
 
     # parameters
     self.phi = parameter(phi, self.varshape, allow_none=False)
     self.g_max = parameter(g_max, self.varshape, allow_none=False)
     self.E = parameter(E, self.varshape, allow_none=False)
 
     # variable
@@ -81,31 +85,31 @@
     self.integral = odeint(self.derivative, method=method)
 
   def derivative(self, p, t, V):
     return self.phi * (self.f_p_inf(V) - p) / self.f_p_tau(V)
 
   def reset_state(self, V, batch_size=None):
     self.p.value = self.f_p_inf(V)
-    if batch_size is not None:
+    if isinstance(batch_size, int):
       assert self.p.shape[0] == batch_size
 
-  def update(self, tdi, V):
-    self.p.value = self.integral(self.p.value, tdi['t'], V, tdi['dt'])
+  def update(self, V):
+    self.p.value = self.integral(self.p.value, share['t'], V, share['dt'])
 
   def current(self, V):
     return self.g_max * self.p * (self.E - V)
 
   def f_p_inf(self, V):
     return 1. / (1. + bm.exp((V + 75.) / 5.5))
 
   def f_p_tau(self, V):
     return 1. / (bm.exp(-0.086 * V - 14.59) + bm.exp(0.0701 * V - 1.87))
 
 
-class Ih_De1996(IhChannel, CalciumChannel):
+class Ih_De1996(IonChannel):
   r"""The hyperpolarization-activated cation current model propsoed by (Destexhe, et al., 1996) [1]_.
 
   The full kinetic schema was
 
   .. math::
 
      \begin{gathered}
@@ -167,23 +171,21 @@
       g_max: Union[float, ArrayType, Initializer, Callable] = 0.02,
       g_inc: Union[float, ArrayType, Initializer, Callable] = 2.,
       Ca_half: Union[float, ArrayType, Initializer, Callable] = 2e-3,
       T: Union[float, ArrayType] = 36.,
       T_base: Union[float, ArrayType] = 3.,
       phi: Union[float, ArrayType, Initializer, Callable] = None,
       method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
   ):
-    # IhChannel.__init__(self, size, name=name, keep_size=keep_size)
-    CalciumChannel.__init__(self,
-                            size,
-                            keep_size=keep_size,
-                            name=name,
-                            mode=mode)
+    super().__init__(size,
+                     keep_size=keep_size,
+                     name=name,
+                     mode=mode)
 
     # parameters
     self.T = parameter(T, self.varshape, allow_none=False)
     self.T_base = parameter(T_base, self.varshape, allow_none=False)
     if phi is None:
       self.phi = self.T_base ** ((self.T - 24.) / 10)
     else:
@@ -215,31 +217,31 @@
 
   def dOL(self, OL, t, O, P1):
     return self.k3 * P1 * O - self.k4 * OL
 
   def dP1(self, P1, t, C_Ca):
     return self.k1 * C_Ca ** 4 * (1 - P1) - self.k2 * P1
 
-  def update(self, tdi, V, C_Ca, E_Ca):
+  def update(self, V, C_Ca, E_Ca):
     self.O.value, self.OL.value, self.P1.value = self.integral(self.O.value, self.OL.value, self.P1.value,
-                                                               tdi['t'], V=V, C_Ca=C_Ca, dt=tdi['dt'])
+                                                               share['t'], V=V, C_Ca=C_Ca, dt=share['dt'])
 
   def current(self, V, C_Ca, E_Ca):
     return self.g_max * (self.O + self.g_inc * self.OL) * (self.E - V)
 
   def reset_state(self, V, C_Ca, E_Ca, batch_size=None):
     varshape = self.varshape if (batch_size is None) else ((batch_size,) + self.varshape)
     self.P1.value = bm.broadcast_to(self.k1 * C_Ca ** 4 / (self.k1 * C_Ca ** 4 + self.k2), varshape)
     inf = self.f_inf(V)
     tau = self.f_tau(V)
     alpha = inf / tau
     beta = (1 - inf) / tau
     self.O.value = alpha / (alpha + alpha * self.k3 * self.P1 / self.k4 + beta)
     self.OL.value = self.k3 * self.P1 * self.O / self.k4
-    if batch_size is not None:
+    if isinstance(batch_size, int):
       assert self.P1.shape[0] == batch_size
       assert self.O.shape[0] == batch_size
       assert self.OL.shape[0] == batch_size
 
   def f_inf(self, V):
     return 1 / (1 + bm.exp((V + 75 - self.V_sh) / 5.5))
```

## Comparing `brainpy/_src/dyn/channels/K.py` & `brainpy/_src/dyn/channels/potassium_compatible.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 # -*- coding: utf-8 -*-
 
 """
 This module implements voltage-dependent potassium channels.
 
 """
 
-from typing import Union, Callable, Optional
+from typing import Union, Callable, Optional, Sequence
 
 import brainpy.math as bm
+from brainpy._src.context import share
+from brainpy._src.dyn.channels.base import IonChannel
+from brainpy._src.dyn.neurons.hh import HHTypedNeuron
 from brainpy._src.initialize import Initializer, parameter, variable
 from brainpy._src.integrators import odeint, JointEq
-from brainpy.types import Shape, ArrayType
-from .base import PotassiumChannel
+from brainpy.types import ArrayType
 
 __all__ = [
   'IKDR_Ba2002',
   'IK_TM1991',
   'IK_HH1952',
-
   'IKA1_HM1992',
   'IKA2_HM1992',
-
   'IKK2A_HM1992',
   'IKK2B_HM1992',
-
   'IKNI_Ya1989',
+  'IKL',
 ]
 
 
-class _IK_p4_markov(PotassiumChannel):
+class _IK_p4_markov(IonChannel):
   r"""The delayed rectifier potassium channel of :math:`p^4`
   current which described with first-order Markov chain.
 
   This general potassium current model should have the form of
 
   .. math::
 
@@ -58,55 +58,56 @@
     The temperature-dependent factor.
   method: str
     The numerical integration method.
   name: str
     The object name.
 
   """
+  master_type = HHTypedNeuron
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 10.,
       phi: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
       name: str = None,
       mode: bm.Mode = None,
   ):
-    super(_IK_p4_markov, self).__init__(size,
-                                        keep_size=keep_size,
-                                        name=name,
-                                        mode=mode)
+    super().__init__(size,
+                     keep_size=keep_size,
+                     name=name,
+                     mode=mode)
 
     self.E = parameter(E, self.varshape, allow_none=False)
     self.g_max = parameter(g_max, self.varshape, allow_none=False)
     self.phi = parameter(phi, self.varshape, allow_none=False)
 
     # variables
     self.p = variable(bm.zeros, self.mode, self.varshape)
 
     # function
     self.integral = odeint(self.derivative, method=method)
 
   def derivative(self, p, t, V):
     return self.phi * (self.f_p_alpha(V) * (1. - p) - self.f_p_beta(V) * p)
 
-  def update(self, tdi, V):
-    self.p.value = self.integral(self.p.value, tdi['t'], V, tdi['dt'])
+  def update(self, V):
+    self.p.value = self.integral(self.p.value, share['t'], V, share['dt'])
 
   def current(self, V):
     return self.g_max * self.p ** 4 * (self.E - V)
 
   def reset_state(self, V, batch_size=None):
     alpha = self.f_p_alpha(V)
     beta = self.f_p_beta(V)
     self.p.value = alpha / (alpha + beta)
-    if batch_size is not None:
+    if isinstance(batch_size, int):
       assert self.p.shape[0] == batch_size
 
   def f_p_alpha(self, V):
     raise NotImplementedError
 
   def f_p_beta(self, V):
     raise NotImplementedError
@@ -157,15 +158,15 @@
   .. [1] Bazhenov, Maxim, et al. "Model of thalamocortical slow-wave sleep oscillations
          and transitions to activated states." Journal of neuroscience 22.19 (2002): 8691-8704.
 
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 10.,
       V_sh: Union[float, ArrayType, Initializer, Callable] = -50.,
       T_base: Union[float, ArrayType] = 3.,
       T: Union[float, ArrayType] = 36.,
       phi: Optional[Union[float, ArrayType, Initializer, Callable]] = None,
@@ -234,15 +235,15 @@
   See Also
   --------
   INa_TM1991
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 10.,
       phi: Union[float, ArrayType, Initializer, Callable] = 1.,
       V_sh: Union[int, float, ArrayType, Initializer, Callable] = -60.,
       method: str = 'exp_auto',
       name: str = None,
@@ -305,15 +306,15 @@
   See Also
   --------
   INa_HH1952
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 10.,
       phi: Union[float, ArrayType, Initializer, Callable] = 1.,
       V_sh: Union[int, float, ArrayType, Initializer, Callable] = -45.,
       method: str = 'exp_auto',
       name: str = None,
@@ -333,15 +334,15 @@
     temp = V - self.V_sh + 10
     return 0.01 * temp / (1 - bm.exp(-temp / 10))
 
   def f_p_beta(self, V):
     return 0.125 * bm.exp(-(V - self.V_sh + 20) / 80)
 
 
-class _IKA_p4q_ss(PotassiumChannel):
+class _IKA_p4q_ss(IonChannel):
   r"""The rapidly inactivating Potassium channel of :math:`p^4q`
   current which described with steady-state format.
 
   This model is developed according to the average behavior of
   rapidly inactivating Potassium channel in Thalamus relay neurons [2]_ [3]_.
 
   .. math::
@@ -374,31 +375,32 @@
   .. [2] Huguenard, John R., and David A. McCormick. "Simulation of the
          currents involved in rhythmic oscillations in thalamic relay
          neurons." Journal of neurophysiology 68.4 (1992): 1373-1383.
   .. [3] Huguenard, J. R., and D. A. Prince. "Slow inactivation of a
          TEA-sensitive K current in acutely isolated rat thalamic relay
          neurons." Journal of neurophysiology 66.4 (1991): 1316-1328.
   """
+  master_type = HHTypedNeuron
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 10.,
       phi_p: Union[float, ArrayType, Initializer, Callable] = 1.,
       phi_q: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
       name: str = None,
       mode: bm.Mode = None,
   ):
-    super(_IKA_p4q_ss, self).__init__(size,
-                                      keep_size=keep_size,
-                                      name=name,
-                                      mode=mode)
+    super().__init__(size,
+                     keep_size=keep_size,
+                     name=name,
+                     mode=mode)
 
     # parameters
     self.E = parameter(E, self.varshape, allow_none=False)
     self.g_max = parameter(g_max, self.varshape, allow_none=False)
     self.phi_p = parameter(phi_p, self.varshape, allow_none=False)
     self.phi_q = parameter(phi_q, self.varshape, allow_none=False)
 
@@ -411,25 +413,24 @@
 
   def dp(self, p, t, V):
     return self.phi_p * (self.f_p_inf(V) - p) / self.f_p_tau(V)
 
   def dq(self, q, t, V):
     return self.phi_q * (self.f_q_inf(V) - q) / self.f_q_tau(V)
 
-  def update(self, tdi, V):
-    t, dt = tdi['t'], tdi['dt']
-    self.p.value, self.q.value = self.integral(self.p.value, self.q.value, t, V, dt)
+  def update(self, V):
+    self.p.value, self.q.value = self.integral(self.p.value, self.q.value, share['t'], V, share['dt'])
 
   def current(self, V):
     return self.g_max * self.p ** 4 * self.q * (self.E - V)
 
   def reset_state(self, V, batch_size=None):
     self.p.value = self.f_p_inf(V)
     self.q.value = self.f_q_inf(V)
-    if batch_size is not None:
+    if isinstance(batch_size, int):
       assert self.p.shape[0] == batch_size
       assert self.q.shape[0] == batch_size
 
   def f_p_inf(self, V):
     raise NotImplementedError
 
   def f_p_tau(self, V):
@@ -492,15 +493,15 @@
   See Also
   --------
   IKA2_HM1992
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 30.,
       V_sh: Union[float, ArrayType, Initializer, Callable] = 0.,
       phi_p: Union[float, ArrayType, Initializer, Callable] = 1.,
       phi_q: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
@@ -587,15 +588,15 @@
   See Also
   --------
   IKA1_HM1992
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 20.,
       V_sh: Union[float, ArrayType, Initializer, Callable] = 0.,
       phi_p: Union[float, ArrayType, Initializer, Callable] = 1.,
       phi_q: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
@@ -628,15 +629,15 @@
   def f_q_tau(self, V):
     return bm.where(V < -63 + self.V_sh,
                     1. / (bm.exp((V - self.V_sh + 46.) / 5.) +
                           bm.exp(-(V - self.V_sh + 238.) / 37.5)),
                     19.)
 
 
-class _IKK2_pq_ss(PotassiumChannel):
+class _IKK2_pq_ss(IonChannel):
   r"""The slowly inactivating Potassium channel of :math:`pq`
   current which described with steady-state format.
 
   The dynamics of the model is given as [2]_ [3]_.
 
   .. math::
 
@@ -669,31 +670,32 @@
          currents involved in rhythmic oscillations in thalamic relay
          neurons." Journal of neurophysiology 68.4 (1992): 1373-1383.
   .. [3] Huguenard, J. R., and D. A. Prince. "Slow inactivation of a
          TEA-sensitive K current in acutely isolated rat thalamic relay
          neurons." Journal of neurophysiology 66.4 (1991): 1316-1328.
 
   """
+  master_type = HHTypedNeuron
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 10.,
       phi_p: Union[float, ArrayType, Initializer, Callable] = 1.,
       phi_q: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
       name: str = None,
       mode: bm.Mode = None,
   ):
-    super(_IKK2_pq_ss, self).__init__(size,
-                                      keep_size=keep_size,
-                                      name=name,
-                                      mode=mode)
+    super().__init__(size,
+                     keep_size=keep_size,
+                     name=name,
+                     mode=mode)
 
     # parameters
     self.E = parameter(E, self.varshape, allow_none=False)
     self.g_max = parameter(g_max, self.varshape, allow_none=False)
     self.phi_p = parameter(phi_p, self.varshape, allow_none=False)
     self.phi_q = parameter(phi_q, self.varshape, allow_none=False)
 
@@ -706,25 +708,24 @@
 
   def dp(self, p, t, V):
     return self.phi_p * (self.f_p_inf(V) - p) / self.f_p_tau(V)
 
   def dq(self, q, t, V):
     return self.phi_q * (self.f_q_inf(V) - q) / self.f_q_tau(V)
 
-  def update(self, tdi, V):
-    t, dt = tdi['t'], tdi['dt']
-    self.p.value, self.q.value = self.integral(self.p.value, self.q.value, t, V, dt)
+  def update(self, V):
+    self.p.value, self.q.value = self.integral(self.p.value, self.q.value, share['t'], V, share['dt'])
 
   def current(self, V):
     return self.g_max * self.p * self.q * (self.E - V)
 
   def reset_state(self, V, batch_size=None):
     self.p.value = self.f_p_inf(V)
     self.q.value = self.f_q_inf(V)
-    if batch_size is not None:
+    if isinstance(batch_size, int):
       assert self.p.shape[0] == batch_size
       assert self.q.shape[0] == batch_size
 
   def f_p_inf(self, V):
     raise NotImplementedError
 
   def f_p_tau(self, V):
@@ -783,15 +784,15 @@
          TEA-sensitive K current in acutely isolated rat thalamic relay
          neurons." Journal of neurophysiology 66.4 (1991): 1316-1328.
 
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 10.,
       V_sh: Union[float, ArrayType, Initializer, Callable] = 0.,
       phi_p: Union[float, ArrayType, Initializer, Callable] = 1.,
       phi_q: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
@@ -819,15 +820,15 @@
                  bm.exp(-(V - self.V_sh + 132) / 18.)) + 9.9
 
   def f_q_inf(self, V):
     return 1. / (1. + bm.exp((V - self.V_sh + 58.) / 10.6))
 
   def f_q_tau(self, V):
     return 1. / (bm.exp((V - self.V_sh - 1329.) / 200.) +
-                bm.exp(-(V - self.V_sh + 130.) / 7.1))
+                 bm.exp(-(V - self.V_sh + 130.) / 7.1))
 
 
 class IKK2B_HM1992(_IKK2_pq_ss):
   r"""The slowly inactivating Potassium channel (IK2b) model proposed by (Huguenard & McCormick, 1992) [2]_.
 
   The dynamics of the model is given as [2]_ [3]_.
 
@@ -874,15 +875,15 @@
          TEA-sensitive K current in acutely isolated rat thalamic relay
          neurons." Journal of neurophysiology 66.4 (1991): 1316-1328.
 
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 10.,
       V_sh: Union[float, ArrayType, Initializer, Callable] = 0.,
       phi_p: Union[float, ArrayType, Initializer, Callable] = 1.,
       phi_q: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
@@ -910,20 +911,20 @@
                  bm.exp(-(V - self.V_sh + 132) / 18.)) + 9.9
 
   def f_q_inf(self, V):
     return 1. / (1. + bm.exp((V - self.V_sh + 58.) / 10.6))
 
   def f_q_tau(self, V):
     return bm.where(V < -70 + self.V_sh,
-                   1. / (bm.exp((V - self.V_sh - 1329.) / 200.) +
-                         bm.exp(-(V - self.V_sh + 130.) / 7.1)),
-                   8.9)
+                    1. / (bm.exp((V - self.V_sh - 1329.) / 200.) +
+                          bm.exp(-(V - self.V_sh + 130.) / 7.1)),
+                    8.9)
 
 
-class IKNI_Ya1989(PotassiumChannel):
+class IKNI_Ya1989(IonChannel):
   r"""A slow non-inactivating K+ current described by Yamada et al. (1989) [1]_.
 
   This slow potassium current can effectively account for spike-frequency adaptation.
 
   .. math::
 
     \begin{aligned}
@@ -956,18 +957,19 @@
     The :math:`tau_{\max}` parameter.
 
   References
   ----------
   .. [1] Yamada, Walter M. "Multiple channels and calcium dynamics." Methods in neuronal modeling (1989): 97-133.
 
   """
+  master_type = HHTypedNeuron
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[float, ArrayType, Initializer, Callable] = -90.,
       g_max: Union[float, ArrayType, Initializer, Callable] = 0.004,
       phi_p: Union[float, ArrayType, Initializer, Callable] = 1.,
       phi_q: Union[float, ArrayType, Initializer, Callable] = 1.,
       tau_max: Union[float, ArrayType, Initializer, Callable] = 4e3,
       V_sh: Union[float, ArrayType, Initializer, Callable] = 0.,
@@ -993,25 +995,67 @@
 
     # function
     self.integral = odeint(self.dp, method=method)
 
   def dp(self, p, t, V):
     return self.phi_p * (self.f_p_inf(V) - p) / self.f_p_tau(V)
 
-  def update(self, tdi, V):
-    t, dt = tdi['t'], tdi['dt']
-    self.p.value = self.integral(self.p.value, t, V, dt)
+  def update(self, V):
+    self.p.value = self.integral(self.p.value, share['t'], V, share['dt'])
 
   def current(self, V):
     return self.g_max * self.p * (self.E - V)
 
   def reset_state(self, V, batch_size=None):
     self.p.value = self.f_p_inf(V)
-    if batch_size is not None:
+    if isinstance(batch_size, int):
       assert self.p.shape[0] == batch_size
 
   def f_p_inf(self, V):
     return 1. / (1. + bm.exp(-(V - self.V_sh + 35.) / 10.))
 
   def f_p_tau(self, V):
     temp = V - self.V_sh + 35.
     return self.tau_max / (3.3 * bm.exp(temp / 20.) + bm.exp(-temp / 20.))
+
+
+class IKL(IonChannel):
+  """The potassium leak channel current.
+
+  Parameters
+  ----------
+  g_max : float
+    The potassium leakage conductance which is modulated by both
+    acetylcholine and norepinephrine.
+  E : float
+    The reversal potential.
+  """
+
+  master_type = HHTypedNeuron
+
+  def __init__(
+      self,
+      size: Union[int, Sequence[int]],
+      keep_size: bool = False,
+      g_max: Union[int, float, ArrayType, Initializer, Callable] = 0.005,
+      E: Union[int, float, ArrayType, Initializer, Callable] = -90.,
+      method: str = None,
+      name: str = None,
+      mode: bm.Mode = None,
+  ):
+    super().__init__(size,
+                     keep_size=keep_size,
+                     name=name,
+                     mode=mode)
+
+    self.E = parameter(E, self.varshape, allow_none=False)
+    self.g_max = parameter(g_max, self.varshape, allow_none=False)
+    self.method = method
+
+  def reset_state(self, V, batch_size=None):
+    pass
+
+  def update(self, V):
+    pass
+
+  def current(self, V):
+    return self.g_max * (self.E - V)
```

## Comparing `brainpy/_src/dyn/channels/KCa.py` & `brainpy/_src/dyn/channels/potassium_calcium_compatible.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,25 +5,27 @@
 This module implements calcium-dependent potassium channels.
 
 """
 
 from typing import Union, Callable
 
 import brainpy.math as bm
+from brainpy._src.context import share
+from brainpy._src.dyn.ions.calcium import Calcium
 from brainpy._src.initialize import Initializer, parameter, variable
 from brainpy._src.integrators.ode.generic import odeint
 from brainpy.types import Shape, ArrayType
-from .base import Calcium, CalciumChannel, PotassiumChannel
+from .base import IonChannel
 
 __all__ = [
   'IAHP_De1994',
 ]
 
 
-class IAHP_De1994(PotassiumChannel, CalciumChannel):
+class IAHP_De1994(IonChannel):
   r"""The calcium-dependent potassium current model proposed by (Destexhe, et al., 1994) [1]_.
 
   Both in vivo (Contreras et al. 1993; Mulle et al. 1986) and in
   vitro recordings (Avanzini et al. 1989) show the presence of a
   marked after-hyper-polarization (AHP) after each burst of the RE
   cell. This slow AHP is mediated by a slow :math:`Ca^{2+}`-dependent K+
   current (Bal and McCormick 1993). (Destexhe, et al., 1994) adopted a
@@ -80,19 +82,18 @@
       alpha: Union[float, ArrayType, Initializer, Callable] = 48.,
       beta: Union[float, ArrayType, Initializer, Callable] = 0.09,
       phi: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
       name: str = None,
       mode: bm.Mode = None,
   ):
-    CalciumChannel.__init__(self,
-                            size=size,
-                            keep_size=keep_size,
-                            name=name,
-                            mode=mode)
+    super().__init__(size=size,
+                     keep_size=keep_size,
+                     name=name,
+                     mode=mode)
 
     # parameters
     self.E = parameter(E, self.varshape, allow_none=False)
     self.g_max = parameter(g_max, self.varshape, allow_none=False)
     self.n = parameter(n, self.varshape, allow_none=False)
     self.alpha = parameter(alpha, self.varshape, allow_none=False)
     self.beta = parameter(beta, self.varshape, allow_none=False)
@@ -101,26 +102,34 @@
     # variables
     self.p = variable(bm.zeros, self.mode, self.varshape)
 
     # function
     self.integral = odeint(self.dp, method=method)
 
   def dp(self, p, t, C_Ca):
-    C2 = self.alpha * bm.power(bm.as_jax(C_Ca), self.n)
+    C2 = self.alpha * bm.power(C_Ca, self.n)
     C3 = C2 + self.beta
     return self.phi * (C2 / C3 - p) * C3
 
-  def update(self, tdi, V, C_Ca, E_Ca):
-    t, dt = tdi['t'], tdi['dt']
-    self.p.value = self.integral(self.p.value, t, C_Ca=C_Ca, dt=dt)
+  def update(self, V, C_Ca, E_Ca):
+    self.p.value = self.integral(self.p.value, share['t'], C_Ca=C_Ca, dt=share['dt'])
 
   def current(self, V, C_Ca, E_Ca):
     return self.g_max * self.p * self.p * (self.E - V)
 
   def reset_state(self, V, C_Ca, E_Ca, batch_size=None):
     C2 = self.alpha * bm.power(C_Ca, self.n)
     C3 = C2 + self.beta
-    if batch_size is None:
-      self.p.value = bm.broadcast_to(C2 / C3, self.varshape)
+    self.p[:] = C2 / C3
+    if isinstance(batch_size, int):
+      batch_size = batch_size
+      size = (batch_size,) + self.varshape
+    elif isinstance(batch_size, bm.Mode):
+      if isinstance(batch_size, bm.BatchingMode):
+        size = (batch_size.batch_size,) + self.varshape
+      else:
+        batch_size = None
+        size = self.varshape
     else:
-      self.p.value = bm.broadcast_to(C2 / C3, (batch_size,) + self.varshape)
-      assert self.p.shape[0] == batch_size
+      size = self.varshape
+    self.p.value = bm.broadcast_to(C2 / C3, size)
+
```

## Comparing `brainpy/_src/dyn/channels/Na.py` & `brainpy/_src/dyn/channels/sodium_compatible.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,30 +1,32 @@
 # -*- coding: utf-8 -*-
 
 """
 This module implements voltage-dependent sodium channels.
 
 """
 
-from typing import Union, Callable
+from typing import Union, Callable, Sequence
 
 import brainpy.math as bm
+from brainpy._src.context import share
+from brainpy._src.dyn.neurons.hh import HHTypedNeuron
 from brainpy._src.initialize import Initializer, parameter, variable
 from brainpy._src.integrators import odeint, JointEq
-from brainpy.types import ArrayType, Shape
-from .base import SodiumChannel
+from brainpy.types import ArrayType
+from .base import IonChannel
 
 __all__ = [
   'INa_Ba2002',
   'INa_TM1991',
   'INa_HH1952',
 ]
 
 
-class _INa_p3q_markov(SodiumChannel):
+class _INa_p3q_markov(IonChannel):
   r"""The sodium current model of :math:`p^3q` current which described with first-order Markov chain.
 
   The general model can be used to model the dynamics with:
 
   .. math::
 
     \begin{aligned}
@@ -45,27 +47,28 @@
     The temperature-dependent factor.
   method: str
     The numerical method
   name: str
     The name of the object.
 
   """
+  master_type = HHTypedNeuron
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
-      E: Union[int, float, ArrayType, Initializer, Callable] = 50.,
+      E: Union[int, float, ArrayType, Initializer, Callable] = None,
       g_max: Union[int, float, ArrayType, Initializer, Callable] = 90.,
       phi: Union[int, float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
       name: str = None,
       mode: bm.Mode = None,
   ):
-    super(_INa_p3q_markov, self).__init__(size=size,
+    super().__init__(size=size,
                                           keep_size=keep_size,
                                           name=name,
                                           mode=mode)
 
     # parameters
     self.E = parameter(E, self.varshape, allow_none=False)
     self.phi = parameter(phi, self.varshape, allow_none=False)
@@ -81,27 +84,26 @@
   def reset_state(self, V, batch_size=None):
     alpha = self.f_p_alpha(V)
     beta = self.f_p_beta(V)
     self.p.value = alpha / (alpha + beta)
     alpha = self.f_q_alpha(V)
     beta = self.f_q_beta(V)
     self.q.value = alpha / (alpha + beta)
-    if batch_size is not None:
+    if isinstance(batch_size, int):
       assert self.p.shape[0] == batch_size
       assert self.q.shape[0] == batch_size
 
   def dp(self, p, t, V):
     return self.phi * (self.f_p_alpha(V) * (1. - p) - self.f_p_beta(V) * p)
 
   def dq(self, q, t, V):
     return self.phi * (self.f_q_alpha(V) * (1. - q) - self.f_q_beta(V) * q)
 
-  def update(self, tdi, V):
-    t, dt = tdi['t'], tdi['dt']
-    p, q = self.integral(self.p, self.q, t, V, dt)
+  def update(self, V):
+    p, q = self.integral(self.p, self.q, share['t'], V, share['dt'])
     self.p.value, self.q.value = p, q
 
   def current(self, V):
     return self.g_max * self.p ** 3 * self.q * (self.E - V)
 
   def f_p_alpha(self, V):
     raise NotImplementedError
@@ -157,25 +159,25 @@
   See Also
   --------
   INa_TM1991
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       T: Union[int, float, ArrayType] = 36.,
       E: Union[int, float, ArrayType, Initializer, Callable] = 50.,
       g_max: Union[int, float, ArrayType, Initializer, Callable] = 90.,
       V_sh: Union[int, float, ArrayType, Initializer, Callable] = -50.,
       method: str = 'exp_auto',
       name: str = None,
       mode: bm.Mode = None,
   ):
-    super(INa_Ba2002, self).__init__(size,
+    super().__init__(size,
                                      keep_size=keep_size,
                                      name=name,
                                      method=method,
                                      phi=3 ** ((T - 36) / 10),
                                      g_max=g_max,
                                      E=E,
                                      mode=mode)
@@ -244,25 +246,25 @@
   See Also
   --------
   INa_Ba2002
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[int, float, ArrayType, Initializer, Callable] = 50.,
       g_max: Union[int, float, ArrayType, Initializer, Callable] = 120.,
       phi: Union[int, float, ArrayType, Initializer, Callable] = 1.,
       V_sh: Union[int, float, ArrayType, Initializer, Callable] = -63.,
       method: str = 'exp_auto',
       name: str = None,
       mode: bm.Mode = None,
   ):
-    super(INa_TM1991, self).__init__(size,
+    super().__init__(size,
                                      keep_size=keep_size,
                                      name=name,
                                      method=method,
                                      E=E,
                                      phi=phi,
                                      g_max=g_max,
                                      mode=mode)
@@ -331,25 +333,25 @@
   See Also
   --------
   IK_HH1952
   """
 
   def __init__(
       self,
-      size: Shape,
+      size: Union[int, Sequence[int]],
       keep_size: bool = False,
       E: Union[int, float, ArrayType, Initializer, Callable] = 50.,
       g_max: Union[int, float, ArrayType, Initializer, Callable] = 120.,
       phi: Union[int, float, ArrayType, Initializer, Callable] = 1.,
       V_sh: Union[int, float, ArrayType, Initializer, Callable] = -45.,
       method: str = 'exp_auto',
       name: str = None,
       mode: bm.Mode = None,
   ):
-    super(INa_HH1952, self).__init__(size,
+    super().__init__(size,
                                      keep_size=keep_size,
                                      name=name,
                                      method=method,
                                      E=E,
                                      phi=phi,
                                      g_max=g_max,
                                      mode=mode)
```

## Comparing `brainpy/_src/dyn/neurons/input.py` & `brainpy/_src/dyn/others/input.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,88 +1,95 @@
 # -*- coding: utf-8 -*-
+import warnings
+from functools import partial
+from typing import Union, Sequence, Any, Optional, Callable
 
-from typing import Union, Sequence, Any
-
+import jax
 import jax.numpy as jnp
+
+from brainpy import math as bm
 from brainpy._src.context import share
-import brainpy.math as bm
-from brainpy._src.initialize import Initializer, parameter, variable_
-from brainpy.types import Shape, ArrayType
+from brainpy._src.dyn.utils import get_spk_type
 from brainpy._src.dyn.base import NeuDyn
+from brainpy._src.initialize import parameter, variable_
+from brainpy._src.mixin import ReturnInfo
+from brainpy.types import Shape, ArrayType
 
 __all__ = [
   'InputGroup',
   'OutputGroup',
   'SpikeTimeGroup',
   'PoissonGroup',
 ]
 
 
 class InputGroup(NeuDyn):
   """Input neuron group for place holder.
 
-  Parameters
-  ----------
-  size: int, tuple of int
-  keep_size: bool
-  mode: Mode
-  name: str
+  Args:
+    size: int, tuple of int
+    keep_size: bool
+    mode: Mode
+    name: str
   """
 
   def __init__(
       self,
       size: Union[int, Sequence[int]],
       sharding: Any = None,
       keep_size: bool = False,
-      mode: bm.Mode = None,
-      name: str = None,
+      mode: Optional[bm.Mode] = None,
+      name: Optional[str] = None,
   ):
-    super(InputGroup, self).__init__(name=name,
-                                     sharding=sharding,
-                                     size=size,
-                                     keep_size=keep_size,
-                                     mode=mode)
-    self.spike = None
+    super().__init__(name=name,
+                     sharding=sharding,
+                     size=size,
+                     keep_size=keep_size,
+                     mode=mode)
 
   def update(self, x):
     return x
 
+  def return_info(self):
+    return ReturnInfo(self.varshape, self.sharding, self.mode, bm.zeros)
+
   def reset_state(self, batch_size=None):
     pass
 
 
 class OutputGroup(NeuDyn):
   """Output neuron group for place holder.
 
-  Parameters
-  ----------
-  size: int, tuple of int
-  keep_size: bool
-  mode: Mode
-  name: str
+  Args:
+    size: int, tuple of int
+    keep_size: bool
+    mode: Mode
+    name: str
   """
 
   def __init__(
       self,
       size: Union[int, Sequence[int]],
       sharding: Any = None,
       keep_size: bool = False,
-      mode: bm.Mode = None,
-      name: str = None,
+      mode: Optional[bm.Mode] = None,
+      name: Optional[str] = None,
   ):
-    super(OutputGroup, self).__init__(name=name,
-                                      sharding=sharding,
-                                      size=size,
-                                      keep_size=keep_size,
-                                      mode=mode)
-    self.spike = None
+    super().__init__(name=name,
+                     sharding=sharding,
+                     size=size,
+                     keep_size=keep_size,
+                     mode=mode)
 
   def update(self, x):
     return x
 
+  def return_info(self):
+    return ReturnInfo(self.varshape, self.sharding, self.mode, bm.zeros)
+
   def reset_state(self, batch_size=None):
     pass
 
 
 class SpikeTimeGroup(NeuDyn):
   """The input neuron group characterized by spikes emitting at given times.
 
@@ -112,60 +119,70 @@
   """
 
   def __init__(
       self,
       size: Union[int, Sequence[int]],
       indices: Union[Sequence, ArrayType],
       times: Union[Sequence, ArrayType],
-      name: str = None,
-      sharding: Any = None,
+      spk_type: Optional[type] = None,
+      name: Optional[str] = None,
+      sharding: Optional[Sequence[str]] = None,
       keep_size: bool = False,
-      mode: bm.Mode = None,
+      mode: Optional[bm.Mode] = None,
       need_sort: bool = True,
   ):
-    super(SpikeTimeGroup, self).__init__(size=size,
-                                         sharding=sharding,
-                                         name=name,
-                                         keep_size=keep_size,
-                                         mode=mode)
+    super().__init__(size=size,
+                     sharding=sharding,
+                     name=name,
+                     keep_size=keep_size,
+                     mode=mode)
 
     # parameters
     if keep_size:
       raise NotImplementedError(f'Do not support keep_size=True in {self.__class__.__name__}')
     if len(indices) != len(times):
       raise ValueError(f'The length of "indices" and "times" must be the same. '
                        f'However, we got {len(indices)} != {len(times)}.')
     self.num_times = len(times)
+    self.spk_type = get_spk_type(spk_type, self.mode)
 
     # data about times and indices
     self.times = bm.asarray(times)
     self.indices = bm.asarray(indices, dtype=bm.int_)
     if need_sort:
       sort_idx = bm.argsort(self.times)
       self.indices.value = self.indices[sort_idx]
       self.times.value = self.times[sort_idx]
 
     # variables
     self.reset_state(self.mode)
 
   def reset_state(self, batch_size=None):
     self.i = bm.Variable(bm.asarray(0))
-    self.spike = variable_(lambda s: jnp.zeros(s, dtype=bool), self.varshape, batch_size)
+    self.spike = variable_(partial(jnp.zeros, dtype=self.spk_type),
+                           self.varshape,
+                           batch_size,
+                           axis_names=self.sharding,
+                           batch_axis_name=bm.sharding.BATCH_AXIS)
 
   def update(self):
+    # self.spike.value = bm.sharding.partition(bm.zeros_like(self.spike), self.spike.sharding)
     self.spike.value = bm.zeros_like(self.spike)
-    bm.while_loop(self._body_fun, self._cond_fun, share.load('t'))
+    bm.while_loop(self._body_fun, self._cond_fun, ())
     return self.spike.value
 
+  def return_info(self):
+    return self.spike
+
   # functions
-  def _cond_fun(self, t):
+  def _cond_fun(self):
     i = self.i.value
-    return bm.logical_and(i < self.num_times, t >= self.times[i])
+    return bm.logical_and(i < self.num_times, share['t'] >= self.times[i])
 
-  def _body_fun(self, t):
+  def _body_fun(self):
     i = self.i.value
     if isinstance(self.mode, bm.BatchingMode):
       self.spike[:, self.indices[i]] = True
     else:
       self.spike[self.indices[i]] = True
     self.i += 1
 
@@ -173,38 +190,47 @@
 class PoissonGroup(NeuDyn):
   """Poisson Neuron Group.
   """
 
   def __init__(
       self,
       size: Shape,
-      freqs: Union[int, float, jnp.ndarray, bm.Array, Initializer],
-      seed: int = None,
-      name: str = None,
-      sharding: Any = None,
+      freqs: Union[int, float, jax.Array, bm.Array, Callable],
       keep_size: bool = False,
-      mode: bm.Mode = None,
+      sharding: Optional[Sequence[str]] = None,
+      spk_type: Optional[type] = None,
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
+      seed=None,
   ):
-    super(PoissonGroup, self).__init__(size=size,
-                                       sharding=sharding,
-                                       name=name,
-                                       keep_size=keep_size,
-                                       mode=mode)
+    super().__init__(size=size,
+                     sharding=sharding,
+                     name=name,
+                     keep_size=keep_size,
+                     mode=mode)
+
+    if seed is not None:
+      warnings.warn('')
 
     # parameters
-    self.keep_size = keep_size
-    self.seed = seed
     self.freqs = parameter(freqs, self.num, allow_none=False)
+    self.spk_type = get_spk_type(spk_type, self.mode)
 
     # variables
     self.reset_state(self.mode)
 
   def update(self):
-    spikes = bm.random.rand_like(self.spike) <= (self.freqs * share.dt / 1000.)
+    spikes = bm.random.rand_like(self.spike) <= (self.freqs * share['dt'] / 1000.)
+    spikes = bm.asarray(spikes, dtype=self.spk_type)
+    # spikes = bm.sharding.partition(spikes, self.spike.sharding)
     self.spike.value = spikes
     return spikes
 
-  def reset_state(self, batch_size=None):
-    self.spike = variable_(lambda s: jnp.zeros(s, dtype=bool), self.varshape, batch_size)
-
-
+  def return_info(self):
+    return self.spike
 
+  def reset_state(self, batch_size=None):
+    self.spike = variable_(partial(jnp.zeros, dtype=self.spk_type),
+                           self.varshape,
+                           batch_size,
+                           axis_names=self.sharding,
+                           batch_axis_name=bm.sharding.BATCH_AXIS)
```

## Comparing `brainpy/_src/dyn/synapses/dynamics.py` & `brainpy/_src/dyn/channels/calcium.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,864 +1,798 @@
-from typing import Union, Sequence, Callable, Optional
+# -*- coding: utf-8 -*-
 
-from brainpy import math as bm
+"""
+This module implements voltage-dependent calcium channels.
+
+"""
+
+from typing import Union, Callable, Optional
+
+import brainpy.math as bm
 from brainpy._src.context import share
-from brainpy._src.dyn._docs import pneu_doc
-from brainpy._src.dyn.base import SynDyn
+from brainpy._src.dyn.ions.calcium import Calcium, CalciumDyna
+from brainpy._src.initialize import Initializer, parameter, variable
 from brainpy._src.integrators.joint_eq import JointEq
 from brainpy._src.integrators.ode.generic import odeint
-from brainpy._src.mixin import AlignPost, ReturnInfo
-from brainpy.types import ArrayType
+from brainpy.types import Shape, ArrayType
+from .base import IonChannel
 
 __all__ = [
-  'Expon',
-  'DualExpon',
-  'Alpha',
-  'NMDA',
-  'STD',
-  'STP',
-  'AMPA',
-  'GABAa',
-  'BioNMDA',
+  'CalciumChannel',
+
+  'ICaN_IS2008',
+  'ICaT_HM1992',
+  'ICaT_HP1992',
+  'ICaHT_HM1992',
+  'ICaL_IS2008',
 ]
 
 
+class CalciumChannel(IonChannel):
+  """Base class for Calcium ion channels."""
 
-class Expon(SynDyn, AlignPost):
-  r"""Exponential decay synapse model.
+  master_type = Calcium
+  '''The type of the master object.'''
 
-  **Model Descriptions**
+  def update(self, V, C, E):
+    raise NotImplementedError
 
-  The single exponential decay synapse model assumes the release of neurotransmitter,
-  its diffusion across the cleft, the receptor binding, and channel opening all happen
-  very quickly, so that the channels instantaneously jump from the closed to the open state.
-  Therefore, its expression is given by
+  def current(self, V, C, E):
+    raise NotImplementedError
 
-  .. math::
+  def reset(self, V, C, E, batch_size: int = None):
+    self.reset_state(V, C, E, batch_size)
 
-      g_{\mathrm{syn}}(t)=g_{\mathrm{max}} e^{-\left(t-t_{0}\right) / \tau}
+  def reset_state(self, V, C, E, batch_size: int = None):
+    raise NotImplementedError('Must be implemented by the subclass.')
 
-  where :math:`\tau_{delay}` is the time constant of the synaptic state decay,
-  :math:`t_0` is the time of the pre-synaptic spike,
-  :math:`g_{\mathrm{max}}` is the maximal conductance.
 
-  Accordingly, the differential form of the exponential synapse is given by
+class _ICa_p2q_ss(CalciumChannel):
+  r"""The calcium current model of :math:`p^2q` current which described with steady-state format.
+
+  The dynamics of this generalized calcium current model is given by:
 
   .. math::
 
-      \begin{aligned}
-       & \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}).
-       \end{aligned}
+      I_{CaT} &= g_{max} p^2 q(V-E_{Ca}) \\
+      {dp \over dt} &= {\phi_p \cdot (p_{\infty}-p)\over \tau_p} \\
+      {dq \over dt} &= {\phi_q \cdot (q_{\infty}-q) \over \tau_q} \\
+
+  where :math:`\phi_p` and :math:`\phi_q` are temperature-dependent factors,
+  :math:`E_{Ca}` is the reversal potential of Calcium channel.
+
+  Parameters
+  ----------
+  size: int, tuple of int
+    The size of the simulation target.
+  keep_size: bool
+    Keep size or flatten the size?
+  method: str
+    The numerical method
+  name: str
+    The name of the object.
+  g_max : float, ArrayType, Callable, Initializer
+    The maximum conductance.
+  phi_p : float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`p`.
+  phi_q : float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`q`.
 
-  .. [1] Sterratt, David, Bruce Graham, Andrew Gillies, and David Willshaw.
-          "The Synapse." Principles of Computational Modelling in Neuroscience.
-          Cambridge: Cambridge UP, 2011. 172-95. Print.
-
-  Args:
-    tau: float, ArrayType, Callable. The time constant of decay. [ms]
-    %s
   """
 
   def __init__(
       self,
-      size: Union[int, Sequence[int]],
+      size: Shape,
       keep_size: bool = False,
-      sharding: Optional[Sequence[str]] = None,
+      phi_p: Union[float, ArrayType, Initializer, Callable] = 3.,
+      phi_q: Union[float, ArrayType, Initializer, Callable] = 3.,
+      g_max: Union[float, ArrayType, Initializer, Callable] = 2.,
       method: str = 'exp_auto',
-      name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
-
-      # synapse parameters
-      tau: Union[float, ArrayType, Callable] = 8.0,
+      name: Optional[str] = None
   ):
-    super().__init__(name=name,
-                     mode=mode,
-                     size=size,
+    super().__init__(size,
                      keep_size=keep_size,
-                     sharding=sharding)
+                     name=name,
+                     mode=mode, )
 
     # parameters
-    self.tau = self.init_param(tau)
+    self.phi_p = parameter(phi_p, self.varshape, allow_none=False)
+    self.phi_q = parameter(phi_q, self.varshape, allow_none=False)
+    self.g_max = parameter(g_max, self.varshape, allow_none=False)
+
+    # variables
+    self.p = variable(bm.zeros, self.mode, self.varshape)
+    self.q = variable(bm.zeros, self.mode, self.varshape)
 
-    # function
-    self.integral = odeint(self.derivative, method=method)
+    # functions
+    self.integral = odeint(JointEq([self.dp, self.dq]), method=method)
 
-    self.reset_state(self.mode)
+  def dp(self, p, t, V):
+    return self.phi_p * (self.f_p_inf(V) - p) / self.f_p_tau(V)
 
-  def derivative(self, g, t):
-    return -g / self.tau
+  def dq(self, q, t, V):
+    return self.phi_q * (self.f_q_inf(V) - q) / self.f_q_tau(V)
 
-  def reset_state(self, batch_size=None):
-    self.g = self.init_variable(bm.zeros, batch_size)
+  def update(self, V, C, E):
+    self.p.value, self.q.value = self.integral(self.p, self.q, share['t'], V, share['dt'])
 
-  def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
-    self.g.value = self.integral(self.g.value, t, dt)
-    if x is not None:
-      self.g.value += x
-    return self.g.value
+  def current(self, V, C, E):
+    return self.g_max * self.p * self.p * self.q * (E - V)
 
-  def add_current(self, x):
-    self.g.value += x
+  def reset_state(self, V, C, E, batch_size=None):
+    self.p.value = self.f_p_inf(V)
+    self.q.value = self.f_q_inf(V)
+    if isinstance(batch_size, int):
+      assert self.p.shape[0] == batch_size
+      assert self.q.shape[0] == batch_size
 
-  def return_for_delay(self):
-    return self.g
+  def f_p_inf(self, V):
+    raise NotImplementedError
 
+  def f_p_tau(self, V):
+    raise NotImplementedError
 
-Expon.__doc__ = Expon.__doc__ % (pneu_doc,)
+  def f_q_inf(self, V):
+    raise NotImplementedError
 
+  def f_q_tau(self, V):
+    raise NotImplementedError
 
-class DualExpon(SynDyn):
-  r"""Dual exponential synapse model.
 
-  **Model Descriptions**
+class _ICa_p2q_markov(CalciumChannel):
+  r"""The calcium current model of :math:`p^2q` current which described with first-order Markov chain.
 
-  The dual exponential synapse model [1]_, also named as *difference of two exponentials* model,
-  is given by:
+  The dynamics of this generalized calcium current model is given by:
 
   .. math::
 
-    g_{\mathrm{syn}}(t)=g_{\mathrm{max}} \frac{\tau_{1} \tau_{2}}{
-        \tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)
-        -\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right)
-
-  where :math:`\tau_1` is the time constant of the decay phase, :math:`\tau_2`
-  is the time constant of the rise phase, :math:`t_0` is the time of the pre-synaptic
-  spike, :math:`g_{\mathrm{max}}` is the maximal conductance.
+      I_{CaT} &= g_{max} p^2 q(V-E_{Ca}) \\
+      {dp \over dt} &= \phi_p (\alpha_p(V)(1-p) - \beta_p(V)p) \\
+      {dq \over dt} &= \phi_q (\alpha_q(V)(1-q) - \beta_q(V)q) \\
 
-  However, in practice, this formula is hard to implement. The equivalent solution is
-  two coupled linear differential equations [2]_:
+  where :math:`\phi_p` and :math:`\phi_q` are temperature-dependent factors,
+  :math:`E_{Ca}` is the reversal potential of Calcium channel.
 
-  .. math::
-
-      \begin{aligned}
-      &\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\
-      &\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right),
-      \end{aligned}
+  Parameters
+  ----------
+  size: int, tuple of int
+    The size of the simulation target.
+  keep_size: bool
+    Keep size or flatten the size?
+  method: str
+    The numerical method
+  name: str
+    The name of the object.
+  g_max : float, ArrayType, Callable, Initializer
+    The maximum conductance.
+  phi_p : float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`p`.
+  phi_q : float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`q`.
 
-  .. [1] Sterratt, David, Bruce Graham, Andrew Gillies, and David Willshaw.
-         "The Synapse." Principles of Computational Modelling in Neuroscience.
-         Cambridge: Cambridge UP, 2011. 172-95. Print.
-  .. [2] Roth, A., & Van Rossum, M. C. W. (2009). Modeling Synapses. Computational
-         Modeling Methods for Neuroscientists.
-
-  Args:
-    tau_decay: float, ArrayArray, Callable. The time constant of the synaptic decay phase. [ms]
-    tau_rise: float, ArrayArray, Callable. The time constant of the synaptic rise phase. [ms]
-    %s
   """
 
   def __init__(
       self,
-      size: Union[int, Sequence[int]],
+      size: Shape,
       keep_size: bool = False,
-      sharding: Optional[Sequence[str]] = None,
+      phi_p: Union[float, ArrayType, Initializer, Callable] = 3.,
+      phi_q: Union[float, ArrayType, Initializer, Callable] = 3.,
+      g_max: Union[float, ArrayType, Initializer, Callable] = 2.,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
-
-      # synapse parameters
-      tau_decay: Union[float, ArrayType, Callable] = 10.0,
-      tau_rise: Union[float, ArrayType, Callable] = 1.,
   ):
-    super().__init__(name=name,
-                     mode=mode,
-                     size=size,
+    super().__init__(size,
                      keep_size=keep_size,
-                     sharding=sharding)
+                     name=name,
+                     mode=mode)
 
     # parameters
-    self.tau_rise = self.init_param(tau_rise)
-    self.tau_decay = self.init_param(tau_decay)
-
-    # integrator
-    self.integral = odeint(JointEq(self.dg, self.dh), method=method)
-
-    self.reset_state(self.mode)
-
-  def reset_state(self, batch_size=None):
-    self.h = self.init_variable(bm.zeros, batch_size)
-    self.g = self.init_variable(bm.zeros, batch_size)
+    self.phi_p = parameter(phi_p, self.varshape, allow_none=False)
+    self.phi_q = parameter(phi_q, self.varshape, allow_none=False)
+    self.g_max = parameter(g_max, self.varshape, allow_none=False)
+
+    # variables
+    self.p = variable(bm.zeros, self.mode, self.varshape)
+    self.q = variable(bm.zeros, self.mode, self.varshape)
 
-  def dh(self, h, t):
-    return -h / self.tau_rise
-
-  def dg(self, g, t, h):
-    return -g / self.tau_decay + h
+    # functions
+    self.integral = odeint(JointEq([self.dp, self.dq]), method=method)
 
-  def update(self, x):
-    t = share.load('t')
-    dt = share.load('dt')
+  def dp(self, p, t, V):
+    return self.phi_p * (self.f_p_alpha(V) * (1 - p) - self.f_p_beta(V) * p)
 
-    # update synaptic variables
-    self.g.value, self.h.value = self.integral(self.g.value, self.h.value, t, dt=dt)
-    self.h += x
-    return self.g.value
+  def dq(self, q, t, V):
+    return self.phi_q * (self.f_q_alpha(V) * (1 - q) - self.f_q_beta(V) * q)
 
-  def return_for_delay(self):
-    return self.g
+  def update(self, V, C, E):
+    self.p.value, self.q.value = self.integral(self.p, self.q, share['t'], V, share['dt'])
 
+  def current(self, V, C, E):
+    return self.g_max * self.p * self.p * self.q * (E - V)
 
-DualExpon.__doc__ = DualExpon.__doc__ % (pneu_doc,)
+  def reset_state(self, V, C, E, batch_size=None):
+    alpha, beta = self.f_p_alpha(V), self.f_p_beta(V)
+    self.p.value = alpha / (alpha + beta)
+    alpha, beta = self.f_q_alpha(V), self.f_q_beta(V)
+    self.q.value = alpha / (alpha + beta)
+    if isinstance(batch_size, int):
+      assert self.p.shape[0] == batch_size
+      assert self.q.shape[0] == batch_size
 
+  def f_p_alpha(self, V):
+    raise NotImplementedError
 
-class Alpha(DualExpon):
-  r"""Alpha synapse model.
+  def f_p_beta(self, V):
+    raise NotImplementedError
 
-  **Model Descriptions**
+  def f_q_alpha(self, V):
+    raise NotImplementedError
 
-  The analytical expression of alpha synapse is given by:
+  def f_q_beta(self, V):
+    raise NotImplementedError
 
-  .. math::
 
-      g_{syn}(t)= g_{max} \frac{t-t_{s}}{\tau} \exp \left(-\frac{t-t_{s}}{\tau}\right).
+class ICaN_IS2008(CalciumChannel):
+  r"""The calcium-activated non-selective cation channel model
+  proposed by (Inoue & Strowbridge, 2008) [2]_.
 
-  While, this equation is hard to implement. So, let's try to convert it into the
-  differential forms:
+  The dynamics of the calcium-activated non-selective cation channel model [1]_ [2]_ is given by:
 
   .. math::
 
       \begin{aligned}
-      &\frac{d g}{d t}=-\frac{g}{\tau}+h \\
-      &\frac{d h}{d t}=-\frac{h}{\tau}+\delta\left(t_{0}-t\right)
+      I_{CAN} &=g_{\mathrm{max}} M\left([Ca^{2+}]_{i}\right) p \left(V-E\right)\\
+      &M\left([Ca^{2+}]_{i}\right) ={[Ca^{2+}]_{i} \over 0.2+[Ca^{2+}]_{i}} \\
+      &{dp \over dt} = {\phi \cdot (p_{\infty}-p)\over \tau_p} \\
+      &p_{\infty} = {1.0 \over 1 + \exp(-(V + 43) / 5.2)} \\
+      &\tau_{p} = {2.7 \over \exp(-(V + 55) / 15) + \exp((V + 55) / 15)} + 1.6
       \end{aligned}
 
-  .. [1] Sterratt, David, Bruce Graham, Andrew Gillies, and David Willshaw.
-          "The Synapse." Principles of Computational Modelling in Neuroscience.
-          Cambridge: Cambridge UP, 2011. 172-95. Print.
-
-  Args:
-    tau_decay: float, ArrayType, Callable. The time constant [ms] of the synaptic decay phase.
-       The name of this synaptic projection.
-    %s
-  """
-
-  def __init__(
-      self,
-      size: Union[int, Sequence[int]],
-      keep_size: bool = False,
-      sharding: Optional[Sequence[str]] = None,
-      method: str = 'exp_auto',
-      name: Optional[str] = None,
-      mode: Optional[bm.Mode] = None,
-
-      # synapse parameters
-      tau_decay: Union[float, ArrayType, Callable] = 10.0,
-  ):
-    super().__init__(
-      tau_decay=tau_decay,
-      tau_rise=tau_decay,
-      method=method,
-      name=name,
-      mode=mode,
-      size=size,
-      keep_size=keep_size,
-      sharding=sharding
-    )
-
-
-Alpha.__doc__ = Alpha.__doc__ % (pneu_doc,)
-
-
-class NMDA(SynDyn):
-  r"""NMDA synapse model.
-
-  **Model Descriptions**
-
-  The NMDA receptor is a glutamate receptor and ion channel found in neurons.
-  The NMDA receptor is one of three types of ionotropic glutamate receptors,
-  the other two being AMPA and kainate receptors.
-
-  The NMDA receptor mediated conductance depends on the postsynaptic voltage.
-  The voltage dependence is due to the blocking of the pore of the NMDA receptor
-  from the outside by a positively charged magnesium ion. The channel is
-  nearly completely blocked at resting potential, but the magnesium block is
-  relieved if the cell is depolarized. The fraction of channels :math:`g_{\infty}`
-  that are not blocked by magnesium can be fitted to
+  where :math:`\phi` is the temperature factor.
 
-  .. math::
-
-      g_{\infty}(V,[{Mg}^{2+}]_{o}) = (1+{e}^{-\alpha V}
-      \frac{[{Mg}^{2+}]_{o}} {\beta})^{-1}
-
-  Here :math:`[{Mg}^{2+}]_{o}` is the extracellular magnesium concentration,
-  usually 1 mM. Thus, the channel acts as a
-  "coincidence detector" and only once both of these conditions are met, the
-  channel opens and it allows positively charged ions (cations) to flow through
-  the cell membrane [2]_.
-
-  If we make the approximation that the magnesium block changes
-  instantaneously with voltage and is independent of the gating of the channel,
-  the net NMDA receptor-mediated synaptic current is given by
-
-  .. math::
-
-      I_{syn} = g_\mathrm{NMDA}(t) (V(t)-E) \cdot g_{\infty}
-
-  where :math:`V(t)` is the post-synaptic neuron potential, :math:`E` is the
-  reversal potential.
-
-  Simultaneously, the kinetics of synaptic state :math:`g` is given by
-
-  .. math::
-
-      & g_\mathrm{NMDA} (t) = g_{max} g \\
-      & \frac{d g}{dt} = -\frac{g} {\tau_{decay}}+a x(1-g) \\
-      & \frac{d x}{dt} = -\frac{x}{\tau_{rise}}+ \sum_{k} \delta(t-t_{j}^{k})
-
-  where the decay time of NMDA currents is usually taken to be
-  :math:`\tau_{decay}` =100 ms, :math:`a= 0.5 ms^{-1}`, and :math:`\tau_{rise}` =2 ms.
-
-  The NMDA receptor has been thought to be very important for controlling
-  synaptic plasticity and mediating learning and memory functions [3]_.
-
-
-  .. [1] Brunel N, Wang X J. Effects of neuromodulation in a
-         cortical network model of object working memory dominated
-         by recurrent inhibition[J].
-         Journal of computational neuroscience, 2001, 11(1): 63-85.
-  .. [2] Furukawa, Hiroyasu, Satinder K. Singh, Romina Mancusso, and
-         Eric Gouaux. "Subunit arrangement and function in NMDA receptors."
-         Nature 438, no. 7065 (2005): 185-192.
-  .. [3] Li, F. and Tsien, J.Z., 2009. Memory and the NMDA receptors. The New
-         England journal of medicine, 361(3), p.302.
-  .. [4] https://en.wikipedia.org/wiki/NMDA_receptor
-
-  Args:
-    tau_decay: float, ArrayType, Callable. The time constant of the synaptic decay phase. Default 100 [ms]
-    tau_rise: float, ArrayType, Callable. The time constant of the synaptic rise phase. Default 2 [ms]
-    a: float, ArrayType, Callable. Default 0.5 ms^-1.
-    %s
+  Parameters
+  ----------
+  g_max : float
+    The maximal conductance density (:math:`mS/cm^2`).
+  E : float
+    The reversal potential (mV).
+  phi : float
+    The temperature factor.
+
+  References
+  ----------
+
+  .. [1] Destexhe, Alain, et al. "A model of spindle rhythmicity in the isolated
+         thalamic reticular nucleus." Journal of neurophysiology 72.2 (1994): 803-818.
+  .. [2] Inoue T, Strowbridge BW (2008) Transient activity induces a long-lasting
+         increase in the excitability of olfactory bulb interneurons.
+         J Neurophysiol 99: 187199.
   """
 
+  '''The type of the master object.'''
+  master_type = CalciumDyna
+
   def __init__(
       self,
-      size: Union[int, Sequence[int]],
+      size: Shape,
       keep_size: bool = False,
-      sharding: Optional[Sequence[str]] = None,
+      E: Union[float, ArrayType, Initializer, Callable] = 10.,
+      g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
+      phi: Union[float, ArrayType, Initializer, Callable] = 1.,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
-
-      # synapse parameters
-      a: Union[float, ArrayType, Callable] = 0.5,
-      tau_decay: Union[float, ArrayType, Callable] = 100.,
-      tau_rise: Union[float, ArrayType, Callable] = 2.,
   ):
-    super().__init__(name=name,
-                     mode=mode,
-                     size=size,
+    super().__init__(size,
                      keep_size=keep_size,
-                     sharding=sharding)
+                     name=name,
+                     mode=mode)
 
     # parameters
-    self.tau_decay = self.init_param(tau_decay)
-    self.tau_rise = self.init_param(tau_rise)
-    self.a = self.init_param(a)
-
-    # integral
-    self.integral = odeint(method=method, f=JointEq(self.dg, self.dx))
-
-    self.reset_state(self.mode)
-
-  def dg(self, g, t, x):
-    return -g / self.tau_decay + self.a * x * (1 - g)
-
-  def dx(self, x, t):
-    return -x / self.tau_rise
+    self.E = parameter(E, self.varshape, allow_none=False)
+    self.g_max = parameter(g_max, self.varshape, allow_none=False)
+    self.phi = parameter(phi, self.varshape, allow_none=False)
 
-  def reset_state(self, batch_size=None):
-    self.g = self.init_variable(bm.zeros, batch_size)
-    self.x = self.init_variable(bm.zeros, batch_size)
+    # variables
+    self.p = variable(bm.zeros, self.mode, self.varshape)
 
-  def update(self, pre_spike):
-    t = share.load('t')
-    dt = share.load('dt')
-    self.g.value, self.x.value = self.integral(self.g, self.x, t, dt=dt)
-    self.x += pre_spike
-    return self.g.value
-
-  def return_for_delay(self):
-    return self.g
-
-
-NMDA.__doc__ = NMDA.__doc__ % (pneu_doc,)
-
-
-class STD(SynDyn):
-  r"""Synaptic output with short-term depression.
-
-  This model filters the synaptic current by the following equation:
-
-  .. math::
-
-     I_{syn}^+(t) = I_{syn}^-(t) * x
-
-  where :math:`x` is the normalized variable between 0 and 1, and
-  :math:`I_{syn}^-(t)` and :math:`I_{syn}^+(t)` are the synaptic currents before
-  and after STD filtering.
-
-  Moreover, :math:`x` is updated according to the dynamics of:
-
-  .. math::
-
-     \frac{dx}{dt} = \frac{1-x}{\tau} - U * x * \delta(t-t_{spike})
-
-  where :math:`U` is the fraction of resources used per action potential,
-  :math:`\tau` is the time constant of recovery of the synaptic vesicles.
+    # function
+    self.integral = odeint(self.derivative, method=method)
 
-  Args:
-    tau: float, ArrayType, Callable. The time constant of recovery of the synaptic vesicles.
-    U: float, ArrayType, Callable. The fraction of resources used per action potential.
-    %s
+  def derivative(self, p, t, V):
+    phi_p = 1.0 / (1 + bm.exp(-(V + 43.) / 5.2))
+    p_inf = 2.7 / (bm.exp(-(V + 55.) / 15.) + bm.exp((V + 55.) / 15.)) + 1.6
+    return self.phi * (phi_p - p) / p_inf
+
+  def update(self, V, C, E):
+    self.p.value = self.integral(self.p.value, share['t'], V, share['dt'])
+
+  def current(self, V, C, E):
+    M = C / (C + 0.2)
+    g = self.g_max * M * self.p
+    return g * (self.E - V)
+
+  def reset_state(self, V, C, E, batch_size=None):
+    self.p.value = 1.0 / (1 + bm.exp(-(V + 43.) / 5.2))
+    if isinstance(batch_size, int):
+      assert self.p.shape[0] == batch_size
+
+
+class ICaT_HM1992(_ICa_p2q_ss):
+  r"""The low-threshold T-type calcium current model proposed by (Huguenard & McCormick, 1992) [1]_.
+
+  The dynamics of the low-threshold T-type calcium current model [1]_ is given by:
+
+  .. math::
+
+      I_{CaT} &= g_{max} p^2 q(V-E_{Ca}) \\
+      {dp \over dt} &= {\phi_p \cdot (p_{\infty}-p)\over \tau_p} \\
+      &p_{\infty} = {1 \over 1+\exp [-(V+59-V_{sh}) / 6.2]} \\
+      &\tau_{p} = 0.612 + {1 \over \exp [-(V+132.-V_{sh}) / 16.7]+\exp [(V+16.8-V_{sh}) / 18.2]} \\
+      {dq \over dt} &= {\phi_q \cdot (q_{\infty}-q) \over \tau_q} \\
+      &q_{\infty} = {1 \over 1+\exp [(V+83-V_{sh}) / 4]} \\
+      & \begin{array}{l} \tau_{q} = \exp \left(\frac{V+467-V_{sh}}{66.6}\right)  \quad V< (-80 +V_{sh})\, mV  \\
+          \tau_{q} = \exp \left(\frac{V+22-V_{sh}}{-10.5}\right)+28 \quad V \geq (-80 + V_{sh})\, mV \end{array}
+
+  where :math:`\phi_p = 3.55^{\frac{T-24}{10}}` and :math:`\phi_q = 3^{\frac{T-24}{10}}`
+  are temperature-dependent factors (:math:`T` is the temperature in Celsius),
+  :math:`E_{Ca}` is the reversal potential of Calcium channel.
+
+  Parameters
+  ----------
+  T : float, ArrayType
+    The temperature.
+  T_base_p : float, ArrayType
+    The brainpy_object temperature factor of :math:`p` channel.
+  T_base_q : float, ArrayType
+    The brainpy_object temperature factor of :math:`q` channel.
+  g_max : float, ArrayType, Callable, Initializer
+    The maximum conductance.
+  V_sh : float, ArrayType, Callable, Initializer
+    The membrane potential shift.
+  phi_p : optional, float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`p`.
+  phi_q : optional, float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`q`.
+
+  References
+  ----------
+
+  .. [1] Huguenard JR, McCormick DA (1992) Simulation of the currents involved in
+         rhythmic oscillations in thalamic relay neurons. J Neurophysiol 68:13731383.
+
+  See Also
+  --------
+  ICa_p2q_form
   """
 
   def __init__(
       self,
-      size: Union[int, Sequence[int]],
+      size: Shape,
       keep_size: bool = False,
-      sharding: Optional[Sequence[str]] = None,
+      T: Union[float, ArrayType] = 36.,
+      T_base_p: Union[float, ArrayType] = 3.55,
+      T_base_q: Union[float, ArrayType] = 3.,
+      g_max: Union[float, ArrayType, Initializer, Callable] = 2.,
+      V_sh: Union[float, ArrayType, Initializer, Callable] = -3.,
+      phi_p: Union[float, ArrayType, Initializer, Callable] = None,
+      phi_q: Union[float, ArrayType, Initializer, Callable] = None,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
-
-      # synapse parameters
-      tau: Union[float, ArrayType, Callable] = 200.,
-      U: Union[float, ArrayType, Callable] = 0.07,
   ):
-    super().__init__(name=name,
-                     mode=mode,
-                     size=size,
+    phi_p = T_base_p ** ((T - 24) / 10) if phi_p is None else phi_p
+    phi_q = T_base_q ** ((T - 24) / 10) if phi_q is None else phi_q
+    super().__init__(size,
                      keep_size=keep_size,
-                     sharding=sharding)
+                     name=name,
+                     method=method,
+                     g_max=g_max,
+                     phi_p=phi_p,
+                     phi_q=phi_q,
+                     mode=mode)
 
     # parameters
-    self.tau = self.init_param(tau)
-    self.U = self.init_param(U)
-
-    # integral function
-    self.integral = odeint(lambda x, t: (1 - x) / self.tau, method=method)
-
-    self.reset_state(self.mode)
-
-  def reset_state(self, batch_size=None):
-    self.x = self.init_variable(bm.ones, batch_size)
-
-  def update(self, pre_spike):
-    t = share.load('t')
-    dt = share.load('dt')
-    x = self.integral(self.x.value, t, dt)
-    self.x.value = bm.where(pre_spike, x - self.U * self.x, x)
-    return self.x.value
-
-  def return_for_delay(self):
-    return self.x
-
-
-STD.__doc__ = STD.__doc__ % (pneu_doc,)
-
-
-class STP(SynDyn):
-  r"""Synaptic output with short-term plasticity.
-
-  This model filters the synaptic currents according to two variables: :math:`u` and :math:`x`.
-
-  .. math::
-
-     I_{syn}^+(t) = I_{syn}^-(t) * x * u
-
-  where :math:`I_{syn}^-(t)` and :math:`I_{syn}^+(t)` are the synaptic currents before
-  and after STP filtering, :math:`x` denotes the fraction of resources that remain available
-  after neurotransmitter depletion, and :math:`u` represents the fraction of available
-  resources ready for use (release probability).
-
-  The dynamics of :math:`u` and :math:`x` are governed by
-
-  .. math::
-
-     \begin{aligned}
-    \frac{du}{dt} & = & -\frac{u}{\tau_f}+U(1-u^-)\delta(t-t_{sp}), \\
-    \frac{dx}{dt} & = & \frac{1-x}{\tau_d}-u^+x^-\delta(t-t_{sp}), \\
-    \tag{1}\end{aligned}
-
-  where :math:`t_{sp}` denotes the spike time and :math:`U` is the increment
-  of :math:`u` produced by a spike. :math:`u^-, x^-` are the corresponding
-  variables just before the arrival of the spike, and :math:`u^+`
-  refers to the moment just after the spike.
-
-  Args:
-    tau_f: float, ArrayType, Callable. The time constant of short-term facilitation.
-    tau_d: float, ArrayType, Callable. The time constant of short-term depression.
-    U: float, ArrayType, Callable. The fraction of resources used per action potential.
-    %s
+    self.T = parameter(T, self.varshape, allow_none=False)
+    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
+    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
+    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
+
+  def f_p_inf(self, V):
+    return 1. / (1 + bm.exp(-(V + 59. - self.V_sh) / 6.2))
+
+  def f_p_tau(self, V):
+    return 1. / (bm.exp(-(V + 132. - self.V_sh) / 16.7) +
+                 bm.exp((V + 16.8 - self.V_sh) / 18.2)) + 0.612
+
+  def f_q_inf(self, V):
+    return 1. / (1. + bm.exp((V + 83. - self.V_sh) / 4.0))
+
+  def f_q_tau(self, V):
+    return bm.where(V >= (-80. + self.V_sh),
+                    bm.exp(-(V + 22. - self.V_sh) / 10.5) + 28.,
+                    bm.exp((V + 467. - self.V_sh) / 66.6))
+
+
+class ICaT_HP1992(_ICa_p2q_ss):
+  r"""The low-threshold T-type calcium current model for thalamic
+  reticular nucleus proposed by (Huguenard & Prince, 1992) [1]_.
+
+  The dynamics of the low-threshold T-type calcium current model in thalamic
+  reticular nucleus neurons [1]_ is given by:
+
+  .. math::
+
+      I_{CaT} &= g_{max} p^2 q(V-E_{Ca}) \\
+      {dp \over dt} &= {\phi_p \cdot (p_{\infty}-p)\over \tau_p} \\
+      &p_{\infty} = {1 \over 1+\exp [-(V+52-V_{sh}) / 7.4]}  \\
+      &\tau_{p} = 3+{1 \over \exp [(V+27-V_{sh}) / 10]+\exp [-(V+102-V_{sh}) / 15]} \\
+      {dq \over dt} &= {\phi_q \cdot (q_{\infty}-q) \over \tau_q} \\
+      &q_{\infty} = {1 \over 1+\exp [(V+80-V_{sh}) / 5]} \\
+      & \tau_q = 85+ {1 \over \exp [(V+48-V_{sh}) / 4]+\exp [-(V+407-V_{sh}) / 50]}
+
+  where :math:`\phi_p = 5^{\frac{T-24}{10}}` and :math:`\phi_q = 3^{\frac{T-24}{10}}`
+  are temperature-dependent factors (:math:`T` is the temperature in Celsius),
+  :math:`E_{Ca}` is the reversal potential of Calcium channel.
+
+  Parameters
+  ----------
+  T : float, ArrayType
+    The temperature.
+  T_base_p : float, ArrayType
+    The brainpy_object temperature factor of :math:`p` channel.
+  T_base_q : float, ArrayType
+    The brainpy_object temperature factor of :math:`q` channel.
+  g_max : float, ArrayType, Callable, Initializer
+    The maximum conductance.
+  V_sh : float, ArrayType, Callable, Initializer
+    The membrane potential shift.
+  phi_p : optional, float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`p`.
+  phi_q : optional, float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`q`.
+
+  References
+  ----------
+
+  .. [1] Huguenard JR, Prince DA (1992) A novel T-type current underlies
+         prolonged Ca2+- dependent burst firing in GABAergic neurons of rat
+         thalamic reticular nucleus. J Neurosci 12: 38043817.
+
+  See Also
+  --------
+  ICa_p2q_form
   """
 
   def __init__(
       self,
-      size: Union[int, Sequence[int]],
+      size: Shape,
       keep_size: bool = False,
-      sharding: Optional[Sequence[str]] = None,
+      T: Union[float, ArrayType] = 36.,
+      T_base_p: Union[float, ArrayType] = 5.,
+      T_base_q: Union[float, ArrayType] = 3.,
+      g_max: Union[float, ArrayType, Initializer, Callable] = 1.75,
+      V_sh: Union[float, ArrayType, Initializer, Callable] = -3.,
+      phi_p: Union[float, ArrayType, Initializer, Callable] = None,
+      phi_q: Union[float, ArrayType, Initializer, Callable] = None,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
-
-      # synapse parameters
-      U: Union[float, ArrayType, Callable] = 0.15,
-      tau_f: Union[float, ArrayType, Callable] = 1500.,
-      tau_d: Union[float, ArrayType, Callable] = 200.,
   ):
-    super().__init__(name=name,
-                     mode=mode,
-                     size=size,
+    phi_p = T_base_p ** ((T - 24) / 10) if phi_p is None else phi_p
+    phi_q = T_base_q ** ((T - 24) / 10) if phi_q is None else phi_q
+    super().__init__(size,
                      keep_size=keep_size,
-                     sharding=sharding)
+                     name=name,
+                     method=method,
+                     g_max=g_max,
+                     phi_p=phi_p,
+                     phi_q=phi_q,
+                     mode=mode)
 
     # parameters
-    self.tau_f = self.init_param(tau_f)
-    self.tau_d = self.init_param(tau_d)
-    self.U = self.init_param(U)
-    self.method = method
-
-    # integral function
-    self.integral = odeint(self.derivative, method=self.method)
-
-    self.reset_state(self.mode)
-
-  def reset_state(self, batch_size=None):
-    self.x = self.init_variable(bm.ones, batch_size)
-    self.u = self.init_variable(bm.ones, batch_size)
-    self.u.fill_(self.U)
-
-  @property
-  def derivative(self):
-    du = lambda u, t: self.U - u / self.tau_f
-    dx = lambda x, t: (1 - x) / self.tau_d
-    return JointEq([du, dx])
-
-  def update(self, pre_spike):
-    t = share.load('x')
-    dt = share.load('dt')
-    u, x = self.integral(self.u.value, self.x.value, t, dt)
-    u = bm.where(pre_spike, u + self.U * (1 - self.u), u)
-    x = bm.where(pre_spike, x - u * self.x, x)
-    self.x.value = x
-    self.u.value = u
-    return u * x
-
-  def return_for_delay(self):
-    return ReturnInfo(size=self.varshape,
-                      batch_or_mode=self.mode,
-                      axis_names=self.sharding,
-                      init=bm.zeros)
-
-
-STP.__doc__ = STP.__doc__ % (pneu_doc,)
-
-
-class AMPA(SynDyn):
-  r"""AMPA synapse model.
-
-  **Model Descriptions**
-
-  AMPA receptor is an ionotropic receptor, which is an ion channel.
-  When it is bound by neurotransmitters, it will immediately open the
-  ion channel, causing the change of membrane potential of postsynaptic neurons.
-
-  A classical model is to use the Markov process to model ion channel switch.
-  Here :math:`g` represents the probability of channel opening, :math:`1-g`
-  represents the probability of ion channel closing, and :math:`\alpha` and
-  :math:`\beta` are the transition probability. Because neurotransmitters can
-  open ion channels, the transfer probability from :math:`1-g` to :math:`g`
-  is affected by the concentration of neurotransmitters. We denote the concentration
-  of neurotransmitters as :math:`[T]` and get the following Markov process.
+    self.T = parameter(T, self.varshape, allow_none=False)
+    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
+    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
+    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
 
-  .. image:: ../../../_static/synapse_markov.png
-      :align: center
+  def f_p_inf(self, V):
+    return 1. / (1. + bm.exp(-(V + 52. - self.V_sh) / 7.4))
 
-  We obtained the following formula when describing the process by a differential equation.
+  def f_p_tau(self, V):
+    return 3. + 1. / (bm.exp((V + 27. - self.V_sh) / 10.) +
+                      bm.exp(-(V + 102. - self.V_sh) / 15.))
 
-  .. math::
+  def f_q_inf(self, V):
+    return 1. / (1. + bm.exp((V + 80. - self.V_sh) / 5.))
 
-      \frac{ds}{dt} =\alpha[T](1-g)-\beta g
+  def f_q_tau(self, V):
+    return 85. + 1. / (bm.exp((V + 48. - self.V_sh) / 4.) +
+                       bm.exp(-(V + 407. - self.V_sh) / 50.))
 
-  where :math:`\alpha [T]` denotes the transition probability from state :math:`(1-g)`
-  to state :math:`(g)`; and :math:`\beta` represents the transition probability of
-  the other direction. :math:`\alpha` is the binding constant. :math:`\beta` is the
-  unbinding constant. :math:`[T]` is the neurotransmitter concentration, and
-  has the duration of 0.5 ms.
 
-  Moreover, the post-synaptic current on the post-synaptic neuron is formulated as
+class ICaHT_HM1992(_ICa_p2q_ss):
+  r"""The high-threshold T-type calcium current model proposed by (Huguenard & McCormick, 1992) [1]_.
 
-  .. math::
+  The high-threshold T-type calcium current model is adopted from [1]_.
+  Its dynamics is given by
 
-      I_{syn} = g_{max} g (V-E)
+  .. math::
 
-  where :math:`g_{max}` is the maximum conductance, and `E` is the reverse potential.
+      \begin{aligned}
+      I_{\mathrm{Ca/HT}} &= g_{\mathrm{max}} p^2 q (V-E_{Ca})
+      \\
+      {dp \over dt} &= {\phi_{p} \cdot (p_{\infty} - p) \over \tau_{p}} \\
+      &\tau_{p} =\frac{1}{\exp \left(\frac{V+132-V_{sh}}{-16.7}\right)+\exp \left(\frac{V+16.8-V_{sh}}{18.2}\right)}+0.612 \\
+      & p_{\infty} = {1 \over 1+exp[-(V+59-V_{sh}) / 6.2]}
+      \\
+      {dq \over dt} &= {\phi_{q} \cdot (q_{\infty} - h) \over \tau_{q}} \\
+      & \begin{array}{l} \tau_q = \exp \left(\frac{V+467-V_{sh}}{66.6}\right)  \quad V< (-80 +V_{sh})\, mV  \\
+      \tau_q = \exp \left(\frac{V+22-V_{sh}}{-10.5}\right)+28 \quad V \geq (-80 + V_{sh})\, mV \end{array} \\
+      &q_{\infty}  = {1 \over 1+exp[(V+83 -V_{shift})/4]}
+      \end{aligned}
 
-  .. [1] Vijayan S, Kopell N J. Thalamic model of awake alpha oscillations
-         and implications for stimulus processing[J]. Proceedings of the
-         National Academy of Sciences, 2012, 109(45): 18553-18558.
-
-  Args:
-    alpha: float, ArrayType, Callable. Binding constant.
-    beta: float, ArrayType, Callable. Unbinding constant.
-    T: float, ArrayType, Callable. Transmitter concentration when synapse is triggered by
-      a pre-synaptic spike.. Default 1 [mM].
-    T_dur: float, ArrayType, Callable. Transmitter concentration duration time after being triggered. Default 1 [ms]
-    %s
+  where :math:`phi_p = 3.55^{\frac{T-24}{10}}` and :math:`phi_q = 3^{\frac{T-24}{10}}`
+  are temperature-dependent factors (:math:`T` is the temperature in Celsius),
+  :math:`E_{Ca}` is the reversal potential of Calcium channel.
+
+  Parameters
+  ----------
+  T : float, ArrayType
+    The temperature.
+  T_base_p : float, ArrayType
+    The brainpy_object temperature factor of :math:`p` channel.
+  T_base_q : float, ArrayType
+    The brainpy_object temperature factor of :math:`q` channel.
+  g_max : float, ArrayType, Initializer, Callable
+    The maximum conductance.
+  V_sh : float, ArrayType, Initializer, Callable
+    The membrane potential shift.
+
+  References
+  ----------
+  .. [1] Huguenard JR, McCormick DA (1992) Simulation of the currents involved in
+         rhythmic oscillations in thalamic relay neurons. J Neurophysiol 68:13731383.
+
+  See Also
+  --------
+  ICa_p2q_form
   """
 
-  supported_modes = (bm.NonBatchingMode,)
-
   def __init__(
       self,
-      size: Union[int, Sequence[int]],
+      size: Shape,
       keep_size: bool = False,
-      sharding: Optional[Sequence[str]] = None,
+      T: Union[float, ArrayType] = 36.,
+      T_base_p: Union[float, ArrayType] = 3.55,
+      T_base_q: Union[float, ArrayType] = 3.,
+      g_max: Union[float, ArrayType, Initializer, Callable] = 2.,
+      V_sh: Union[float, ArrayType, Initializer, Callable] = 25.,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
-
-      # synapse parameters
-      alpha: Union[float, ArrayType, Callable] = 0.98,
-      beta: Union[float, ArrayType, Callable] = 0.18,
-      T: Union[float, ArrayType, Callable] = 0.5,
-      T_dur: Union[float, ArrayType, Callable] = 0.5,
   ):
-    super().__init__(name=name,
-                     mode=mode,
-                     size=size,
+    super().__init__(size,
                      keep_size=keep_size,
-                     sharding=sharding)
+                     name=name,
+                     method=method,
+                     g_max=g_max,
+                     phi_p=T_base_p ** ((T - 24) / 10),
+                     phi_q=T_base_q ** ((T - 24) / 10),
+                     mode=mode)
 
     # parameters
-    self.alpha = self.init_param(alpha)
-    self.beta = self.init_param(beta)
-    self.T = self.init_param(T)
-    self.T_duration = self.init_param(T_dur)
-
-    # functions
-    self.integral = odeint(method=method, f=self.dg)
-
-    self.reset_state(self.mode)
-
-  def reset_state(self, batch_size=None):
-    self.g = self.init_variable(bm.zeros, batch_size)
-    self.spike_arrival_time = self.init_variable(bm.ones, batch_size)
-    self.spike_arrival_time.fill(-1e7)
+    self.T = parameter(T, self.varshape, allow_none=False)
+    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
+    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
+    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
+
+    # variables
+    self.p = variable(bm.zeros, self.mode, self.varshape)
+    self.q = variable(bm.zeros, self.mode, self.varshape)
 
-  def dg(self, g, t, TT):
-    return self.alpha * TT * (1 - g) - self.beta * g
-
-  def update(self, pre_spike):
-    t = share.load('t')
-    dt = share.load('dt')
-    self.spike_arrival_time.value = bm.where(pre_spike, t, self.spike_arrival_time)
-    TT = ((t - self.spike_arrival_time) < self.T_duration) * self.T
-    self.g.value = self.integral(self.g, t, TT, dt)
-    return self.g.value
+    # function
+    self.integral = odeint(JointEq([self.dp, self.dq]), method=method)
 
-  def return_for_delay(self):
-    return self.g
+  def f_p_inf(self, V):
+    return 1. / (1. + bm.exp(-(V + 59. - self.V_sh) / 6.2))
 
+  def f_p_tau(self, V):
+    return 1. / (bm.exp(-(V + 132. - self.V_sh) / 16.7) +
+                 bm.exp((V + 16.8 - self.V_sh) / 18.2)) + 0.612
 
-AMPA.__doc__ = AMPA.__doc__ % (pneu_doc,)
+  def f_q_inf(self, V):
+    return 1. / (1. + bm.exp((V + 83. - self.V_sh) / 4.))
 
+  def f_q_tau(self, V):
+    return bm.where(V >= (-80. + self.V_sh),
+                    bm.exp(-(V + 22. - self.V_sh) / 10.5) + 28.,
+                    bm.exp((V + 467. - self.V_sh) / 66.6))
 
-class GABAa(AMPA):
-  r"""GABAa synapse model.
 
-  **Model Descriptions**
+class ICaHT_Re1993(_ICa_p2q_markov):
+  r"""The high-threshold T-type calcium current model proposed by (Reuveni, et al., 1993) [1]_.
 
-  GABAa synapse model has the same equation with the `AMPA synapse <./brainmodels.synapses.AMPA.rst>`_,
+  HVA Calcium current was described for neocortical neurons by Sayer et al. (1990).
+  Its dynamics is given by (the rate functions are measured under 36 Celsius):
 
   .. math::
 
-      \frac{d g}{d t}&=\alpha[T](1-g) - \beta g \\
-      I_{syn}&= - g_{max} g (V - E)
+     \begin{aligned}
+      I_{L} &=\bar{g}_{L} q^{2} r\left(V-E_{\mathrm{Ca}}\right) \\
+      \frac{\mathrm{d} q}{\mathrm{~d} t} &= \phi_p (\alpha_{q}(V)(1-q)-\beta_{q}(V) q) \\
+      \frac{\mathrm{d} r}{\mathrm{~d} t} &= \phi_q (\alpha_{r}(V)(1-r)-\beta_{r}(V) r) \\
+      \alpha_{q} &=\frac{0.055(-27-V+V_{sh})}{\exp [(-27-V+V_{sh}) / 3.8]-1} \\
+      \beta_{q} &=0.94 \exp [(-75-V+V_{sh}) / 17] \\
+      \alpha_{r} &=0.000457 \exp [(-13-V+V_{sh}) / 50] \\
+      \beta_{r} &=\frac{0.0065}{\exp [(-15-V+V_{sh}) / 28]+1},
+      \end{aligned}
 
-  but with the difference of:
+  Parameters
+  ----------
+  size: int, tuple of int
+    The size of the simulation target.
+  keep_size: bool
+    Keep size or flatten the size?
+  method: str
+    The numerical method
+  name: str
+    The name of the object.
+  g_max : float, ArrayType, Callable, Initializer
+    The maximum conductance.
+  V_sh : float, ArrayType, Callable, Initializer
+    The membrane potential shift.
+  T : float, ArrayType
+    The temperature.
+  T_base_p : float, ArrayType
+    The brainpy_object temperature factor of :math:`p` channel.
+  T_base_q : float, ArrayType
+    The brainpy_object temperature factor of :math:`q` channel.
+  phi_p : optional, float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`p`.
+    If `None`, :math:`\phi_p = \mathrm{T_base_p}^{\frac{T-23}{10}}`.
+  phi_q : optional, float, ArrayType, Callable, Initializer
+    The temperature factor for channel :math:`q`.
+    If `None`, :math:`\phi_q = \mathrm{T_base_q}^{\frac{T-23}{10}}`.
+
+  References
+  ----------
+  .. [1] Reuveni, I., et al. "Stepwise repolarization from Ca2+ plateaus
+         in neocortical pyramidal cells: evidence for nonhomogeneous
+         distribution of HVA Ca2+ channels in dendrites." Journal of
+         Neuroscience 13.11 (1993): 4609-4621.
 
-  - Reversal potential of synapse :math:`E` is usually low, typically -80. mV
-  - Activating rate constant :math:`\alpha=0.53`
-  - De-activating rate constant :math:`\beta=0.18`
-  - Transmitter concentration :math:`[T]=1\,\mu ho(\mu S)` when synapse is
-    triggered by a pre-synaptic spike, with the duration of 1. ms.
-
-  .. [1] Destexhe, Alain, and Denis Par. "Impact of network activity
-         on the integrative properties of neocortical pyramidal neurons
-         in vivo." Journal of neurophysiology 81.4 (1999): 1531-1547.
-
-  Args:
-    alpha: float, ArrayType, Callable. Binding constant. Default 0.062
-    beta: float, ArrayType, Callable. Unbinding constant. Default 3.57
-    T: float, ArrayType, Callable. Transmitter concentration when synapse is triggered by
-      a pre-synaptic spike.. Default 1 [mM].
-    T_dur: float, ArrayType, Callable. Transmitter concentration duration time
-      after being triggered. Default 1 [ms]
-    %s
   """
 
   def __init__(
       self,
-      size: Union[int, Sequence[int]],
+      size: Shape,
       keep_size: bool = False,
-      sharding: Optional[Sequence[str]] = None,
+      T: Union[float, ArrayType] = 36.,
+      T_base_p: Union[float, ArrayType] = 2.3,
+      T_base_q: Union[float, ArrayType] = 2.3,
+      phi_p: Union[float, ArrayType, Initializer, Callable] = None,
+      phi_q: Union[float, ArrayType, Initializer, Callable] = None,
+      g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
+      V_sh: Union[float, ArrayType, Initializer, Callable] = 0.,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
-
-      # synapse parameters
-      alpha: Union[float, ArrayType, Callable] = 0.53,
-      beta: Union[float, ArrayType, Callable] = 0.18,
-      T: Union[float, ArrayType, Callable] = 1.,
-      T_dur: Union[float, ArrayType, Callable] = 1.,
   ):
-    super().__init__(alpha=alpha,
-                     beta=beta,
-                     T=T,
-                     T_dur=T_dur,
-                     method=method,
-                     name=name,
-                     mode=mode,
-                     size=size,
+    phi_p = T_base_p ** ((T - 23.) / 10.) if phi_p is None else phi_p
+    phi_q = T_base_q ** ((T - 23.) / 10.) if phi_q is None else phi_q
+    super().__init__(size,
                      keep_size=keep_size,
-                     sharding=sharding)
-
-
-GABAa.__doc__ = GABAa.__doc__ % (pneu_doc,)
-
-
-class BioNMDA(SynDyn):
-  r"""Biological NMDA synapse model.
-
-  **Model Descriptions**
-
-  The NMDA receptor is a glutamate receptor and ion channel found in neurons.
-  The NMDA receptor is one of three types of ionotropic glutamate receptors,
-  the other two being AMPA and kainate receptors.
-
-  The NMDA receptor mediated conductance depends on the postsynaptic voltage.
-  The voltage dependence is due to the blocking of the pore of the NMDA receptor
-  from the outside by a positively charged magnesium ion. The channel is
-  nearly completely blocked at resting potential, but the magnesium block is
-  relieved if the cell is depolarized. The fraction of channels :math:`g_{\infty}`
-  that are not blocked by magnesium can be fitted to
-
-  .. math::
-
-      g_{\infty}(V,[{Mg}^{2+}]_{o}) = (1+{e}^{-a V}
-      \frac{[{Mg}^{2+}]_{o}} {b})^{-1}
-
-  Here :math:`[{Mg}^{2+}]_{o}` is the extracellular magnesium concentration,
-  usually 1 mM. Thus, the channel acts as a
-  "coincidence detector" and only once both of these conditions are met, the
-  channel opens and it allows positively charged ions (cations) to flow through
-  the cell membrane [2]_.
-
-  If we make the approximation that the magnesium block changes
-  instantaneously with voltage and is independent of the gating of the channel,
-  the net NMDA receptor-mediated synaptic current is given by
-
-  .. math::
-
-      I_{syn} = g_\mathrm{NMDA}(t) (V(t)-E) \cdot g_{\infty}
-
-  where :math:`V(t)` is the post-synaptic neuron potential, :math:`E` is the
-  reversal potential.
-
-  Simultaneously, the kinetics of synaptic state :math:`g` is determined by a 2nd-order kinetics [1]_:
-
-  .. math::
-
-      & \frac{d g}{dt} = \alpha_1 x (1 - g) - \beta_1 g \\
-      & \frac{d x}{dt} = \alpha_2 [T] (1 - x) - \beta_2 x
-
-  where :math:`\alpha_1, \beta_1` refers to the conversion rate of variable g and
-  :math:`\alpha_2, \beta_2` refers to the conversion rate of variable x.
-
-  The NMDA receptor has been thought to be very important for controlling
-  synaptic plasticity and mediating learning and memory functions [3]_.
-
-  .. [1] Devaney A J . Mathematical Foundations of Neuroscience[M].
-         Springer New York, 2010: 162.
-  .. [2] Furukawa, Hiroyasu, Satinder K. Singh, Romina Mancusso, and
-         Eric Gouaux. "Subunit arrangement and function in NMDA receptors."
-         Nature 438, no. 7065 (2005): 185-192.
-  .. [3] Li, F. and Tsien, J.Z., 2009. Memory and the NMDA receptors. The New
-         England journal of medicine, 361(3), p.302.
-  .. [4] https://en.wikipedia.org/wiki/NMDA_receptor
-
-
-  Args:
-    alpha1: float, ArrayType, Callable. The conversion rate of g from inactive to active. Default 2 ms^-1.
-    beta1: float, ArrayType, Callable. The conversion rate of g from active to inactive. Default 0.01 ms^-1.
-    alpha2: float, ArrayType, Callable. The conversion rate of x from inactive to active. Default 1 ms^-1.
-    beta2: float, ArrayType, Callable. The conversion rate of x from active to inactive. Default 0.5 ms^-1.
-    T: float, ArrayType, Callable. Transmitter concentration when synapse is
-      triggered by a pre-synaptic spike. Default 1 [mM].
-    T_dur: float, ArrayType, Callable. Transmitter concentration duration time after being triggered. Default 1 [ms]
-    %s
+                     name=name,
+                     method=method,
+                     g_max=g_max,
+                     phi_p=phi_p,
+                     phi_q=phi_q,
+                     mode=mode)
+    self.T = parameter(T, self.varshape, allow_none=False)
+    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
+    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
+    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
+
+  def f_p_alpha(self, V):
+    temp = -27 - V + self.V_sh
+    return 0.055 * temp / (bm.exp(temp / 3.8) - 1)
+
+  def f_p_beta(self, V):
+    return 0.94 * bm.exp((-75. - V + self.V_sh) / 17.)
+
+  def f_q_alpha(self, V):
+    return 0.000457 * bm.exp((-13. - V + self.V_sh) / 50.)
+
+  def f_q_beta(self, V):
+    return 0.0065 / (bm.exp((-15. - V + self.V_sh) / 28.) + 1.)
+
+
+class ICaL_IS2008(_ICa_p2q_ss):
+  r"""The L-type calcium channel model proposed by (Inoue & Strowbridge, 2008) [1]_.
+
+  The L-type calcium channel model is adopted from (Inoue, et, al., 2008) [1]_.
+  Its dynamics is given by:
+
+  .. math::
+
+      I_{CaL} &= g_{max} p^2 q(V-E_{Ca}) \\
+      {dp \over dt} &= {\phi_p \cdot (p_{\infty}-p)\over \tau_p} \\
+      & p_{\infty} = {1 \over 1+\exp [-(V+10-V_{sh}) / 4.]} \\
+      & \tau_{p} = 0.4+{0.7 \over \exp [(V+5-V_{sh}) / 15]+\exp [-(V+5-V_{sh}) / 15]} \\
+      {dq \over dt} &= {\phi_q \cdot (q_{\infty}-q) \over \tau_q} \\
+      & q_{\infty} = {1 \over 1+\exp [(V+25-V_{sh}) / 2]} \\
+      & \tau_q = 300 + {100 \over \exp [(V+40-V_{sh}) / 9.5]+\exp [-(V+40-V_{sh}) / 9.5]}
+
+  where :math:`phi_p = 3.55^{\frac{T-24}{10}}` and :math:`phi_q = 3^{\frac{T-24}{10}}`
+  are temperature-dependent factors (:math:`T` is the temperature in Celsius),
+  :math:`E_{Ca}` is the reversal potential of Calcium channel.
+
+  Parameters
+  ----------
+  T : float
+    The temperature.
+  T_base_p : float
+    The brainpy_object temperature factor of :math:`p` channel.
+  T_base_q : float
+    The brainpy_object temperature factor of :math:`q` channel.
+  g_max : float
+    The maximum conductance.
+  V_sh : float
+    The membrane potential shift.
+
+  References
+  ----------
+
+  .. [1] Inoue, Tsuyoshi, and Ben W. Strowbridge. "Transient activity induces a long-lasting
+         increase in the excitability of olfactory bulb interneurons." Journal of
+         neurophysiology 99, no. 1 (2008): 187-199.
+
+  See Also
+  --------
+  ICa_p2q_form
   """
-  supported_modes = (bm.NonBatchingMode,)
 
   def __init__(
       self,
-      size: Union[int, Sequence[int]],
+      size: Shape,
       keep_size: bool = False,
-      sharding: Optional[Sequence[str]] = None,
+      T: Union[float, ArrayType, Initializer, Callable] = 36.,
+      T_base_p: Union[float, ArrayType, Initializer, Callable] = 3.55,
+      T_base_q: Union[float, ArrayType, Initializer, Callable] = 3.,
+      g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
+      V_sh: Union[float, ArrayType, Initializer, Callable] = 0.,
       method: str = 'exp_auto',
       name: Optional[str] = None,
       mode: Optional[bm.Mode] = None,
-
-      # synapse parameters
-      alpha1: Union[float, ArrayType, Callable] = 2.,
-      beta1: Union[float, ArrayType, Callable] = 0.01,
-      alpha2: Union[float, ArrayType, Callable] = 1.,
-      beta2: Union[float, ArrayType, Callable] = 0.5,
-      T: Union[float, ArrayType, Callable] = 1.,
-      T_dur: Union[float, ArrayType, Callable] = 0.5,
   ):
-    super().__init__(name=name,
-                     mode=mode,
-                     size=size,
+    super().__init__(size,
                      keep_size=keep_size,
-                     sharding=sharding)
+                     name=name,
+                     method=method,
+                     g_max=g_max,
+                     phi_p=T_base_p ** ((T - 24) / 10),
+                     phi_q=T_base_q ** ((T - 24) / 10),
+                     mode=mode)
 
     # parameters
-    self.beta1 = self.init_param(beta1)
-    self.beta2 = self.init_param(beta2)
-    self.alpha1 = self.init_param(alpha1)
-    self.alpha2 = self.init_param(alpha2)
-    self.T = self.init_param(T)
-    self.T_dur = self.init_param(T_dur)
-
-    # integral
-    self.integral = odeint(method=method, f=JointEq([self.dg, self.dx]))
-
-    self.reset_state(self.mode)
-
-  def reset_state(self, batch_size=None):
-    self.g = self.init_variable(bm.zeros, batch_size)
-    self.x = self.init_variable(bm.zeros, batch_size)
-    self.spike_arrival_time = self.init_variable(bm.ones, batch_size)
-    self.spike_arrival_time.fill(-1e7)
-
-  def dg(self, g, t, x):
-    return self.alpha1 * x * (1 - g) - self.beta1 * g
-
-  def dx(self, x, t, T):
-    return self.alpha2 * T * (1 - x) - self.beta2 * x
-
-  def update(self, pre_spike):
-    t = share.load('t')
-    dt = share.load('dt')
-    self.spike_arrival_time.value = bm.where(pre_spike, t, self.spike_arrival_time)
-    T = ((t - self.spike_arrival_time) < self.T_dur) * self.T
-    self.g.value, self.x.value = self.integral(self.g, self.x, t, T, dt)
-    return self.g.value
-
-  def return_for_delay(self):
-    return self.g
-
-BioNMDA.__doc__ = BioNMDA.__doc__ % (pneu_doc,)
+    self.T = parameter(T, self.varshape, allow_none=False)
+    self.T_base_p = parameter(T_base_p, self.varshape, allow_none=False)
+    self.T_base_q = parameter(T_base_q, self.varshape, allow_none=False)
+    self.V_sh = parameter(V_sh, self.varshape, allow_none=False)
+
+  def f_p_inf(self, V):
+    return 1. / (1 + bm.exp(-(V + 10. - self.V_sh) / 4.))
+
+  def f_p_tau(self, V):
+    return 0.4 + .7 / (bm.exp(-(V + 5. - self.V_sh) / 15.) +
+                       bm.exp((V + 5. - self.V_sh) / 15.))
+
+  def f_q_inf(self, V):
+    return 1. / (1. + bm.exp((V + 25. - self.V_sh) / 2.))
+
+  def f_q_tau(self, V):
+    return 300. + 100. / (bm.exp((V + 40 - self.V_sh) / 9.5) +
+                          bm.exp(-(V + 40 - self.V_sh) / 9.5))
```

## Comparing `brainpy/_src/dyn/synapses/outputs.py` & `brainpy/_src/dyn/outs/outputs.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,15 +1,14 @@
-
 from typing import Union, Optional, Sequence
 
 import numpy as np
 
 from brainpy import math as bm, initialize as init
-from brainpy._src.dyn.base import SynOut
 from brainpy.types import ArrayType
+from .base import SynOut
 
 __all__ = [
   'COBA',
   'CUBA',
   'MgBlock'
 ]
 
@@ -23,25 +22,27 @@
 
      I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t))
 
   Parameters
   ----------
   E: float, ArrayType, ndarray
     The reversal potential.
+  sharding: sequence of str
+    The axis names for variable for parallelization.
   name: str
     The model name.
 
   See Also
   --------
   CUBA
   """
 
   def __init__(
       self,
-      E: Union[float, ArrayType] = 0.,
+      E: Union[float, ArrayType],
       sharding: Optional[Sequence[str]] = None,
       name: Optional[str] = None,
   ):
     super().__init__(name=name)
 
     self.sharding = sharding
     self.E = init.parameter(E, np.shape(E), sharding=sharding)
@@ -60,26 +61,18 @@
      I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t)
 
   Parameters
   ----------
   name: str
     The model name.
 
-
   See Also
   --------
   COBA
   """
-
-  def __init__(
-      self,
-      name: Optional[str] = None,
-  ):
-    super().__init__(name=name)
-
   def update(self, conductance, potential=None):
     return conductance
 
 
 class MgBlock(SynOut):
   r"""Synaptic output based on Magnesium blocking.
 
@@ -103,14 +96,16 @@
     The reversal potential for the synaptic current. [mV]
   alpha: float, ArrayType
     Binding constant. Default 0.062
   beta: float, ArrayType
     Unbinding constant. Default 3.57
   cc_Mg: float, ArrayType
     Concentration of Magnesium ion. Default 1.2 [mM].
+  sharding: sequence of str
+    The axis names for variable for parallelization.
   name: str
     The model name.
   """
   def __init__(
       self,
       E: Union[float, ArrayType] = 0.,
       cc_Mg: Union[float, ArrayType] = 1.2,
```

## Comparing `brainpy/_src/neurons/fractional_models.py` & `brainpy/_src/dynold/neurons/fractional_models.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 # -*- coding: utf-8 -*-
 
 from typing import Union, Sequence, Callable
 
 import jax.numpy as jnp
+
 import brainpy.math as bm
-from brainpy._src.dynsys import NeuGroupNS
 from brainpy._src.context import share
+from brainpy._src.dyn.base import NeuDyn
 from brainpy._src.initialize import ZeroInit, OneInit, Initializer, parameter
 from brainpy._src.integrators.fde import CaputoL1Schema
 from brainpy._src.integrators.fde import GLShortMemory
 from brainpy._src.integrators.joint_eq import JointEq
 from brainpy.check import is_float, is_integer, is_initializer
 from brainpy.types import Shape, ArrayType
 
 __all__ = [
   'FractionalNeuron',
   'FractionalFHR',
   'FractionalIzhikevich',
 ]
 
 
-class FractionalNeuron(NeuGroupNS):
+class FractionalNeuron(NeuDyn):
   """Fractional-order neuron model."""
   pass
 
 
 class FractionalFHR(FractionalNeuron):
   r"""The fractional-order FH-R model [1]_.
 
@@ -314,23 +315,21 @@
     return dudt / self.tau
 
   @property
   def derivative(self):
     return JointEq(self.dV, self.du)
 
   def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
     if self.input_var:
       if x is not None:
         self.input += x
       x = self.input.value
     else:
       x = 0. if x is None else x
-    V, u = self.integral(self.V, self.u, t=t, I_ext=x, dt=dt)
+    V, u = self.integral(self.V, self.u, t=share['t'], I_ext=x, dt=share['dt'])
     spikes = V >= self.V_th
     self.V.value = jnp.where(spikes, self.c, V)
     self.u.value = jnp.where(spikes, u + self.d, u)
     self.spike.value = spikes
     return spikes
 
   def clear_input(self):
```

## Comparing `brainpy/_src/neurons/noise_groups.py` & `brainpy/_src/dyn/others/noise.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,25 +1,24 @@
-# -*- coding: utf-8 -*-
-
 from typing import Union, Callable
 
 import jax.numpy as jnp
+
+import brainpy.math as bm
 from brainpy._src.context import share
-from brainpy import math as bm, initialize as init
-from brainpy._src.dynsys import NeuGroupNS
-from brainpy._src.initialize import Initializer
+from brainpy._src.dyn.base import NeuDyn
+from brainpy._src.initialize import variable_, parameter
 from brainpy._src.integrators.sde.generic import sdeint
-from brainpy.types import ArrayType, Shape
+from brainpy.types import Shape, ArrayType
 
 __all__ = [
   'OUProcess',
 ]
 
 
-class OUProcess(NeuGroupNS):
+class OUProcess(NeuDyn):
   r"""The OrnsteinUhlenbeck process.
 
   The OrnsteinUhlenbeck process :math:`x_{t}` is defined by the following
   stochastic differential equation:
 
   .. math::
 
@@ -43,43 +42,42 @@
   name: str
     The model name.
   """
 
   def __init__(
       self,
       size: Shape,
-      mean: Union[float, ArrayType, Initializer, Callable] = 0.,
-      sigma: Union[float, ArrayType, Initializer, Callable] = 1.,
-      tau: Union[float, ArrayType, Initializer, Callable] = 10.,
+      mean: Union[float, ArrayType, Callable] = 0.,
+      sigma: Union[float, ArrayType, Callable] = 1.,
+      tau: Union[float, ArrayType, Callable] = 10.,
       method: str = 'exp_euler',
       keep_size: bool = False,
       mode: bm.Mode = None,
       name: str = None,
   ):
     super(OUProcess, self).__init__(size=size, name=name, keep_size=keep_size, mode=mode)
 
     # parameters
-    self.mean = init.parameter(mean, self.varshape, allow_none=False)
-    self.sigma = init.parameter(sigma, self.varshape, allow_none=False)
-    self.tau = init.parameter(tau, self.varshape, allow_none=False)
+    self.mean = parameter(mean, self.varshape, allow_none=False)
+    self.sigma = parameter(sigma, self.varshape, allow_none=False)
+    self.tau = parameter(tau, self.varshape, allow_none=False)
 
     # variables
     self.reset_state(self.mode)
 
     # integral functions
     self.integral = sdeint(f=self.df, g=self.dg, method=method)
 
   def reset_state(self, batch_size=None):
-    self.x = init.variable_(lambda s: jnp.ones(s) * self.mean, self.varshape, batch_size)
+    self.x = variable_(lambda s: jnp.ones(s) * self.mean, self.varshape, batch_size)
 
   def df(self, x, t):
     return (self.mean - x) / self.tau
 
   def dg(self, x, t):
     return self.sigma
 
   def update(self):
     t = share.load('t')
     dt = share.load('dt')
     self.x.value = self.integral(self.x, t, dt)
     return self.x.value
-
```

## Comparing `brainpy/_src/neurons/reduced_models.py` & `brainpy/_src/dynold/neurons/reduced_models.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # -*- coding: utf-8 -*-
 
-from functools import partial
 from typing import Union, Callable, Optional
 
 from jax.lax import stop_gradient
 
 import brainpy.math as bm
-from brainpy._src.dynsys import NeuGroupNS
 from brainpy._src.context import share
+from brainpy._src.dyn.neurons import lif
+from brainpy._src.dyn.base import NeuDyn
 from brainpy._src.initialize import (ZeroInit,
                                      OneInit,
                                      Initializer,
                                      parameter,
                                      variable_,
                                      noise as init_noise)
 from brainpy._src.integrators import sdeint, odeint, JointEq
@@ -29,167 +29,15 @@
   'ALIFBellec2020',
   'Izhikevich',
   'HindmarshRose',
   'FHN',
 ]
 
 
-class Leaky(NeuGroupNS):
-  r"""Leaky Integrator Model.
-  
-  **Model Descriptions**
-  
-  This class implements a leaky model, in which its dynamics is
-  given by:
-  
-  .. math::
-  
-     x(t + \Delta t) = \exp{-1/\tau \Delta t} x(t) + I
-
-  Parameters
-  ----------
-  size: sequence of int, int
-    The size of the neuron group.
-  tau: float, ArrayType, Initializer, callable
-    Membrane time constant.
-  method: str
-    The numerical integration method.
-  name: str
-    The group name.
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      tau: Union[float, ArrayType, Initializer, Callable] = 10.,
-      name: str = None,
-      mode: bm.Mode = None,
-      method: str = 'exp_auto',
-  ):
-    super().__init__(size=size,
-                     mode=mode,
-                     keep_size=keep_size,
-                     name=name)
-    assert self.mode.is_parent_of(bm.TrainingMode, bm.NonBatchingMode)
-
-    # parameters
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-
-    # integral
-    self.integral = odeint(method=method, f=self.derivative)
-
-    # variables
-    self.reset_state(self.mode)
-
-  def derivative(self, x, t):
-    return -x / self.tau
-
-  def reset_state(self, batch_size=None):
-    self.x = variable_(bm.zeros, self.varshape, batch_size)
-
-  def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
-    r = self.integral(self.x.value, t, dt)
-    if x is not None:
-      r += x
-    self.x.value = r
-    return r
-
-
-class Integrator(NeuGroupNS):
-  r"""Integrator Model.
-
-  This class implements an integrator model, in which its dynamics is
-  given by:
-
-  .. math::
-
-     \tau \frac{dx}{dt} = - x(t) + I(t)
-
-  where :math:`x` is the integrator value, and :math:`\tau` is the time constant.
-
-  Parameters
-  ----------
-  size: sequence of int, int
-    The size of the neuron group.
-  tau: float, ArrayType, Initializer, callable
-    Membrane time constant.
-  x_initializer: ArrayType, Initializer, callable
-    The initializer of :math:`x`.
-  noise: ArrayType, Initializer, callable
-    The noise added onto the membrane potential
-  method: str
-    The numerical integration method.
-  name: str
-    The group name.
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-      tau: Union[float, ArrayType, Initializer, Callable] = 10.,
-      x_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      noise: Union[float, ArrayType, Initializer, Callable] = None,
-      input_var: bool = False,
-      name: str = None,
-      mode: bm.Mode = None,
-      method: str = 'exp_auto',
-  ):
-    super().__init__(size=size,
-                     mode=mode,
-                     keep_size=keep_size,
-                     name=name)
-    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode))
-
-    # parameters
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-    self.noise = init_noise(noise, self.varshape)
-    self.input_var = input_var
-
-    # initializers
-    self._x_initializer = is_initializer(x_initializer)
-
-    # integral
-    if self.noise is None:
-      self.integral = odeint(method=method, f=self.derivative)
-    else:
-      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
-
-    # variables
-    self.reset_state(self.mode)
-
-  def derivative(self, V, t, I_ext):
-    return (-V + I_ext) / self.tau
-
-  def reset_state(self, batch_size=None):
-    self.x = variable_(self._x_initializer, self.varshape, batch_size)
-    if self.input_var:
-      self.input = variable_(bm.zeros, self.varshape, batch_size)
-
-  def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
-    if self.input_var:
-      if x is not None:
-        self.input += x
-      x = self.input.value
-    else:
-      x = 0. if x is None else x
-    self.x.value = self.integral(self.x.value, t, I_ext=x, dt=dt)
-    return self.x.value
-
-  def clear_input(self):
-    if self.input_var:
-      self.input[:] = 0.
-
-
-class LeakyIntegrator(NeuGroupNS):
+class LeakyIntegrator(NeuDyn):
   r"""Leaky Integrator Model.
 
   **Model Descriptions**
 
   This class implements a leaky integrator model, in which its dynamics is
   given by:
 
@@ -287,15 +135,15 @@
     return self.V.value
 
   def clear_input(self):
     if self.input_var:
       self.input[:] = 0.
 
 
-class LIF(NeuGroupNS):
+class LIF(lif.LifRef):
   r"""Leaky integrate-and-fire neuron model.
 
   **Model Descriptions**
 
   The formal equations of a LIF model [1]_ is given by:
 
   .. math::
@@ -345,141 +193,46 @@
 
   .. [1] Abbott, Larry F. "Lapicques introduction of the integrate-and-fire model
          neuron (1907)." Brain research bulletin 50, no. 5-6 (1999): 303-304.
   """
 
   def __init__(
       self,
-      size: Shape,
-      keep_size: bool = False,
-
-      # neuron parameter
-      V_rest: Union[float, ArrayType, Initializer, Callable] = 0.,
-      V_reset: Union[float, ArrayType, Initializer, Callable] = -5.,
-      V_th: Union[float, ArrayType, Initializer, Callable] = 20.,
-      R: Union[float, ArrayType, Initializer, Callable] = 1.,
-      tau: Union[float, ArrayType, Initializer, Callable] = 10.,
-      tau_ref: Optional[Union[float, ArrayType, Initializer, Callable]] = None,
-      V_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      noise: Optional[Union[float, ArrayType, Initializer, Callable]] = None,
-
-      # training parameter
-      mode: Optional[bm.Mode] = None,
-      spike_fun: Callable = bm.surrogate.inv_square_grad,
-
-      # other parameters
+      *args,
       input_var: bool = True,
-      ref_var: bool = False,
-      method: str = 'exp_auto',
-      name: Optional[str] = None,
+      noise: Optional[Union[float, ArrayType, Initializer, Callable]] = None,
+      **kwargs,
   ):
-    # initialization
-    super().__init__(size=size,
-                     name=name,
-                     keep_size=keep_size,
-                     mode=mode)
-    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode), self.name)
-
-    # parameters
-    self.V_rest = parameter(V_rest, self.varshape, allow_none=False)
-    self.V_reset = parameter(V_reset, self.varshape, allow_none=False)
-    self.V_th = parameter(V_th, self.varshape, allow_none=False)
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-    self.R = parameter(R, self.varshape, allow_none=False)
-    self.tau_ref = parameter(tau_ref, self.varshape, allow_none=True)
-    self.noise = init_noise(noise, self.varshape)
-    self.spike_fun = is_callable(spike_fun, 'spike_fun')
     self.input_var = input_var
-    self.ref_var = ref_var
-
-    # initializers
-    self._V_initializer = is_initializer(V_initializer)
-
-    # variables
+    super().__init__(*args, **kwargs, init_var=False)
+    self.noise = init_noise(noise, self.varshape)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
     self.reset_state(self.mode)
 
-    # integral
-    if self.noise is None:
-      self.integral = odeint(method=method, f=self.derivative)
-    else:
-      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
-
-  def derivative(self, V, t, I):
-    return (-V + self.V_rest + self.R * I) / self.tau
-
   def reset_state(self, batch_size=None):
-    self.V = variable_(self._V_initializer, self.varshape, batch_size)
+    super().reset_state(batch_size)
     if self.input_var:
       self.input = variable_(bm.zeros, self.varshape, batch_size)
-    sp_type = bm.float_ if isinstance(self.mode, bm.TrainingMode) else bool  # the gradient of spike is a float
-    self.spike = variable_(lambda s: bm.zeros(s, dtype=sp_type), self.varshape, batch_size)
-    if self.tau_ref is not None:
-      self.t_last_spike = variable_(lambda s: bm.ones(s) * -1e7, self.varshape, batch_size)
-      if self.ref_var:
-        self.refractory = variable_(lambda s: bm.zeros(s, dtype=bool), self.varshape, batch_size)
 
   def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
     if self.input_var:
       if x is not None:
         self.input += x
       x = self.input.value
     else:
       x = 0. if x is None else x
-
-    # integrate membrane potential
-    V = self.integral(self.V.value, t, x, dt)
-
-    if self.tau_ref is not None:
-      # refractory
-      refractory = (t - self.t_last_spike) <= self.tau_ref
-      if isinstance(self.mode, bm.TrainingMode):
-        refractory = stop_gradient(refractory)
-      V = bm.where(refractory, self.V.value, V)
-
-      # spike, refractory, spiking time, and membrane potential reset
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun(V - self.V_th)
-        spike_no_grad = stop_gradient(spike)
-        V += (self.V_reset - V) * spike_no_grad
-        spike_ = spike_no_grad > 0.
-        # will be used in other place, like Delta Synapse, so stop its gradient
-        if self.ref_var:
-          self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
-        t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
-      else:
-        spike = V >= self.V_th
-        V = bm.where(spike, self.V_reset, V)
-        if self.ref_var:
-          self.refractory.value = bm.logical_or(refractory, spike)
-        t_last_spike = bm.where(spike, t, self.t_last_spike.value)
-      self.V.value = V
-      self.spike.value = spike
-      self.t_last_spike.value = t_last_spike
-
-    else:
-      # spike, spiking time, and membrane potential reset
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun(V - self.V_th)
-        spike_no_grad = stop_gradient(spike)
-        V += (self.V_reset - V) * spike_no_grad
-      else:
-        spike = V >= self.V_th
-        V = bm.where(spike, self.V_reset, V)
-      self.V.value = V
-      self.spike.value = spike
-    return spike
+    return super().update(x)
 
   def clear_input(self):
     if self.input_var:
-      self.input[:] = 0.
+      self.input.value = bm.zeros_like(self.input)
 
 
-class ExpIF(NeuGroupNS):
+class ExpIF(lif.ExpIFRef):
   r"""Exponential integrate-and-fire neuron model.
 
   **Model Descriptions**
 
   In the exponential integrate-and-fire model [1]_, the differential
   equation for the membrane potential is given by
 
@@ -571,135 +324,46 @@
          integrate-and-fire neurons to modulated current-based and
          conductance-based synaptic drive." Physical Review E 76, no. 2 (2007): 021919.
   .. [5] https://en.wikipedia.org/wiki/Exponential_integrate-and-fire
   """
 
   def __init__(
       self,
-      size: Shape,
-      V_rest: Union[float, ArrayType, Initializer, Callable] = -65.,
-      V_reset: Union[float, ArrayType, Initializer, Callable] = -68.,
-      V_th: Union[float, ArrayType, Initializer, Callable] = -30.,
-      V_T: Union[float, ArrayType, Initializer, Callable] = -59.9,
-      delta_T: Union[float, ArrayType, Initializer, Callable] = 3.48,
-      R: Union[float, ArrayType, Initializer, Callable] = 1.,
-      tau: Union[float, ArrayType, Initializer, Callable] = 10.,
-      tau_ref: Union[float, ArrayType, Initializer, Callable] = None,
-      V_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      noise: Union[float, ArrayType, Initializer, Callable] = None,
-      spike_fun: Callable = bm.surrogate.inv_square_grad,
-      keep_size: bool = False,
+      *args,
       input_var: bool = True,
-      ref_var: bool = False,
-      mode: bm.Mode = None,
-      method: str = 'exp_auto',
-      name: str = None
+      noise: Union[float, ArrayType, Initializer, Callable] = None,
+      **kwargs,
   ):
-    # initialize
-    super(ExpIF, self).__init__(size=size,
-                                name=name,
-                                mode=mode,
-                                keep_size=keep_size, )
-    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode))
-
-    # parameters
-    self.V_rest = parameter(V_rest, self.varshape, allow_none=False)
-    self.V_reset = parameter(V_reset, self.varshape, allow_none=False)
-    self.V_th = parameter(V_th, self.varshape, allow_none=False)
-    self.V_T = parameter(V_T, self.varshape, allow_none=False)
-    self.delta_T = parameter(delta_T, self.varshape, allow_none=False)
-    self.tau_ref = parameter(tau_ref, self.varshape, allow_none=True)
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-    self.R = parameter(R, self.varshape, allow_none=False)
-    self.noise = init_noise(noise, self.varshape)
     self.input_var = input_var
-    self.ref_var = ref_var
-
-    # initializers
-    self._V_initializer = is_initializer(V_initializer)
-
-    # training setting
-    self.spike_fun = is_callable(spike_fun, 'spike_fun')
-
-    # variables
+    super().__init__(*args, **kwargs, init_var=False)
+    self.noise = init_noise(noise, self.varshape)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
     self.reset_state(self.mode)
 
-    # integral
-    if self.noise is None:
-      self.integral = odeint(method=method, f=self.derivative)
-    else:
-      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
-
   def reset_state(self, batch_size=None):
-    self.V = variable_(self._V_initializer, self.varshape, batch_size)
+    super().reset_state(batch_size)
     if self.input_var:
       self.input = variable_(bm.zeros, self.varshape, batch_size)
-    sp_type = bm.float_ if isinstance(self.mode, bm.TrainingMode) else bool
-    self.spike = variable_(lambda s: bm.zeros(s, dtype=sp_type), self.varshape, batch_size)
-    if self.tau_ref is not None:
-      self.t_last_spike = variable_(lambda s: bm.ones(s) * -1e7, self.varshape, batch_size)
-      if self.ref_var:
-        self.refractory = variable_(lambda s: bm.zeros(s, dtype=bool), self.varshape, batch_size)
-
-  def derivative(self, V, t, I_ext):
-    exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T)
-    dvdt = (- (V - self.V_rest) + exp_v + self.R * I_ext) / self.tau
-    return dvdt
 
   def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
     if self.input_var:
       if x is not None:
         self.input += x
       x = self.input.value
     else:
       x = 0. if x is None else x
-
-    V = self.integral(self.V.value, t, x, dt)
-
-    if self.tau_ref is not None:
-      refractory = (t - self.t_last_spike) <= self.tau_ref
-      if isinstance(self.mode, bm.TrainingMode):
-        refractory = stop_gradient(refractory)
-      V = bm.where(refractory, self.V.value, V)
-
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun(V - self.V_th)
-        spike_no_grad = stop_gradient(spike)
-        V += (self.V_reset - V) * spike_no_grad
-        spike_ = spike_no_grad > 0.
-        self.t_last_spike.value = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
-        if self.ref_var:
-        # will be used in other place, like Delta Synapse, so stop its gradient
-          self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
-      else:
-        spike = self.V_th <= V
-        V = bm.where(spike, self.V_reset, V)
-        self.t_last_spike.value = bm.where(spike, t, self.t_last_spike)
-        if self.ref_var:
-          self.refractory.value = bm.logical_or(refractory, spike)
-    else:
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun(V - self.V_th)
-        spike_no_grad = stop_gradient(spike)
-        V += (self.V_reset - V) * spike_no_grad
-      else:
-        spike = self.V_th <= V
-        V = bm.where(spike, self.V_reset, V)
-    self.V.value = V
-    self.spike.value = spike
-    return spike
+    return super().update(x)
 
   def clear_input(self):
     if self.input_var:
-      self.input[:] = 0.
+      self.input.value = bm.zeros_like(self.input)
 
 
-class AdExIF(NeuGroupNS):
+class AdExIF(lif.AdExIFRef):
   r"""Adaptive exponential integrate-and-fire neuron model.
 
   **Model Descriptions**
 
   The **adaptive exponential integrate-and-fire model**, also called AdEx, is a
   spiking neuron model with two variables [1]_ [2]_.
 
@@ -768,139 +432,46 @@
          mechanisms determine the neuronal response to fluctuating
          inputs." Journal of Neuroscience 23.37 (2003): 11628-11640.
   .. [2] http://www.scholarpedia.org/article/Adaptive_exponential_integrate-and-fire_model
   """
 
   def __init__(
       self,
-      size: Shape,
-      V_rest: Union[float, ArrayType, Initializer, Callable] = -65.,
-      V_reset: Union[float, ArrayType, Initializer, Callable] = -68.,
-      V_th: Union[float, ArrayType, Initializer, Callable] = -30.,
-      V_T: Union[float, ArrayType, Initializer, Callable] = -59.9,
-      delta_T: Union[float, ArrayType, Initializer, Callable] = 3.48,
-      a: Union[float, ArrayType, Initializer, Callable] = 1.,
-      b: Union[float, ArrayType, Initializer, Callable] = 1.,
-      tau: Union[float, ArrayType, Initializer, Callable] = 10.,
-      tau_w: Union[float, ArrayType, Initializer, Callable] = 30.,
-      tau_ref: Optional[Union[float, ArrayType, Initializer, Callable]] = None,
-      R: Union[float, ArrayType, Initializer, Callable] = 1.,
-      V_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      w_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      noise: Optional[Union[float, ArrayType, Initializer, Callable]] = None,
-      spike_fun: Callable = bm.surrogate.inv_square_grad,
-      method: str = 'exp_auto',
-      keep_size: bool = False,
+      *args,
       input_var: bool = True,
-      mode: bm.Mode = None,
-      name: Optional[str] = None
+      noise: Optional[Union[float, ArrayType, Initializer, Callable]] = None,
+      **kwargs,
   ):
-    super(AdExIF, self).__init__(size=size,
-                                 keep_size=keep_size,
-                                 name=name,
-                                 mode=mode, )
-    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode))
-
-    # parameters
-    self.V_rest = parameter(V_rest, self.varshape, allow_none=False)
-    self.V_reset = parameter(V_reset, self.varshape, allow_none=False)
-    self.V_th = parameter(V_th, self.varshape, allow_none=False)
-    self.V_T = parameter(V_T, self.varshape, allow_none=False)
-    self.a = parameter(a, self.varshape, allow_none=False)
-    self.b = parameter(b, self.varshape, allow_none=False)
-    self.R = parameter(R, self.varshape, allow_none=False)
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-    self.tau_w = parameter(tau_w, self.varshape, allow_none=False)
-    self.tau_ref = parameter(tau_ref, self.varshape, allow_none=True)
-    self.delta_T = parameter(delta_T, self.varshape, allow_none=False)
-    self.noise = init_noise(noise, self.varshape, num_vars=2)
     self.input_var = input_var
-    self.spike_fun = is_callable(spike_fun, 'spike_fun')
-
-    # initializers
-    self._V_initializer = is_initializer(V_initializer)
-    self._w_initializer = is_initializer(w_initializer)
-
-    # variables
+    super().__init__(*args, **kwargs, init_var=False)
+    self.noise = init_noise(noise, self.varshape, num_vars=2)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
     self.reset_state(self.mode)
 
-    # functions
-    if self.noise is None:
-      self.integral = odeint(method=method, f=self.derivative)
-    else:
-      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
-
   def reset_state(self, batch_size=None):
-    self.V = variable_(self._V_initializer, self.varshape, batch_size)
-    self.w = variable_(self._w_initializer, self.varshape, batch_size)
+    super().reset_state(batch_size)
     if self.input_var:
       self.input = variable_(bm.zeros, self.varshape, batch_size)
-    sp_type = bm.float_ if isinstance(self.mode, bm.BatchingMode) else bool
-    self.spike = variable_(lambda s: bm.zeros(s, dtype=sp_type), self.varshape, batch_size)
-    if self.tau_ref is not None:
-      self.refractory = variable_(partial(bm.zeros, dtype=bool), self.varshape, batch_size)
-      self.t_last_spike = variable_(lambda s: bm.ones(s) * -1e8, self.varshape, batch_size)
-
-  def dV(self, V, t, w, I_ext):
-    exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T)
-    dVdt = (- V + self.V_rest + exp - self.R * w + self.R * I_ext) / self.tau
-    return dVdt
-
-  def dw(self, w, t, V):
-    dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w
-    return dwdt
-
-  @property
-  def derivative(self):
-    return JointEq([self.dV, self.dw])
 
   def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
     if self.input_var:
       if x is not None:
         self.input += x
       x = self.input.value
     else:
       x = 0. if x is None else x
-
-    V, w = self.integral(self.V.value, self.w.value, t, x, dt)
-
-    if self.tau_ref is not None:
-      refractory = (t - self.t_last_spike) <= self.tau_ref
-      if isinstance(self.mode, bm.TrainingMode):
-        refractory = stop_gradient(refractory)
-      V = bm.where(refractory, self.V.value, V)
-
-    if isinstance(self.mode, bm.TrainingMode):
-      spike = self.spike_fun(V - self.V_th)
-      spike_no_grad = stop_gradient(spike)
-      V += (self.V_reset - V) * spike_no_grad
-      w += self.b * spike_no_grad
-      spike_ = spike_no_grad > 0.
-      if self.tau_ref is not None:
-        self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
-        self.t_last_spike.value = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
-    else:
-      spike = V >= self.V_th
-      self.V.value = bm.where(spike, self.V_reset, V)
-      self.w.value = bm.where(spike, w + self.b, w)
-      self.spike.value = spike
-      if self.tau_ref is not None:
-        self.refractory.value = bm.logical_or(refractory, spike)
-        self.t_last_spike.value = bm.where(spike, t, self.t_last_spike.value)
-
-    return spike
+    return super().update(x)
 
   def clear_input(self):
     if self.input_var:
-      self.input[:] = 0.
+      self.input.value = bm.zeros_like(self.input)
 
 
-class QuaIF(NeuGroupNS):
+class QuaIF(lif.QuaIFRef):
   r"""Quadratic Integrate-and-Fire neuron model.
 
   **Model Descriptions**
 
   In contrast to physiologically accurate but computationally expensive
   neuron models like the HodgkinHuxley model, the QIF model [1]_ seeks only
   to produce **action potential-like patterns** and ignores subtleties
@@ -961,126 +532,46 @@
   .. [1]  P. E. Latham, B.J. Richmond, P. Nelson and S. Nirenberg
           (2000) Intrinsic dynamics in neuronal networks. I. Theory.
           J. Neurophysiology 83, pp. 808827.
   """
 
   def __init__(
       self,
-      size: Shape,
-      V_rest: Union[float, ArrayType, Initializer, Callable] = -65.,
-      V_reset: Union[float, ArrayType, Initializer, Callable] = -68.,
-      V_th: Union[float, ArrayType, Initializer, Callable] = -30.,
-      V_c: Union[float, ArrayType, Initializer, Callable] = -50.0,
-      c: Union[float, ArrayType, Initializer, Callable] = .07,
-      R: Union[float, ArrayType, Initializer, Callable] = 1.,
-      tau: Union[float, ArrayType, Initializer, Callable] = 10.,
-      tau_ref: Union[float, ArrayType, Initializer, Callable] = None,
-      V_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      noise: Union[float, ArrayType, Initializer, Callable] = None,
-      spike_fun: Callable = bm.surrogate.inv_square_grad,
-      keep_size: bool = False,
+      *args,
       input_var: bool = True,
-      mode: bm.Mode = None,
-      method: str = 'exp_auto',
-      name: str = None
+      noise: Union[float, ArrayType, Initializer, Callable] = None,
+      **kwargs,
   ):
-    # initialization
-    super(QuaIF, self).__init__(size=size,
-                                keep_size=keep_size,
-                                name=name,
-                                mode=mode)
-    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode))
-
-    # parameters
-    self.V_rest = parameter(V_rest, self.varshape, allow_none=False)
-    self.V_reset = parameter(V_reset, self.varshape, allow_none=False)
-    self.V_th = parameter(V_th, self.varshape, allow_none=False)
-    self.V_c = parameter(V_c, self.varshape, allow_none=False)
-    self.c = parameter(c, self.varshape, allow_none=False)
-    self.R = parameter(R, self.varshape, allow_none=False)
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-    self.tau_ref = parameter(tau_ref, self.varshape, allow_none=True)
-    self.noise = init_noise(noise, self.varshape, num_vars=1)
     self.input_var = input_var
-    self.spike_fun = is_callable(spike_fun, 'spike_fun')
-
-    # initializers
-    self._V_initializer = is_initializer(V_initializer)
-
-    # variables
+    super().__init__(*args, **kwargs, init_var=False)
+    self.noise = init_noise(noise, self.varshape, num_vars=1)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
     self.reset_state(self.mode)
 
-    # integral
-    if self.noise is None:
-      self.integral = odeint(method=method, f=self.derivative)
-    else:
-      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
-
   def reset_state(self, batch_size=None):
-    self.V = variable_(self._V_initializer, self.varshape, batch_size)
+    super().reset_state(batch_size)
     if self.input_var:
       self.input = variable_(bm.zeros, self.varshape, batch_size)
-    sp_type = bm.float_ if isinstance(self.mode, bm.TrainingMode) else bool
-    self.spike = variable_(lambda s: bm.zeros(s, dtype=sp_type), self.varshape, batch_size)
-    if self.tau_ref is not None:
-      self.t_last_spike = variable_(lambda s: bm.ones(s) * -1e7, self.varshape, batch_size)
-      self.refractory = variable_(lambda s: bm.zeros(s, dtype=bool), self.varshape, batch_size)
-
-  def derivative(self, V, t, I_ext):
-    dVdt = (self.c * (V - self.V_rest) * (V - self.V_c) + self.R * I_ext) / self.tau
-    return dVdt
 
   def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
     if self.input_var:
       if x is not None:
         self.input += x
       x = self.input.value
     else:
       x = 0. if x is None else x
-
-    V = self.integral(self.V.value, t, x, dt)
-
-    if self.tau_ref is not None:
-      refractory = (t - self.t_last_spike) <= self.tau_ref
-      if isinstance(self.mode, bm.TrainingMode):
-        refractory = stop_gradient(refractory)
-      V = bm.where(refractory, self.V.value, V)
-
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun(V - self.V_th)
-        spike_no_grad = stop_gradient(spike)
-        V += (self.V_reset - V) * spike_no_grad
-        spike_ = spike_no_grad > 0.
-        self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_).value)
-        self.t_last_spike.value = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
-      else:
-        spike = self.V_th <= V
-        t_last_spike = bm.where(spike, t, self.t_last_spike.value)
-        V = bm.where(spike, self.V_reset, V)
-        self.refractory.value = bm.logical_or(refractory, spike)
-        self.t_last_spike.value = t_last_spike
-    else:
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun(V - self.V_th)
-        spike_no_grad = stop_gradient(spike)
-        V += (self.V_reset - V) * spike_no_grad
-      else:
-        spike = self.V_th <= V
-        V = bm.where(spike, self.V_reset, V)
-    self.V.value = V
-    self.spike.value = spike
+    return super().update(x)
 
   def clear_input(self):
     if self.input_var:
-      self.input[:] = 0.
+      self.input.value = bm.zeros_like(self.input)
 
 
-class AdQuaIF(NeuGroupNS):
+class AdQuaIF(lif.AdQuaIFRef):
   r"""Adaptive quadratic integrate-and-fire neuron model.
 
   **Model Descriptions**
 
   The adaptive quadratic integrate-and-fire neuron model [1]_ is given by:
 
   .. math::
@@ -1151,117 +642,46 @@
   .. [2] Touboul, Jonathan. "Bifurcation analysis of a general class of
          nonlinear integrate-and-fire neurons." SIAM Journal on Applied
          Mathematics 68, no. 4 (2008): 1045-1079.
   """
 
   def __init__(
       self,
-      size: Shape,
-      V_rest: Union[float, ArrayType, Initializer, Callable] = -65.,
-      V_reset: Union[float, ArrayType, Initializer, Callable] = -68.,
-      V_th: Union[float, ArrayType, Initializer, Callable] = -30.,
-      V_c: Union[float, ArrayType, Initializer, Callable] = -50.0,
-      a: Union[float, ArrayType, Initializer, Callable] = 1.,
-      b: Union[float, ArrayType, Initializer, Callable] = .1,
-      c: Union[float, ArrayType, Initializer, Callable] = .07,
-      tau: Union[float, ArrayType, Initializer, Callable] = 10.,
-      tau_w: Union[float, ArrayType, Initializer, Callable] = 10.,
-      V_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      w_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      noise: Union[float, ArrayType, Initializer, Callable] = None,
-      spike_fun: Callable = bm.surrogate.inv_square_grad,
-      method: str = 'exp_auto',
-      keep_size: bool = False,
+      *args,
       input_var: bool = True,
-      mode: bm.Mode = None,
-      name: str = None
+      noise: Union[float, ArrayType, Initializer, Callable] = None,
+      **kwargs,
   ):
-    super(AdQuaIF, self).__init__(size=size,
-                                  keep_size=keep_size,
-                                  name=name,
-                                  mode=mode, )
-    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode))
-
-    # parameters
-    self.V_rest = parameter(V_rest, self.varshape, allow_none=False)
-    self.V_reset = parameter(V_reset, self.varshape, allow_none=False)
-    self.V_th = parameter(V_th, self.varshape, allow_none=False)
-    self.V_c = parameter(V_c, self.varshape, allow_none=False)
-    self.c = parameter(c, self.varshape, allow_none=False)
-    self.a = parameter(a, self.varshape, allow_none=False)
-    self.b = parameter(b, self.varshape, allow_none=False)
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-    self.tau_w = parameter(tau_w, self.varshape, allow_none=False)
-    self.noise = init_noise(noise, self.varshape, num_vars=2)
     self.input_var = input_var
-    self.spike_fun = is_callable(spike_fun, 'spike_fun')
-
-    # initializers
-    self._V_initializer = is_initializer(V_initializer)
-    self._w_initializer = is_initializer(w_initializer)
-
-    # variables
+    super().__init__(*args, **kwargs, init_var=False)
+    self.noise = init_noise(noise, self.varshape, num_vars=2)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
     self.reset_state(self.mode)
 
-    # integral
-    if self.noise is None:
-      self.integral = odeint(method=method, f=self.derivative)
-    else:
-      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
-
   def reset_state(self, batch_size=None):
-    self.V = variable_(self._V_initializer, self.varshape, batch_size)
-    self.w = variable_(self._w_initializer, self.varshape, batch_size)
-    sp_type = bm.float_ if isinstance(self.mode, bm.TrainingMode) else bool
-    self.spike = variable_(lambda s: bm.zeros(s, dtype=sp_type), self.varshape, batch_size)
+    super().reset_state(batch_size)
     if self.input_var:
       self.input = variable_(bm.zeros, self.varshape, batch_size)
 
-  def dV(self, V, t, w, I_ext):
-    dVdt = (self.c * (V - self.V_rest) * (V - self.V_c) - w + I_ext) / self.tau
-    return dVdt
-
-  def dw(self, w, t, V):
-    dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w
-    return dwdt
-
-  @property
-  def derivative(self):
-    return JointEq([self.dV, self.dw])
-
   def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
     if self.input_var:
       if x is not None:
         self.input += x
       x = self.input.value
     else:
       x = 0. if x is None else x
-
-    V, w = self.integral(self.V.value, self.w.value, t, x, dt)
-
-    if isinstance(self.mode, bm.TrainingMode):
-      spike = self.spike_fun(V - self.V_th)
-      spike_no_grad = stop_gradient(spike)
-      V += (self.V_reset - V) * spike_no_grad
-      w += self.b * spike_no_grad
-    else:
-      spike = self.V_th <= V
-      self.V.value = bm.where(spike, self.V_reset, V)
-      self.w.value = bm.where(spike, w + self.b, w)
-    self.spike.value = spike
-    return spike
+    return super().update(x)
 
   def clear_input(self):
     if self.input_var:
-      self.input[:] = 0.
+      self.input.value = bm.zeros_like(self.input)
 
 
-class GIF(NeuGroupNS):
+class GIF(lif.GifRef):
   r"""Generalized Integrate-and-Fire model.
 
   **Model Descriptions**
 
   The generalized integrate-and-fire model [1]_ is given by
 
   .. math::
@@ -1337,312 +757,46 @@
          Gouwens, David Feng, Jim Berg, Aaron Szafer et al. "Generalized
          leaky integrate-and-fire models classify multiple neuron types."
          Nature communications 9, no. 1 (2018): 1-15.
   """
 
   def __init__(
       self,
-      size: Shape,
-      V_rest: Union[float, ArrayType, Initializer, Callable] = -70.,
-      V_reset: Union[float, ArrayType, Initializer, Callable] = -70.,
-      V_th_inf: Union[float, ArrayType, Initializer, Callable] = -50.,
-      V_th_reset: Union[float, ArrayType, Initializer, Callable] = -60.,
-      R: Union[float, ArrayType, Initializer, Callable] = 20.,
-      tau: Union[float, ArrayType, Initializer, Callable] = 20.,
-      a: Union[float, ArrayType, Initializer, Callable] = 0.,
-      b: Union[float, ArrayType, Initializer, Callable] = 0.01,
-      k1: Union[float, ArrayType, Initializer, Callable] = 0.2,
-      k2: Union[float, ArrayType, Initializer, Callable] = 0.02,
-      R1: Union[float, ArrayType, Initializer, Callable] = 0.,
-      R2: Union[float, ArrayType, Initializer, Callable] = 1.,
-      A1: Union[float, ArrayType, Initializer, Callable] = 0.,
-      A2: Union[float, ArrayType, Initializer, Callable] = 0.,
-      V_initializer: Union[Initializer, Callable, ArrayType] = OneInit(-70.),
-      I1_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      I2_initializer: Union[Initializer, Callable, ArrayType] = ZeroInit(),
-      Vth_initializer: Union[Initializer, Callable, ArrayType] = OneInit(-50.),
-      noise: Union[float, ArrayType, Initializer, Callable] = None,
-      method: str = 'exp_auto',
-      keep_size: bool = False,
+      *args,
       input_var: bool = True,
-      name: str = None,
-
-      # parameter for training
-      mode: bm.Mode = None,
-      spike_fun: Callable = bm.surrogate.sigmoid,
-  ):
-    # initialization
-    super().__init__(size=size,
-                     keep_size=keep_size,
-                     name=name,
-                     mode=mode)
-    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode))
-
-    # params
-    self.V_rest = parameter(V_rest, self.varshape, allow_none=False)
-    self.V_reset = parameter(V_reset, self.varshape, allow_none=False)
-    self.V_th_inf = parameter(V_th_inf, self.varshape, allow_none=False)
-    self.V_th_reset = parameter(V_th_reset, self.varshape, allow_none=False)
-    self.R = parameter(R, self.varshape, allow_none=False)
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-    self.a = parameter(a, self.varshape, allow_none=False)
-    self.b = parameter(b, self.varshape, allow_none=False)
-    self.k1 = parameter(k1, self.varshape, allow_none=False)
-    self.k2 = parameter(k2, self.varshape, allow_none=False)
-    self.R1 = parameter(R1, self.varshape, allow_none=False)
-    self.R2 = parameter(R2, self.varshape, allow_none=False)
-    self.A1 = parameter(A1, self.varshape, allow_none=False)
-    self.A2 = parameter(A2, self.varshape, allow_none=False)
-    self.noise = init_noise(noise, self.varshape, num_vars=4)
-    self.spike_fun = is_callable(spike_fun, 'spike_fun')
-    self.input_var = input_var
-
-    # initializers
-    self._V_initializer = is_initializer(V_initializer, 'V_initializer')
-    self._I1_initializer = is_initializer(I1_initializer, 'I1_initializer')
-    self._I2_initializer = is_initializer(I2_initializer, 'I2_initializer')
-    self._Vth_initializer = is_initializer(Vth_initializer, 'Vth_initializer')
-
-    # variables
-    self.reset_state(self.mode)
-
-    # integral
-    if self.noise is None:
-      self.integral = odeint(method=method, f=self.derivative)
-    else:
-      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
-
-  def reset_state(self, batch_size=None):
-    self.V = variable_(self._V_initializer, self.varshape, batch_size)
-    self.I1 = variable_(self._I1_initializer, self.varshape, batch_size)
-    self.I2 = variable_(self._I2_initializer, self.varshape, batch_size)
-    self.V_th = variable_(self._Vth_initializer, self.varshape, batch_size)
-    if self.input_var:
-      self.input = variable_(bm.zeros, self.varshape, batch_size)
-    sp_type = bm.float_ if self.mode.is_a(bm.TrainingMode) else bool
-    self.spike = variable_(lambda s: bm.zeros(s, dtype=sp_type), self.varshape, batch_size)
-
-  def dI1(self, I1, t):
-    return - self.k1 * I1
-
-  def dI2(self, I2, t):
-    return - self.k2 * I2
-
-  def dVth(self, V_th, t, V):
-    return self.a * (V - self.V_rest) - self.b * (V_th - self.V_th_inf)
-
-  def dV(self, V, t, I1, I2, I_ext):
-    return (- (V - self.V_rest) + self.R * (I_ext + I1 + I2)) / self.tau
-
-  @property
-  def derivative(self):
-    return JointEq(self.dI1, self.dI2, self.dVth, self.dV)
-
-  def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
-    if self.input_var:
-      if x is not None:
-        self.input += x
-      x = self.input.value
-    else:
-      x = 0. if x is None else x
-    I1, I2, V_th, V = self.integral(self.I1.value, self.I2.value, self.V_th.value, self.V.value, t, x, dt)
-
-    # spike and resets
-    if isinstance(self.mode, bm.TrainingMode):
-      spike = self.spike_fun(V - self.V_th)
-      V += (self.V_reset - V) * spike
-      I1 += spike * (self.R1 * I1 + self.A1 - I1)
-      I2 += spike * (self.R2 * I2 + self.A2 - I2)
-      reset_th = self.spike_fun(self.V_th_reset - V_th) * spike
-      V_th += reset_th * (self.V_th_reset - V_th)
-    else:
-      spike = self.V_th <= V
-      V = bm.where(spike, self.V_reset, V)
-      I1 = bm.where(spike, self.R1 * I1 + self.A1, I1)
-      I2 = bm.where(spike, self.R2 * I2 + self.A2, I2)
-      V_th = bm.where(spike, bm.maximum(self.V_th_reset, V_th), V_th)
-    self.spike.value = spike
-    self.I1.value = I1
-    self.I2.value = I2
-    self.V_th.value = V_th
-    self.V.value = V
-    return spike
-
-  def clear_input(self):
-    if self.input_var:
-      self.input[:] = 0.
-
-
-class ALIFBellec2020(NeuGroupNS):
-  r"""Leaky Integrate-and-Fire model with SFA [1]_.
-
-  This model is similar to the GLIF2 model in the Technical White Paper
-  on generalized LIF (GLIF) models from AllenInstitute [2]_.
-
-  Formally, this model is given by:
-
-  .. math::
-
-     \tau \dot{V} = -(V - V_{\mathrm{rest}}) + R*I \\
-     \tau_a \dot{a} = -a
-
-  Once a spike is induced by :math:`V(t) > V_{\mathrm{th}} + \beta a`, then
-
-  .. math::
-
-     V \gets V - V_{\mathrm{th}} \\
-     a \gets a + 1
-
-
-  References
-  ----------
-  .. [1] Bellec, Guillaume, et al. "A solution to the learning dilemma for
-         recurrent networks of spiking neurons."
-         Nature communications 11.1 (2020): 1-15.
-  .. [2] Allen Institute: Cell Types Database.  2018 Allen Institute for
-         Brain Science. Allen Cell Types Database, cell feature search.
-         Available from: celltypes.brain-map.org/data (2018).
-  """
-
-  def __init__(
-      self,
-      size: Shape,
-      keep_size: bool = False,
-
-      # model parameters
-      V_rest: Union[float, ArrayType, Initializer, Callable] = -70.,
-      V_th: Union[float, ArrayType, Initializer, Callable] = -60.,
-      R: Union[float, ArrayType, Initializer, Callable] = 1.,
-      beta: Union[float, ArrayType, Initializer, Callable] = 1.6,
-      tau: Union[float, ArrayType, Initializer, Callable] = 20.,
-      tau_a: Union[float, ArrayType, Initializer, Callable] = 2000.,
-      tau_ref: Union[float, ArrayType, Initializer, Callable] = None,
       noise: Union[float, ArrayType, Initializer, Callable] = None,
-
-      # initializers
-      V_initializer: Union[Initializer, Callable, ArrayType] = OneInit(-70.),
-      a_initializer: Union[Initializer, Callable, ArrayType] = OneInit(-50.),
-
-      # parameter for training
-      spike_fun: Callable = bm.surrogate.relu_grad,
-      input_var: bool = True,
-
-      # other parameters
-      method: str = 'exp_auto',
-      name: str = None,
-      mode: bm.Mode = None,
-      eprop: bool = False
+      **kwargs,
   ):
-    super().__init__(name=name,
-                     size=size,
-                     keep_size=keep_size,
-                     mode=mode)
-    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode))
-
-    # parameters
-    self.V_rest = parameter(V_rest, self.varshape, allow_none=False)
-    self.V_th = parameter(V_th, self.varshape, allow_none=False)
-    self.R = parameter(R, self.varshape, allow_none=False)
-    self.beta = parameter(beta, self.varshape, allow_none=False)
-    self.tau = parameter(tau, self.varshape, allow_none=False)
-    self.tau_a = parameter(tau_a, self.varshape, allow_none=False)
-    self.tau_ref = parameter(tau_ref, self.varshape, allow_none=True)
-    self.noise = init_noise(noise, self.varshape, num_vars=2)
-    self.spike_fun = is_callable(spike_fun, 'spike_fun')
-    self.eprop = eprop
     self.input_var = input_var
-
-    # initializers
-    self._V_initializer = is_initializer(V_initializer, 'V_initializer')
-    self._a_initializer = is_initializer(a_initializer, 'a_initializer')
-
-    # variables
+    super().__init__(*args, **kwargs, init_var=False)
+    self.noise = init_noise(noise, self.varshape, num_vars=4)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
     self.reset_state(self.mode)
 
-    # integral
-    if self.noise is None:
-      self.integral = odeint(method=method, f=self.derivative)
-    else:
-      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
-
-  def da(self, a, t):
-    return -a / self.tau_a
-
-  def dV(self, V, t, I_ext):
-    return (- (V - self.V_rest) + self.R * I_ext) / self.tau
-
-  @property
-  def derivative(self):
-    return JointEq([self.dV, self.da])
-
   def reset_state(self, batch_size=None):
-    self.a = variable_(self._a_initializer, self.varshape, batch_size)
-    self.V = variable_(self._V_initializer, self.varshape, batch_size)
+    super().reset_state(batch_size)
     if self.input_var:
       self.input = variable_(bm.zeros, self.varshape, batch_size)
-    sp_type = bm.float_ if isinstance(self.mode, bm.TrainingMode) else bool
-    self.spike = variable_(lambda s: bm.zeros(s, dtype=sp_type), self.varshape, batch_size)
-    if self.tau_ref is not None:
-      self.t_last_spike = variable_(lambda s: bm.ones(s) * -1e7, self.varshape, batch_size)
-      self.refractory = variable_(lambda s: bm.zeros(s, dtype=bool), self.varshape, batch_size)
 
   def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
     if self.input_var:
       if x is not None:
         self.input += x
       x = self.input.value
     else:
       x = 0. if x is None else x
-    V, a = self.integral(self.V, self.a, t, x, dt)
-
-    if self.tau_ref is not None:
-      # refractory
-      refractory = (t - self.t_last_spike) <= self.tau_ref
-      if isinstance(self.mode, bm.TrainingMode):
-        refractory = stop_gradient(refractory)
-      V = bm.where(refractory, self.V.value, V)
-      # spike and reset
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun((V - self.V_th - self.beta * self.a) / self.V_th)
-        V -= self.V_th * (stop_gradient(spike) if self.eprop else spike)
-        # will be used in other place, like Delta Synapse, so stop its gradient
-        spike_ = spike > 0.
-        refractory = stop_gradient(bm.logical_or(refractory, spike_))
-        t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
-      else:
-        spike = V >= (self.V_th + self.beta * self.a)
-        refractory = bm.logical_or(refractory, spike)
-        t_last_spike = bm.where(spike, t, self.t_last_spike.value)
-        V -= self.V_th * spike
-      self.refractory.value = refractory
-      self.t_last_spike.value = t_last_spike
-
-    else:
-      # spike and reset
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun((V - self.V_th - self.beta * self.a) / self.V_th)
-        V -= self.V_th * (stop_gradient(spike) if self.eprop else spike)
-      else:
-        spike = V >= (self.V_th + self.beta * self.a)
-        V -= self.V_th * spike
-    self.spike.value = spike
-    self.V.value = V
-    self.a.value = a + spike
-    return spike
+    return super().update(x)
 
   def clear_input(self):
     if self.input_var:
-      self.input[:] = 0.
+      self.input.value = bm.zeros_like(self.input)
 
 
-class Izhikevich(NeuGroupNS):
+class Izhikevich(lif.IzhikevichRef):
   r"""The Izhikevich neuron model.
 
   **Model Descriptions**
 
   The dynamics of the Izhikevich neuron model [1]_ [2]_ is given by:
 
   .. math ::
@@ -1704,144 +858,46 @@
 
   .. [2] Izhikevich, Eugene M. "Which model to use for cortical spiking neurons?."
          IEEE transactions on neural networks 15.5 (2004): 1063-1070.
   """
 
   def __init__(
       self,
-      size: Shape,
-      a: Union[float, ArrayType, Initializer, Callable] = 0.02,
-      b: Union[float, ArrayType, Initializer, Callable] = 0.20,
-      c: Union[float, ArrayType, Initializer, Callable] = -65.,
-      d: Union[float, ArrayType, Initializer, Callable] = 8.,
-      V_th: Union[float, ArrayType, Initializer, Callable] = 30.,
-      tau_ref: Union[float, ArrayType, Initializer, Callable] = None,
-      V_initializer: Union[Initializer, Callable, ArrayType] = None,
-      u_initializer: Union[Initializer, Callable, ArrayType] = None,
-      noise: Union[float, ArrayType, Initializer, Callable] = None,
-      method: str = 'exp_auto',
-      mode: bm.Mode = None,
-      spike_fun: Callable = bm.surrogate.inv_square_grad,
-      keep_size: bool = False,
+      *args,
       input_var: bool = True,
-      ref_var: bool = False,
-      name: str = None
+      noise: Union[float, ArrayType, Initializer, Callable] = None,
+      **kwargs,
   ):
-    # initialization
-    super().__init__(size=size,
-                     keep_size=keep_size,
-                     name=name,
-                     mode=mode)
-    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode))
-
-    # params
-    self.a = parameter(a, self.varshape, allow_none=False)
-    self.b = parameter(b, self.varshape, allow_none=False)
-    self.c = parameter(c, self.varshape, allow_none=False)
-    self.d = parameter(d, self.varshape, allow_none=False)
-    self.V_th = parameter(V_th, self.varshape, allow_none=False)
-    self.tau_ref = parameter(tau_ref, self.varshape, allow_none=True)
-    self.noise = init_noise(noise, self.varshape, num_vars=2)
-    self.spike_fun = is_callable(spike_fun, 'spike_fun')
     self.input_var = input_var
-    self.ref_var = ref_var
-
-    # initializers
-    self._V_initializer = is_initializer(V_initializer, allow_none=True)
-    self._u_initializer = is_initializer(u_initializer, allow_none=True)
-
-    # variables
+    super().__init__(*args, **kwargs, init_var=False)
+    self.noise = init_noise(noise, self.varshape, num_vars=2)
+    if self.noise is not None:
+      self.integral = sdeint(method=self.method, f=self.derivative, g=self.noise)
     self.reset_state(self.mode)
 
-    # functions
-    if self.noise is None:
-      self.integral = odeint(method=method, f=JointEq([self.dV, self.du]))
-    else:
-      self.integral = sdeint(method=method, f=JointEq([self.dV, self.du]), g=self.noise)
-
   def reset_state(self, batch_size=None):
-    v_init = OneInit(-70.) if self._V_initializer is None else self._V_initializer
-    self.V = variable_(v_init, self.varshape, batch_size)
-    u_init = OneInit(self.b * self.V) if self._u_initializer is None else self._u_initializer
-    self.u = variable_(u_init, self.varshape, batch_size)
+    super().reset_state(batch_size)
     if self.input_var:
       self.input = variable_(bm.zeros, self.varshape, batch_size)
-    sp_type = bm.float_ if isinstance(self.mode, bm.TrainingMode) else bool
-    self.spike = variable_(lambda s: bm.zeros(s, dtype=sp_type), self.varshape, batch_size)
-    if self.tau_ref is not None:
-      self.t_last_spike = variable_(lambda s: bm.ones(s) * -1e7, self.varshape, batch_size)
-      if self.ref_var:
-        self.refractory = variable_(lambda s: bm.zeros(s, dtype=bool), self.varshape, batch_size)
-
-  def dV(self, V, t, u, I_ext):
-    dVdt = 0.04 * V * V + 5 * V + 140 - u + I_ext
-    return dVdt
-
-  def du(self, u, t, V):
-    dudt = self.a * (self.b * V - u)
-    return dudt
 
   def update(self, x=None):
-    t = share.load('t')
-    dt = share.load('dt')
     if self.input_var:
       if x is not None:
         self.input += x
       x = self.input.value
     else:
       x = 0. if x is None else x
-    V, u = self.integral(self.V.value, self.u.value, t, x, dt)
-
-    if self.tau_ref is not None:
-      refractory = bm.as_jax((t - self.t_last_spike) <= self.tau_ref)
-      refractory = stop_gradient(refractory)
-      V = bm.where(refractory, self.V.value, V)
-
-      # spike, refractory, and reset membrane potential
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun(V - self.V_th)
-        spike_no_grad = stop_gradient(spike)
-        V += spike_no_grad * (self.c - self.V_th)
-        u += spike_no_grad * self.d
-        t_last_spike = stop_gradient(bm.where(spike_no_grad, t, self.t_last_spike.value))
-        if self.ref_var:
-          self.refractory.value = stop_gradient(bm.logical_or(refractory, spike_no_grad > 0.))
-      else:
-        spike = self.V_th <= V
-        V = bm.where(spike, self.c, V)
-        u = bm.where(spike, u + self.d, u)
-        t_last_spike = bm.where(spike, t, self.t_last_spike.value)
-        if self.ref_var:
-          self.refractory.value = bm.logical_or(refractory, spike)
-      self.t_last_spike.value = t_last_spike
-
-    else:
-      # spike, refractory, and reset membrane potential
-      if isinstance(self.mode, bm.TrainingMode):
-        spike = self.spike_fun(V - self.V_th)
-        spike_no_grad = stop_gradient(spike)
-        V += spike_no_grad * (self.c - self.V_th)
-        u += spike_no_grad * self.d
-      else:
-        spike = self.V_th <= V
-        V = bm.where(spike, self.c, V)
-        u = bm.where(spike, u + self.d, u)
-
-    # finally
-    self.V.value = V
-    self.u.value = u
-    self.spike.value = spike
-    return spike
+    return super().update(x)
 
   def clear_input(self):
     if self.input_var:
-      self.input[:] = 0.
+      self.input.value = bm.zeros_like(self.input)
 
 
-class HindmarshRose(NeuGroupNS):
+class HindmarshRose(NeuDyn):
   r"""Hindmarsh-Rose neuron model.
 
   **Model Descriptions**
 
   The HindmarshRose model [1]_ [2]_ of neuronal activity is aimed to study the
   spiking-bursting behavior of the membrane potential observed in experiments
   made with a single neuron.
@@ -2039,15 +1095,15 @@
     return self.spike.value
 
   def clear_input(self):
     if self.input_var:
       self.input[:] = 0.
 
 
-class FHN(NeuGroupNS):
+class FHN(NeuDyn):
   r"""FitzHugh-Nagumo neuron model.
 
   **Model Descriptions**
 
   The FitzHughNagumo model (FHN), named after Richard FitzHugh (19222007)
   who suggested the system in 1961 [1]_ and J. Nagumo et al. who created the
   equivalent circuit the following year, describes a prototype of an excitable
@@ -2207,15 +1263,179 @@
     return self.spike.value
 
   def clear_input(self):
     if self.input_var:
       self.input[:] = 0.
 
 
-class LIF_SFA_Bellec2020(NeuGroupNS):
+class ALIFBellec2020(NeuDyn):
+  r"""Leaky Integrate-and-Fire model with SFA [1]_.
+
+  This model is similar to the GLIF2 model in the Technical White Paper
+  on generalized LIF (GLIF) models from AllenInstitute [2]_.
+
+  Formally, this model is given by:
+
+  .. math::
+
+     \tau \dot{V} = -(V - V_{\mathrm{rest}}) + R*I \\
+     \tau_a \dot{a} = -a
+
+  Once a spike is induced by :math:`V(t) > V_{\mathrm{th}} + \beta a`, then
+
+  .. math::
+
+     V \gets V - V_{\mathrm{th}} \\
+     a \gets a + 1
+
+
+  References
+  ----------
+  .. [1] Bellec, Guillaume, et al. "A solution to the learning dilemma for
+         recurrent networks of spiking neurons."
+         Nature communications 11.1 (2020): 1-15.
+  .. [2] Allen Institute: Cell Types Database.  2018 Allen Institute for
+         Brain Science. Allen Cell Types Database, cell feature search.
+         Available from: celltypes.brain-map.org/data (2018).
+  """
+
+  def __init__(
+      self,
+      size: Shape,
+      keep_size: bool = False,
+
+      # model parameters
+      V_rest: Union[float, ArrayType, Initializer, Callable] = -70.,
+      V_th: Union[float, ArrayType, Initializer, Callable] = -60.,
+      R: Union[float, ArrayType, Initializer, Callable] = 1.,
+      beta: Union[float, ArrayType, Initializer, Callable] = 1.6,
+      tau: Union[float, ArrayType, Initializer, Callable] = 20.,
+      tau_a: Union[float, ArrayType, Initializer, Callable] = 2000.,
+      tau_ref: Union[float, ArrayType, Initializer, Callable] = None,
+      noise: Union[float, ArrayType, Initializer, Callable] = None,
+
+      # initializers
+      V_initializer: Union[Initializer, Callable, ArrayType] = OneInit(-70.),
+      a_initializer: Union[Initializer, Callable, ArrayType] = OneInit(-50.),
+
+      # parameter for training
+      spike_fun: Callable = bm.surrogate.relu_grad,
+      input_var: bool = True,
+
+      # other parameters
+      method: str = 'exp_auto',
+      name: str = None,
+      mode: bm.Mode = None,
+      eprop: bool = False
+  ):
+    super().__init__(name=name,
+                     size=size,
+                     keep_size=keep_size,
+                     mode=mode)
+    is_subclass(self.mode, (bm.TrainingMode, bm.NonBatchingMode))
+
+    # parameters
+    self.V_rest = parameter(V_rest, self.varshape, allow_none=False)
+    self.V_th = parameter(V_th, self.varshape, allow_none=False)
+    self.R = parameter(R, self.varshape, allow_none=False)
+    self.beta = parameter(beta, self.varshape, allow_none=False)
+    self.tau = parameter(tau, self.varshape, allow_none=False)
+    self.tau_a = parameter(tau_a, self.varshape, allow_none=False)
+    self.tau_ref = parameter(tau_ref, self.varshape, allow_none=True)
+    self.noise = init_noise(noise, self.varshape, num_vars=2)
+    self.spike_fun = is_callable(spike_fun, 'spike_fun')
+    self.eprop = eprop
+    self.input_var = input_var
+
+    # initializers
+    self._V_initializer = is_initializer(V_initializer, 'V_initializer')
+    self._a_initializer = is_initializer(a_initializer, 'a_initializer')
+
+    # variables
+    self.reset_state(self.mode)
+
+    # integral
+    if self.noise is None:
+      self.integral = odeint(method=method, f=self.derivative)
+    else:
+      self.integral = sdeint(method=method, f=self.derivative, g=self.noise)
+
+  def da(self, a, t):
+    return -a / self.tau_a
+
+  def dV(self, V, t, I_ext):
+    return (- (V - self.V_rest) + self.R * I_ext) / self.tau
+
+  @property
+  def derivative(self):
+    return JointEq([self.dV, self.da])
+
+  def reset_state(self, batch_size=None):
+    self.a = variable_(self._a_initializer, self.varshape, batch_size)
+    self.V = variable_(self._V_initializer, self.varshape, batch_size)
+    if self.input_var:
+      self.input = variable_(bm.zeros, self.varshape, batch_size)
+    sp_type = bm.float_ if isinstance(self.mode, bm.TrainingMode) else bool
+    self.spike = variable_(lambda s: bm.zeros(s, dtype=sp_type), self.varshape, batch_size)
+    if self.tau_ref is not None:
+      self.t_last_spike = variable_(lambda s: bm.ones(s) * -1e7, self.varshape, batch_size)
+      self.refractory = variable_(lambda s: bm.zeros(s, dtype=bool), self.varshape, batch_size)
+
+  def update(self, x=None):
+    t = share.load('t')
+    dt = share.load('dt')
+    if self.input_var:
+      if x is not None:
+        self.input += x
+      x = self.input.value
+    else:
+      x = 0. if x is None else x
+    V, a = self.integral(self.V, self.a, t, x, dt)
+
+    if self.tau_ref is not None:
+      # refractory
+      refractory = (t - self.t_last_spike) <= self.tau_ref
+      if isinstance(self.mode, bm.TrainingMode):
+        refractory = stop_gradient(refractory)
+      V = bm.where(refractory, self.V.value, V)
+      # spike and reset
+      if isinstance(self.mode, bm.TrainingMode):
+        spike = self.spike_fun((V - self.V_th - self.beta * self.a) / self.V_th)
+        V -= self.V_th * (stop_gradient(spike) if self.eprop else spike)
+        # will be used in other place, like Delta Synapse, so stop its gradient
+        spike_ = spike > 0.
+        refractory = stop_gradient(bm.logical_or(refractory, spike_))
+        t_last_spike = stop_gradient(bm.where(spike_, t, self.t_last_spike.value))
+      else:
+        spike = V >= (self.V_th + self.beta * self.a)
+        refractory = bm.logical_or(refractory, spike)
+        t_last_spike = bm.where(spike, t, self.t_last_spike.value)
+        V -= self.V_th * spike
+      self.refractory.value = refractory
+      self.t_last_spike.value = t_last_spike
+
+    else:
+      # spike and reset
+      if isinstance(self.mode, bm.TrainingMode):
+        spike = self.spike_fun((V - self.V_th - self.beta * self.a) / self.V_th)
+        V -= self.V_th * (stop_gradient(spike) if self.eprop else spike)
+      else:
+        spike = V >= (self.V_th + self.beta * self.a)
+        V -= self.V_th * spike
+    self.spike.value = spike
+    self.V.value = V
+    self.a.value = a + spike
+    return spike
+
+  def clear_input(self):
+    if self.input_var:
+      self.input[:] = 0.
+
+
+class LIF_SFA_Bellec2020(NeuDyn):
   r"""Leaky Integrate-and-Fire model with SFA [1]_.
 
   This model is similar to the GLIF2 model in the Technical White Paper
   on generalized LIF (GLIF) models from AllenInstitute [2]_.
 
   Formally, this model is given by:
```

## Comparing `brainpy/_src/rates/populations.py` & `brainpy/_src/dyn/rates/populations.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 # -*- coding: utf-8 -*-
 
 from typing import Union, Callable
+
 import jax
 
 from brainpy import math as bm
 from brainpy._src.context import share
-from brainpy._src.dynsys import NeuGroupNS
-from brainpy._src.neurons.noise_groups import OUProcess
+from brainpy._src.dyn.others.noise import OUProcess
+from brainpy._src.dyn.base import NeuDyn
 from brainpy._src.initialize import (Initializer,
                                      Uniform,
                                      parameter,
                                      variable,
                                      variable_,
                                      ZeroInit)
 from brainpy._src.integrators.joint_eq import JointEq
@@ -24,15 +25,15 @@
   'QIF',
   'StuartLandauOscillator',
   'WilsonCowanModel',
   'ThresholdLinearModel',
 ]
 
 
-class RateModel(NeuGroupNS):
+class RateModel(NeuDyn):
   pass
 
 
 class FHN(RateModel):
   r"""FitzHugh-Nagumo system used in [1]_.
 
   .. math::
@@ -94,15 +95,15 @@
       method: str = 'exp_auto',
       name: str = None,
 
       # parameter for training
       mode: bm.Mode = None,
       input_var: bool = True,
   ):
-    super(FHN, self).__init__(size=size,
+    super().__init__(size=size,
                               name=name,
                               keep_size=keep_size,
                               mode=mode)
 
     # model parameters
     self.alpha = parameter(alpha, self.varshape, allow_none=False)
     self.beta = parameter(beta, self.varshape, allow_none=False)
```

## Comparing `brainpy/_src/synapses/abstract_models.py` & `brainpy/_src/dynold/synapses/abstract_models.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,30 +1,32 @@
 # -*- coding: utf-8 -*-
 
 from typing import Union, Dict, Callable, Optional
 
-from jax import vmap
-from jax.lax import stop_gradient
+import jax
 
 import brainpy.math as bm
 from brainpy._src.connect import TwoEndConnector, All2All, One2One
-from brainpy._src.synouts import CUBA, MgBlock
-from brainpy._src.dynsys import NeuGroup, SynOut, SynSTP, TwoEndConn, SynConn
+from brainpy._src.context import share
+from brainpy._src.dyn import synapses
+from brainpy._src.dyn.base import NeuDyn
+from brainpy._src.dnn import linear
+from brainpy._src.dynold.synouts import MgBlock, CUBA
 from brainpy._src.initialize import Initializer, variable_
-from brainpy._src.integrators import odeint, JointEq
-from brainpy.check import is_integer, is_float, is_subclass
+from brainpy._src.integrators.ode.generic import odeint
+from brainpy._src.dyn.projections.aligns import _pre_delay_repr, _init_delay
 from brainpy.types import ArrayType
+from .base import TwoEndConn, _SynSTP, _SynOut, _TwoEndConnAlignPre
 
 __all__ = [
   'Delta',
   'Exponential',
   'DualExponential',
   'Alpha',
   'NMDA',
-  'PoissonInput',
 ]
 
 
 class Delta(TwoEndConn):
   r"""Voltage Jump Synapse Model, or alias of Delta Synapse Model.
 
   **Model Descriptions**
@@ -63,17 +65,17 @@
     >>> plt.plot(runner.mon.ts, runner.mon['post.V'], label='post-V')
     >>> plt.xlim(40, 150)
     >>> plt.legend()
     >>> plt.show()
 
   Parameters
   ----------
-  pre: NeuGroup
+  pre: NeuDyn
     The pre-synaptic neuron group.
-  post: NeuGroup
+  post: NeuDyn
     The post-synaptic neuron group.
   conn: optional, ArrayType, dict of (str, ndarray), TwoEndConnector
     The synaptic connections.
   comp_method: str
     The connection type used for model speed optimization. It can be
     `sparse` and `dense`. The default is `sparse`.
   delay_step: int, ArrayType, Initializer, Callable
@@ -82,36 +84,34 @@
     The synaptic strength. Default is 1.
   post_ref_key: str
     Whether the post-synaptic group has refractory period.
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
-      output: SynOut = CUBA(target_var='V'),
-      stp: Optional[SynSTP] = None,
+      output: _SynOut = CUBA(target_var='V'),
+      stp: Optional[_SynSTP] = None,
       comp_method: str = 'sparse',
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       delay_step: Union[float, ArrayType, Initializer, Callable] = None,
       post_ref_key: str = None,
-
-      # other parameters
       name: str = None,
       mode: bm.Mode = None,
       stop_spike_gradient: bool = False,
   ):
-    super(Delta, self).__init__(name=name,
-                                pre=pre,
-                                post=post,
-                                conn=conn,
-                                output=output,
-                                stp=stp,
-                                mode=mode)
+    super().__init__(name=name,
+                     pre=pre,
+                     post=post,
+                     conn=conn,
+                     output=output,
+                     stp=stp,
+                     mode=mode)
 
     # parameters
     self.stop_spike_gradient = stop_spike_gradient
     self.post_ref_key = post_ref_key
     if post_ref_key:
       self.check_post_attrs(post_ref_key)
     self.comp_method = comp_method
@@ -123,51 +123,53 @@
     self.delay_step = self.register_delay(f"{self.pre.name}.spike", delay_step, self.pre.spike)
 
   def reset_state(self, batch_size=None):
     self.output.reset_state(batch_size)
     if self.stp is not None:
       self.stp.reset_state(batch_size)
 
-  def update(self, tdi, pre_spike=None):
+  def update(self, pre_spike=None):
     # pre-synaptic spikes
     if pre_spike is None:
       pre_spike = self.get_delay_data(f"{self.pre.name}.spike", delay_step=self.delay_step)
     pre_spike = bm.as_jax(pre_spike)
     if self.stop_spike_gradient:
-      pre_spike = stop_gradient(pre_spike)
+      pre_spike = jax.lax.stop_gradient(pre_spike)
 
     # update sub-components
-    self.output.update(tdi)
-    if self.stp is not None: self.stp.update(tdi, pre_spike)
+    if self.stp is not None:
+      self.stp.update(pre_spike)
 
     # synaptic values onto the post
     if isinstance(self.conn, All2All):
       syn_value = bm.asarray(pre_spike, dtype=bm.float_)
       if self.stp is not None:
         syn_value = self.stp(syn_value)
       post_vs = self._syn2post_with_all2all(syn_value, self.g_max)
     elif isinstance(self.conn, One2One):
       syn_value = bm.asarray(pre_spike, dtype=bm.float_)
       if self.stp is not None:
         syn_value = self.stp(syn_value)
       post_vs = self._syn2post_with_one2one(syn_value, self.g_max)
     else:
       if self.comp_method == 'sparse':
-        f = lambda s: bm.event.csrmv(
-          self.g_max, self.conn_mask[0], self.conn_mask[1], s,
-          shape=(self.pre.num, self.post.num), transpose=True
-        )
-        if isinstance(self.mode, bm.BatchingMode): f = vmap(f)
-        post_vs = f(pre_spike)
-        # if not isinstance(self.stp, _NullSynSTP):
-        #   raise NotImplementedError()
-        #   stp_value = self.stp(1.)
-        #   f2 = lambda s: bm.pre2post_sum(s, self.post.num, *self.conn_mask)
-        #   if self.trainable: f2 = vmap(f2)
-        #   post_vs *= f2(stp_value)
+        if self.stp is not None:
+          syn_value = self.stp(pre_spike)
+          f = lambda s: bm.sparse.csrmv(
+            self.g_max, self.conn_mask[0], self.conn_mask[1], s,
+            shape=(self.pre.num, self.post.num), transpose=True
+          )
+        else:
+          syn_value = pre_spike
+          f = lambda s: bm.event.csrmv(
+            self.g_max, self.conn_mask[0], self.conn_mask[1], s,
+            shape=(self.pre.num, self.post.num), transpose=True
+          )
+        if isinstance(self.mode, bm.BatchingMode): f = jax.vmap(f)
+        post_vs = f(syn_value)
       else:
         syn_value = bm.asarray(pre_spike, dtype=bm.float_)
         if self.stp is not None:
           syn_value = self.stp(syn_value)
         post_vs = self._syn2post_with_dense(syn_value, self.g_max, self.conn_mask)
     if self.post_ref_key:
       post_vs = post_vs * (1. - getattr(self.post, self.post_ref_key))
@@ -198,18 +200,18 @@
 
   .. math::
 
       \begin{aligned}
        & g_{\mathrm{syn}}(t) = g_{max} g * \mathrm{STP} \\
        & \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}).
        \end{aligned}
-  
+
   where :math:`\mathrm{STP}` is used to model the short-term plasticity effect.
-  
-  
+
+
   **Model Examples**
 
   - `(Brunel & Hakim, 1999) Fast Global Oscillation <https://brainpy-examples.readthedocs.io/en/latest/oscillation_synchronization/Brunel_Hakim_1999_fast_oscillation.html>`_
   - `(Vreeswijk & Sompolinsky, 1996) E/I balanced network <https://brainpy-examples.readthedocs.io/en/latest/ei_nets/Vreeswijk_1996_EI_net.html>`_
   - `(Brette, et, al., 2007) CUBA <https://brainpy-examples.readthedocs.io/en/latest/ei_nets/Brette_2007_CUBA.html>`_
   - `(Tian, et al., 2020) E/I Net for fast response <https://brainpy-examples.readthedocs.io/en/latest/ei_nets/Tian_2020_EI_net_for_fast_response.html>`_
 
@@ -269,105 +271,94 @@
           "The Synapse." Principles of Computational Modelling in Neuroscience.
           Cambridge: Cambridge UP, 2011. 172-95. Print.
 
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
-      output: Optional[SynOut] = CUBA(),
-      stp: Optional[SynSTP] = None,
+      output: Optional[_SynOut] = CUBA(),
+      stp: Optional[_SynSTP] = None,
       comp_method: str = 'sparse',
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       tau: Union[float, ArrayType] = 8.0,
       method: str = 'exp_auto',
 
       # other parameters
       name: str = None,
       mode: bm.Mode = None,
       stop_spike_gradient: bool = False,
   ):
-    super(Exponential, self).__init__(pre=pre,
-                                      post=post,
-                                      conn=conn,
-                                      output=output,
-                                      stp=stp,
-                                      name=name,
-                                      mode=mode)
+    super().__init__(pre=pre,
+                     post=post,
+                     conn=conn,
+                     output=output,
+                     stp=stp,
+                     name=name,
+                     mode=mode)
     # parameters
     self.stop_spike_gradient = stop_spike_gradient
-    self.comp_method = comp_method
-    self.tau = tau
-    if bm.size(self.tau) != 1:
-      raise ValueError(f'"tau" must be a scalar or a tensor with size of 1. But we got {self.tau}')
 
-    # connections and weights
-    self.g_max, self.conn_mask = self._init_weights(g_max, comp_method, sparse_data='csr')
+    # synapse dynamics
+    self.syn = synapses.Expon(post.varshape, tau=tau, method=method)
+
+    # Projection
+    if isinstance(conn, All2All):
+      self.comm = linear.AllToAll(pre.num, post.num, g_max)
+    elif isinstance(conn, One2One):
+      assert post.num == pre.num
+      self.comm = linear.OneToOne(pre.num, g_max)
+    else:
+      if comp_method == 'dense':
+        self.comm = linear.MaskedLinear(conn, g_max)
+      elif comp_method == 'sparse':
+        if self.stp is None:
+          self.comm = linear.EventCSRLinear(conn, g_max)
+        else:
+          self.comm = linear.CSRLinear(conn, g_max)
+      else:
+        raise ValueError(f'Does not support {comp_method}, only "sparse" or "dense".')
 
     # variables
-    self.g = variable_(bm.zeros, self.post.num, self.mode)
-    self.delay_step = self.register_delay(f"{self.pre.name}.spike", delay_step, self.pre.spike)
+    self.g = self.syn.g
 
-    # function
-    self.integral = odeint(lambda g, t: -g / self.tau, method=method)
+    # delay
+    self.delay_step = self.register_delay(f"{self.pre.name}.spike", delay_step, self.pre.spike)
 
   def reset_state(self, batch_size=None):
-    self.g.value = variable_(bm.zeros, self.post.num, batch_size)
+    self.syn.reset_state(batch_size)
     self.output.reset_state(batch_size)
-    if self.stp is not None: self.stp.reset_state(batch_size)
-
-  def update(self, tdi, pre_spike=None):
-    t, dt = tdi['t'], tdi['dt']
+    if self.stp is not None:
+      self.stp.reset_state(batch_size)
 
+  def update(self, pre_spike=None):
     # delays
     if pre_spike is None:
       pre_spike = self.get_delay_data(f"{self.pre.name}.spike", self.delay_step)
     pre_spike = bm.as_jax(pre_spike)
     if self.stop_spike_gradient:
-      pre_spike = stop_gradient(pre_spike)
+      pre_spike = jax.lax.stop_gradient(pre_spike)
 
     # update sub-components
-    self.output.update(tdi)
-    if self.stp is not None: self.stp.update(tdi, pre_spike)
+    self.output.update()
+    if self.stp is not None:
+      self.stp.update(pre_spike)
+      pre_spike = self.stp(pre_spike)
 
     # post values
-    if isinstance(self.conn, All2All):
-      syn_value = bm.asarray(pre_spike, dtype=bm.float_)
-      if self.stp is not None: syn_value = self.stp(syn_value)
-      post_vs = self._syn2post_with_all2all(syn_value, self.g_max)
-    elif isinstance(self.conn, One2One):
-      syn_value = bm.asarray(pre_spike, dtype=bm.float_)
-      if self.stp is not None: syn_value = self.stp(syn_value)
-      post_vs = self._syn2post_with_one2one(syn_value, self.g_max)
-    else:
-      if self.comp_method == 'sparse':
-        f = lambda s: bm.event.csrmv(
-          self.g_max, self.conn_mask[0], self.conn_mask[1], s,
-          shape=(self.pre.num, self.post.num),
-          transpose=True
-        )
-        if isinstance(self.mode, bm.BatchingMode): f = vmap(f)
-        post_vs = f(pre_spike)
-        # if not isinstance(self.stp, _NullSynSTP):
-        #   raise NotImplementedError()
-      else:
-        syn_value = bm.asarray(pre_spike, dtype=bm.float_)
-        if self.stp is not None: syn_value = self.stp(syn_value)
-        post_vs = self._syn2post_with_dense(syn_value, self.g_max, self.conn_mask)
-    # updates
-    self.g.value = self.integral(self.g.value, t, dt) + post_vs
+    g = self.syn(self.comm(pre_spike))
 
     # output
-    return self.output(self.g)
+    return self.output(g)
 
 
-class DualExponential(TwoEndConn):
+class DualExponential(_TwoEndConnAlignPre):
   r"""Dual exponential synapse model.
 
     **Model Descriptions**
 
     The dual exponential synapse model [1]_, also named as *difference of two exponentials* model,
     is given by:
 
@@ -421,17 +412,17 @@
       >>> plt.plot(runner.mon.ts, runner.mon['syn.g'], label='g')
       >>> plt.plot(runner.mon.ts, runner.mon['syn.h'], label='h')
       >>> plt.legend()
       >>> plt.show()
 
     Parameters
     ----------
-    pre: NeuGroup
+    pre: NeuDyn
       The pre-synaptic neuron group.
-    post: NeuGroup
+    post: NeuDyn
       The post-synaptic neuron group.
     conn: optional, ArrayType, dict of (str, ndarray), TwoEndConnector
       The synaptic connections.
     comp_method: str
       The connection type used for model speed optimization. It can be
       `sparse` and `dense`. The default is `sparse`.
     delay_step: int, ArrayType, Initializer, Callable
@@ -456,114 +447,70 @@
     .. [2] Roth, A., & Van Rossum, M. C. W. (2009). Modeling Synapses. Computational
            Modeling Methods for Neuroscientists.
 
     """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
-      stp: Optional[SynSTP] = None,
-      output: SynOut = CUBA(),
+      stp: Optional[_SynSTP] = None,
+      output: _SynOut = None,  # CUBA(),
       comp_method: str = 'dense',
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       tau_decay: Union[float, ArrayType] = 10.0,
       tau_rise: Union[float, ArrayType] = 1.,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       method: str = 'exp_auto',
 
       # other parameters
       name: str = None,
       mode: bm.Mode = None,
       stop_spike_gradient: bool = False,
   ):
-    super(DualExponential, self).__init__(pre=pre,
-                                          post=post,
-                                          conn=conn,
-                                          output=output,
-                                          stp=stp,
-                                          name=name,
-                                          mode=mode)
+
     # parameters
-    # self.check_pre_attrs('spike')
-    self.check_post_attrs('input')
     self.stop_spike_gradient = stop_spike_gradient
     self.comp_method = comp_method
     self.tau_rise = tau_rise
     self.tau_decay = tau_decay
     if bm.size(self.tau_rise) != 1:
       raise ValueError(f'"tau_rise" must be a scalar or a tensor with size of 1. '
                        f'But we got {self.tau_rise}')
     if bm.size(self.tau_decay) != 1:
       raise ValueError(f'"tau_decay" must be a scalar or a tensor with size of 1. '
                        f'But we got {self.tau_decay}')
 
-    # connections
-    self.g_max, self.conn_mask = self._init_weights(g_max, comp_method, sparse_data='csr')
-
-    # variables
-    self.h = variable_(bm.zeros, self.pre.num, self.mode)
-    self.g = variable_(bm.zeros, self.pre.num, self.mode)
-    self.delay_step = self.register_delay(f"{self.pre.name}.spike", delay_step, self.pre.spike)
-
-    # integral
-    self.integral = odeint(method=method, f=JointEq([self.dg, self.dh]))
-
-  def reset_state(self, batch_size=None):
-    self.h.value = variable_(bm.zeros, self.pre.num, batch_size)
-    self.g.value = variable_(bm.zeros, self.pre.num, batch_size)
-    self.output.reset_state(batch_size)
-    if self.stp is not None: self.stp.reset_state(batch_size)
-
-  def dh(self, h, t):
-    return -h / self.tau_rise
-
-  def dg(self, g, t, h):
-    return -g / self.tau_decay + h
-
-  def update(self, tdi, pre_spike=None):
-    t, dt = tdi['t'], tdi['dt']
+    syn = synapses.DualExpon(pre.size,
+                             pre.keep_size,
+                             mode=mode,
+                             tau_decay=tau_decay,
+                             tau_rise=tau_rise,
+                             method=method, )
+
+    super().__init__(pre=pre,
+                     post=post,
+                     syn=syn,
+                     conn=conn,
+                     output=output,
+                     stp=stp,
+                     comp_method=comp_method,
+                     g_max=g_max,
+                     delay_step=delay_step,
+                     name=name,
+                     mode=mode)
 
-    # pre-synaptic spikes
-    if pre_spike is None:
-      pre_spike = self.get_delay_data(f"{self.pre.name}.spike", self.delay_step)
-    pre_spike = bm.as_jax(pre_spike)
-    if self.stop_spike_gradient:
-      pre_spike = stop_gradient(pre_spike)
-
-    # update sub-components
-    self.output.update(tdi)
-    if self.stp is not None: self.stp.update(tdi, pre_spike)
-
-    # update synaptic variables
-    self.g.value, self.h.value = self.integral(self.g, self.h, t, dt)
-    self.h += pre_spike
-
-    # post values
-    syn_value = self.g.value
-    if self.stp is not None: syn_value = self.stp(syn_value)
-    if isinstance(self.conn, All2All):
-      post_vs = self._syn2post_with_all2all(syn_value, self.g_max)
-    elif isinstance(self.conn, One2One):
-      post_vs = self._syn2post_with_one2one(syn_value, self.g_max)
-    else:
-      if self.comp_method == 'sparse':
-        f = lambda s: bm.sparse.csrmv(
-          self.g_max, self.conn_mask[0], self.conn_mask[1], s,
-          shape=(self.pre.num, self.post.num),
-          transpose=True
-        )
-        if isinstance(self.mode, bm.BatchingMode): f = vmap(f)
-        post_vs = f(syn_value)
-      else:
-        post_vs = self._syn2post_with_dense(syn_value, self.g_max, self.conn_mask)
+    self.check_post_attrs('input')
+    # copy the references
+    self.g = syn.g
+    self.h = syn.h
 
-    # output
-    return self.output(post_vs)
+  def update(self, pre_spike=None):
+    return super().update(pre_spike, stop_spike_gradient=self.stop_spike_gradient)
 
 
 class Alpha(DualExponential):
   r"""Alpha synapse model.
 
   **Model Descriptions**
 
@@ -610,17 +557,17 @@
     >>> plt.plot(runner.mon.ts, runner.mon['syn.g'], label='g')
     >>> plt.plot(runner.mon.ts, runner.mon['syn.h'], label='h')
     >>> plt.legend()
     >>> plt.show()
 
   Parameters
   ----------
-  pre: NeuGroup
+  pre: NeuDyn
     The pre-synaptic neuron group.
-  post: NeuGroup
+  post: NeuDyn
     The post-synaptic neuron group.
   conn: optional, ArrayType, dict of (str, ndarray), TwoEndConnector
     The synaptic connections.
   comp_method: str
     The connection type used for model speed optimization. It can be
     `sparse` and `dense`. The default is `sparse`.
   delay_step: int, ArrayType, Initializer, Callable
@@ -640,19 +587,19 @@
   .. [1] Sterratt, David, Bruce Graham, Andrew Gillies, and David Willshaw.
           "The Synapse." Principles of Computational Modelling in Neuroscience.
           Cambridge: Cambridge UP, 2011. 172-95. Print.
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
-      output: SynOut = CUBA(),
-      stp: Optional[SynSTP] = None,
+      output: _SynOut = None,  # CUBA(),
+      stp: Optional[_SynSTP] = None,
       comp_method: str = 'dense',
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       tau_decay: Union[float, ArrayType] = 10.0,
       method: str = 'exp_auto',
 
       # other parameters
@@ -672,15 +619,15 @@
                                 output=output,
                                 stp=stp,
                                 name=name,
                                 mode=mode,
                                 stop_spike_gradient=stop_spike_gradient)
 
 
-class NMDA(TwoEndConn):
+class NMDA(_TwoEndConnAlignPre):
   r"""NMDA synapse model.
 
   **Model Descriptions**
 
   The NMDA receptor is a glutamate receptor and ion channel found in neurons.
   The NMDA receptor is one of three types of ionotropic glutamate receptors,
   the other two being AMPA and kainate receptors.
@@ -739,15 +686,15 @@
 
     >>> import brainpy as bp
     >>> from brainpy import synapses, neurons
     >>> import matplotlib.pyplot as plt
     >>>
     >>> neu1 = neurons.HH(1)
     >>> neu2 = neurons.HH(1)
-    >>> syn1 = synapses.NMDA(neu1, neu2, bp.connect.All2All(), E=0.)
+    >>> syn1 = synapses.NMDA(neu1, neu2, bp.connect.All2All())
     >>> net = bp.Network(pre=neu1, syn=syn1, post=neu2)
     >>>
     >>> runner = bp.DSRunner(net, inputs=[('pre.input', 5.)], monitors=['pre.V', 'post.V', 'syn.g', 'syn.x'])
     >>> runner.run(150.)
     >>>
     >>> fig, gs = bp.visualize.get_figure(2, 1, 3, 8)
     >>> fig.add_subplot(gs[0, 0])
@@ -759,17 +706,17 @@
     >>> plt.plot(runner.mon.ts, runner.mon['syn.g'], label='g')
     >>> plt.plot(runner.mon.ts, runner.mon['syn.x'], label='x')
     >>> plt.legend()
     >>> plt.show()
 
   Parameters
   ----------
-  pre: NeuGroup
+  pre: NeuDyn
     The pre-synaptic neuron group.
-  post: NeuGroup
+  post: NeuDyn
     The post-synaptic neuron group.
   conn: optional, ArrayType, dict of (str, ndarray), TwoEndConnector
     The synaptic connections.
   comp_method: str
     The connection type used for model speed optimization. It can be
     `sparse` and `dense`. The default is `dense`.
   delay_step: int, ArrayType, Initializer, Callable
@@ -801,189 +748,62 @@
          England journal of medicine, 361(3), p.302.
   .. [4] https://en.wikipedia.org/wiki/NMDA_receptor
 
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
-      output: SynOut = MgBlock(E=0., alpha=0.062, beta=3.57, cc_Mg=1.2),
-      stp: Optional[SynSTP] = None,
+      output: _SynOut = MgBlock(E=0., alpha=0.062, beta=3.57, cc_Mg=1.2),
+      stp: Optional[_SynSTP] = None,
       comp_method: str = 'dense',
       g_max: Union[float, ArrayType, Initializer, Callable] = 0.15,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       tau_decay: Union[float, ArrayType] = 100.,
       a: Union[float, ArrayType] = 0.5,
       tau_rise: Union[float, ArrayType] = 2.,
       method: str = 'exp_auto',
-
-      # other parameters
-      name: str = None,
-      mode: bm.Mode = None,
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
       stop_spike_gradient: bool = False,
   ):
-    super(NMDA, self).__init__(pre=pre,
-                               post=post,
-                               conn=conn,
-                               output=output,
-                               stp=stp,
-                               name=name,
-                               mode=mode)
     # parameters
-    # self.check_post_attrs('input', 'V')
     self.tau_decay = tau_decay
     self.tau_rise = tau_rise
     self.a = a
     if bm.size(a) != 1:
       raise ValueError(f'"a" must be a scalar or a tensor with size of 1. But we got {a}')
     if bm.size(tau_decay) != 1:
       raise ValueError(f'"tau_decay" must be a scalar or a tensor with size of 1. But we got {tau_decay}')
     if bm.size(tau_rise) != 1:
       raise ValueError(f'"tau_rise" must be a scalar or a tensor with size of 1. But we got {tau_rise}')
     self.comp_method = comp_method
     self.stop_spike_gradient = stop_spike_gradient
 
-    # connections and weights
-    self.g_max, self.conn_mask = self._init_weights(g_max, comp_method, sparse_data='csr')
-
-    # variables
-    self.g = variable_(bm.zeros, self.pre.num, self.mode)
-    self.x = variable_(bm.zeros, self.pre.num, self.mode)
-    self.delay_step = self.register_delay(f"{self.pre.name}.spike", delay_step, self.pre.spike)
-
-    # integral
-    self.integral = odeint(method=method, f=JointEq(self.dg, self.dx))
-
-  def dg(self, g, t, x):
-    return -g / self.tau_decay + self.a * x * (1 - g)
-
-  def dx(self, x, t):
-    return -x / self.tau_rise
-
-  def reset_state(self, batch_size=None):
-    self.g.value = variable_(bm.zeros, self.pre.num, batch_size)
-    self.x.value = variable_(bm.zeros, self.pre.num, batch_size)
-    self.output.reset_state(batch_size)
-    if self.stp is not None: self.stp.reset_state(batch_size)
-
-  def update(self, tdi, pre_spike=None):
-    t, dt = tdi['t'], tdi['dt']
-    # delays
-    if pre_spike is None:
-      pre_spike = self.get_delay_data(f"{self.pre.name}.spike", self.delay_step)
-    pre_spike = bm.as_jax(pre_spike)
-    if self.stop_spike_gradient:
-      pre_spike = stop_gradient(pre_spike)
-
-    # update sub-components
-    self.output.update(tdi)
-    if self.stp is not None: self.stp.update(tdi, pre_spike)
-
-    # update synapse variables
-    self.g.value, self.x.value = self.integral(self.g, self.x, t, dt=dt)
-    self.x += pre_spike
-
-    # post-synaptic value
-    syn_value = self.g.value
-    if self.stp is not None: syn_value = self.stp(syn_value)
-    if isinstance(self.conn, All2All):
-      post_vs = self._syn2post_with_all2all(syn_value, self.g_max)
-    elif isinstance(self.conn, One2One):
-      post_vs = self._syn2post_with_one2one(syn_value, self.g_max)
-    else:
-      if self.comp_method == 'sparse':
-        f = lambda s: bm.event.csrmv(
-          self.g_max, self.conn_mask[0], self.conn_mask[1], s,
-          shape=(self.pre.num, self.post.num),
-          transpose=True
-        )
-        if isinstance(self.mode, bm.BatchingMode): f = vmap(f)
-        post_vs = f(syn_value)
-      else:
-        post_vs = self._syn2post_with_dense(syn_value, self.g_max, self.conn_mask)
-
-    # output
-    return self.output(post_vs)
-
-
-class PoissonInput(SynConn):
-  """Poisson Input to the given `Variable`.
-
-  Adds independent Poisson input to a target variable. For large
-  numbers of inputs, this is much more efficient than creating a
-  `PoissonGroup`. The synaptic events are generated randomly during the
-  simulation and are not preloaded and stored in memory. All the inputs must
-  target the same variable, have the same frequency and same synaptic weight.
-  All neurons in the target variable receive independent realizations of
-  Poisson spike trains.
-
-  Parameters
-  ----------
-  target_var: Variable
-    The variable that is targeted by this input.
-  num_input: int
-    The number of inputs.
-  freq: float
-    The frequency of each of the inputs. Must be a scalar.
-  weight: float
-    The synaptic weight. Must be a scalar.
-  """
-
-  def __init__(
-      self,
-      target_var: bm.Variable,
-      num_input: int,
-      freq: Union[int, float],
-      weight: Union[int, float],
-      seed: Optional[int] = None,
-      mode: bm.Mode = None,
-      name: str = None
-  ):
-    from ..neurons.input_groups import InputGroup, OutputGroup
-    super(PoissonInput, self).__init__(InputGroup(1), OutputGroup(1), name=name, mode=mode)
-    self.pre = None
-    self.post = None
-
-    # check data
-    if not isinstance(target_var, bm.Variable):
-      raise TypeError(f'"target_var" must be an instance of Variable. '
-                      f'But we got {type(target_var)}: {target_var}')
-    is_integer(num_input, 'num_input', min_bound=1)
-    is_float(freq, 'freq', min_bound=0., allow_int=True)
-    is_float(weight, 'weight', allow_int=True)
-    is_subclass(mode, (bm.NonBatchingMode, bm.BatchingMode), name=self.__class__.__name__)
-
-    # parameters
-    self.target_var = target_var
-    self.num_input = num_input
-    self.freq = freq
-    self.weight = weight
-    self.seed = seed
-
-  def update(self, tdi):
-    p = self.freq * tdi.dt / 1e3
-    a = self.num_input * p
-    b = self.num_input * (1 - p)
-    if isinstance(tdi.dt, (int, float)):  # dt is not in tracing
-      if (a > 5) and (b > 5):
-        inp = bm.random.normal(a, b * p, self.target_var.shape)
-      else:
-        inp = bm.random.binomial(self.num_input, p, self.target_var.shape)
-
-    else:  # dt is in tracing
-      inp = bm.cond((a > 5) * (b > 5),
-                    lambda _: bm.random.normal(a, b * p, self.target_var.shape),
-                    lambda _: bm.random.binomial(self.num_input, p, self.target_var.shape),
-                    None)
-    self.target_var += inp * self.weight
-
-  def __repr__(self):
-    names = self.__class__.__name__
-    return f'{names}(name={self.name}, num_input={self.num_input}, freq={self.freq}, weight={self.weight})'
-
-  def reset_state(self, batch_size=None):
-    pass
+    syn = synapses.NMDA(pre.size,
+                        pre.keep_size,
+                        mode=mode,
+                        a=a,
+                        tau_decay=tau_decay,
+                        tau_rise=tau_rise,
+                        method=method, )
+
+    super().__init__(pre=pre,
+                     post=post,
+                     syn=syn,
+                     conn=conn,
+                     output=output,
+                     stp=stp,
+                     comp_method=comp_method,
+                     g_max=g_max,
+                     delay_step=delay_step,
+                     name=name,
+                     mode=mode)
+
+    # copy the references
+    self.g = syn.g
+    self.x = syn.x
 
-  def reset(self, batch_size=None):
-    self.reset_state(batch_size)
+  def update(self, pre_spike=None):
+    return super().update(pre_spike, stop_spike_gradient=self.stop_spike_gradient)
```

## Comparing `brainpy/_src/synapses/biological_models.py` & `brainpy/_src/dyn/synapses/abstract_models.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,355 +1,410 @@
-# -*- coding: utf-8 -*-
+from typing import Union, Sequence, Callable, Optional
 
-from typing import Union, Dict, Callable, Optional
-
-from jax import vmap
-from jax.lax import stop_gradient
-
-import brainpy.math as bm
-from brainpy._src.dynsys import NeuGroup, TwoEndConn, SynSTP, SynOut
-from brainpy._src.synouts import COBA, MgBlock
-from brainpy._src.initialize import Initializer, variable
-from brainpy._src.integrators import odeint, JointEq
-from brainpy._src.connect import TwoEndConnector, All2All, One2One
+import jax.numpy
+from brainpy import math as bm
+from brainpy._src.context import share
+from brainpy._src.dyn._docs import pneu_doc
+from brainpy._src.dyn.base import SynDyn
+from brainpy._src.integrators.joint_eq import JointEq
+from brainpy._src.integrators.ode.generic import odeint
+from brainpy._src.mixin import AlignPost, ReturnInfo
+from brainpy._src.initialize import Constant
 from brainpy.types import ArrayType
 
 __all__ = [
-  'AMPA',
-  'GABAa',
-  'BioNMDA',
+  'Delta',
+  'Expon',
+  'DualExpon',
+  'DualExponV2',
+  'Alpha',
+  'NMDA',
+  'STD',
+  'STP',
 ]
 
 
-class AMPA(TwoEndConn):
-  r"""AMPA synapse model.
+class Delta(SynDyn, AlignPost):
+  r"""Delta synapse model.
 
   **Model Descriptions**
 
-  AMPA receptor is an ionotropic receptor, which is an ion channel.
-  When it is bound by neurotransmitters, it will immediately open the
-  ion channel, causing the change of membrane potential of postsynaptic neurons.
-
-  A classical model is to use the Markov process to model ion channel switch.
-  Here :math:`g` represents the probability of channel opening, :math:`1-g`
-  represents the probability of ion channel closing, and :math:`\alpha` and
-  :math:`\beta` are the transition probability. Because neurotransmitters can
-  open ion channels, the transfer probability from :math:`1-g` to :math:`g`
-  is affected by the concentration of neurotransmitters. We denote the concentration
-  of neurotransmitters as :math:`[T]` and get the following Markov process.
-
-  .. image:: ../../../_static/synapse_markov.png
-      :align: center
-
-  We obtained the following formula when describing the process by a differential equation.
-
-  .. math::
-
-      \frac{ds}{dt} =\alpha[T](1-g)-\beta g
-
-  where :math:`\alpha [T]` denotes the transition probability from state :math:`(1-g)`
-  to state :math:`(g)`; and :math:`\beta` represents the transition probability of
-  the other direction. :math:`\alpha` is the binding constant. :math:`\beta` is the
-  unbinding constant. :math:`[T]` is the neurotransmitter concentration, and
-  has the duration of 0.5 ms.
-
-  Moreover, the post-synaptic current on the post-synaptic neuron is formulated as
-
-  .. math::
-
-      I_{syn} = g_{max} g (V-E)
-
-  where :math:`g_{max}` is the maximum conductance, and `E` is the reverse potential.
-
-  **Model Examples**
-
-
-  .. plot::
-    :include-source: True
-
-    >>> import brainpy as bp
-    >>> from brainpy import neurons, synapses
-    >>> import matplotlib.pyplot as plt
-    >>>
-    >>> neu1 = neurons.HH(1)
-    >>> neu2 = neurons.HH(1)
-    >>> syn1 = synapses.AMPA(neu1, neu2, bp.connect.All2All())
-    >>> net = bp.Network(pre=neu1, syn=syn1, post=neu2)
-    >>>
-    >>> runner = bp.DSRunner(net, inputs=[('pre.input', 5.)], monitors=['pre.V', 'post.V', 'syn.g'])
-    >>> runner.run(150.)
-    >>>
-    >>> fig, gs = bp.visualize.get_figure(2, 1, 3, 8)
-    >>> fig.add_subplot(gs[0, 0])
-    >>> plt.plot(runner.mon.ts, runner.mon['pre.V'], label='pre-V')
-    >>> plt.plot(runner.mon.ts, runner.mon['post.V'], label='post-V')
-    >>> plt.legend()
-    >>>
-    >>> fig.add_subplot(gs[1, 0])
-    >>> plt.plot(runner.mon.ts, runner.mon['syn.g'], label='g')
-    >>> plt.legend()
-    >>> plt.show()
-
-  Parameters
-  ----------
-  pre: NeuGroup
-    The pre-synaptic neuron group.
-  post: NeuGroup
-    The post-synaptic neuron group.
-  conn: optional, ArrayType, dict of (str, ndarray), TwoEndConnector
-    The synaptic connections.
-  comp_method: str
-    The connection type used for model speed optimization. It can be
-    `sparse` and `dense`. The default is `dense`.
-  delay_step: int, ArrayType, Initializer, Callable
-    The delay length. It should be the value of :math:`\mathrm{delay\_time / dt}`.
-  E: float, ArrayType
-    The reversal potential for the synaptic current. [mV]
-
-    .. deprecated:: 2.1.13
-       `E` is deprecated in AMPA model. Please define `E` with brainpy.dyn.synouts.COBA.
-       This parameter will be removed since 2.2.0
-
-  g_max: float, ArrayType, Initializer, Callable
-    The synaptic strength (the maximum conductance). Default is 1.
-  alpha: float, ArrayType
-    Binding constant.
-  beta: float, ArrayType
-    Unbinding constant.
-  T: float, ArrayType
-    Transmitter concentration when synapse is triggered by
-    a pre-synaptic spike.. Default 1 [mM].
-  T_duration: float, ArrayType
-    Transmitter concentration duration time after being triggered. Default 1 [ms]
-  name: str
-    The name of this synaptic projection.
-  method: str
-    The numerical integration methods.
-
-  References
-  ----------
-
-  .. [1] Vijayan S, Kopell N J. Thalamic model of awake alpha oscillations
-         and implications for stimulus processing[J]. Proceedings of the
-         National Academy of Sciences, 2012, 109(45): 18553-18558.
+  The single exponential decay synapse model assumes the release of neurotransmitter,
+  its diffusion across the cleft, the receptor binding, and channel opening all happen
+  very quickly, so that the channels instantaneously jump from the closed to the open state.
+  Therefore, its expression is given by
+
+  .. math::
+
+      g_{\mathrm{syn}}(t)=g_{\mathrm{max}} e^{-\left(t-t_{0}\right) / \tau}
+
+  where :math:`\tau_{delay}` is the time constant of the synaptic state decay,
+  :math:`t_0` is the time of the pre-synaptic spike,
+  :math:`g_{\mathrm{max}}` is the maximal conductance.
+
+  Accordingly, the differential form of the exponential synapse is given by
+
+  .. math::
+
+      \begin{aligned}
+       & \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}).
+       \end{aligned}
+
+  .. [1] Sterratt, David, Bruce Graham, Andrew Gillies, and David Willshaw.
+          "The Synapse." Principles of Computational Modelling in Neuroscience.
+          Cambridge: Cambridge UP, 2011. 172-95. Print.
+
+  Args:
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
-      conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
-      output: SynOut = COBA(E=0.),
-      stp: Optional[SynSTP] = None,
-      comp_method: str = 'dense',
-      g_max: Union[float, ArrayType, Initializer, Callable] = 0.42,
-      delay_step: Union[int, ArrayType, Initializer, Callable] = None,
-      alpha: float = 0.98,
-      beta: float = 0.18,
-      T: float = 0.5,
-      T_duration: float = 0.5,
+      size: Union[int, Sequence[int]],
+      keep_size: bool = False,
+      sharding: Optional[Sequence[str]] = None,
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
+  ):
+    super().__init__(name=name,
+                     mode=mode,
+                     size=size,
+                     keep_size=keep_size,
+                     sharding=sharding)
+
+    self.reset_state(self.mode)
+
+  def reset_state(self, batch_size=None):
+    self.g = self.init_variable(bm.zeros, batch_size)
+
+  def update(self, x=None):
+    if x is not None:
+      self.g.value += x
+    return self.g.value
+
+  def add_current(self, x):
+    self.g.value += x
+
+  def return_info(self):
+    return self.g
+
+
+class Expon(SynDyn, AlignPost):
+  r"""Exponential decay synapse model.
+
+  **Model Descriptions**
+
+  The single exponential decay synapse model assumes the release of neurotransmitter,
+  its diffusion across the cleft, the receptor binding, and channel opening all happen
+  very quickly, so that the channels instantaneously jump from the closed to the open state.
+  Therefore, its expression is given by
+
+  .. math::
+
+      g_{\mathrm{syn}}(t)=g_{\mathrm{max}} e^{-\left(t-t_{0}\right) / \tau}
+
+  where :math:`\tau_{delay}` is the time constant of the synaptic state decay,
+  :math:`t_0` is the time of the pre-synaptic spike,
+  :math:`g_{\mathrm{max}}` is the maximal conductance.
+
+  Accordingly, the differential form of the exponential synapse is given by
+
+  .. math::
+
+      \begin{aligned}
+       & \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}).
+       \end{aligned}
+
+  .. [1] Sterratt, David, Bruce Graham, Andrew Gillies, and David Willshaw.
+          "The Synapse." Principles of Computational Modelling in Neuroscience.
+          Cambridge: Cambridge UP, 2011. 172-95. Print.
+
+  Args:
+    tau: float, ArrayType, Callable. The time constant of decay. [ms]
+    %s
+  """
+
+  def __init__(
+      self,
+      size: Union[int, Sequence[int]],
+      keep_size: bool = False,
+      sharding: Optional[Sequence[str]] = None,
       method: str = 'exp_auto',
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
 
-      # other parameters
-      name: str = None,
-      mode: bm.Mode = None,
-      stop_spike_gradient: bool = False,
+      # synapse parameters
+      tau: Union[float, ArrayType, Callable] = 8.0,
   ):
-    super(AMPA, self).__init__(pre=pre,
-                               post=post,
-                               conn=conn,
-                               output=output,
-                               stp=stp,
-                               name=name,
-                               mode=mode)
+    super().__init__(name=name,
+                     mode=mode,
+                     size=size,
+                     keep_size=keep_size,
+                     sharding=sharding)
 
     # parameters
-    self.stop_spike_gradient = stop_spike_gradient
-    self.comp_method = comp_method
-    self.alpha = alpha
-    self.beta = beta
-    self.T = T
-    self.T_duration = T_duration
-    if bm.size(alpha) != 1:
-      raise ValueError(f'"alpha" must be a scalar or a tensor with size of 1. But we got {alpha}')
-    if bm.size(beta) != 1:
-      raise ValueError(f'"beta" must be a scalar or a tensor with size of 1. But we got {beta}')
-    if bm.size(T) != 1:
-      raise ValueError(f'"T" must be a scalar or a tensor with size of 1. But we got {T}')
-    if bm.size(T_duration) != 1:
-      raise ValueError(f'"T_duration" must be a scalar or a tensor with size of 1. But we got {T_duration}')
-
-    # connection
-    self.g_max, self.conn_mask = self._init_weights(g_max, comp_method, sparse_data='ij')
-
-    # variables
-    self.g = variable(bm.zeros, self.mode, self.pre.num)
-    self.spike_arrival_time = variable(lambda s: bm.ones(s) * -1e7, self.mode, self.pre.num)
-    self.delay_step = self.register_delay(f"{self.pre.name}.spike", delay_step, self.pre.spike)
+    self.tau = self.init_param(tau)
+
+    # function
+    self.integral = odeint(self.derivative, method=method)
 
-    # functions
-    self.integral = odeint(method=method, f=self.dg)
+    self.reset_state(self.mode)
+
+  def derivative(self, g, t):
+    return -g / self.tau
 
   def reset_state(self, batch_size=None):
-    self.g = variable(bm.zeros, batch_size, self.pre.num)
-    self.spike_arrival_time = variable(lambda s: bm.ones(s) * -1e7, batch_size, self.pre.num)
-    self.output.reset_state(batch_size)
-    if self.stp is not None: self.stp.reset_state(batch_size)
-
-  def dg(self, g, t, TT):
-    dg = self.alpha * TT * (1 - g) - self.beta * g
-    return dg
-
-  def update(self, tdi, pre_spike=None):
-    t, dt = tdi['t'], tdi['dt']
-
-    # delays
-    if pre_spike is None:
-      pre_spike = self.get_delay_data(f"{self.pre.name}.spike", self.delay_step)
-    pre_spike = bm.as_jax(pre_spike)
-    if self.stop_spike_gradient:
-      pre_spike = stop_gradient(pre_spike)
-
-    # update sub-components
-    self.output.update(tdi)
-    if self.stp is not None: self.stp.update(tdi, pre_spike)
+    self.g = self.init_variable(bm.zeros, batch_size)
+
+  def update(self, x=None):
+    self.g.value = self.integral(self.g.value, share['t'], share['dt'])
+    if x is not None:
+      self.add_current(x)
+    return self.g.value
+
+  def add_current(self, x):
+    self.g.value += x
+
+  def return_info(self):
+    return self.g
+
 
+Expon.__doc__ = Expon.__doc__ % (pneu_doc,)
+
+
+class DualExpon(SynDyn):
+  r"""Dual exponential synapse model.
+
+  **Model Descriptions**
+
+  The dual exponential synapse model [1]_, also named as *difference of two exponentials* model,
+  is given by:
+
+  .. math::
+
+    g_{\mathrm{syn}}(t)=g_{\mathrm{max}} \frac{\tau_{1} \tau_{2}}{
+        \tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)
+        -\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right)
+
+  where :math:`\tau_1` is the time constant of the decay phase, :math:`\tau_2`
+  is the time constant of the rise phase, :math:`t_0` is the time of the pre-synaptic
+  spike, :math:`g_{\mathrm{max}}` is the maximal conductance.
+
+  However, in practice, this formula is hard to implement. The equivalent solution is
+  two coupled linear differential equations [2]_:
+
+  .. math::
+
+      \begin{aligned}
+      &\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\
+      &\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right),
+      \end{aligned}
+
+  .. [1] Sterratt, David, Bruce Graham, Andrew Gillies, and David Willshaw.
+         "The Synapse." Principles of Computational Modelling in Neuroscience.
+         Cambridge: Cambridge UP, 2011. 172-95. Print.
+  .. [2] Roth, A., & Van Rossum, M. C. W. (2009). Modeling Synapses. Computational
+         Modeling Methods for Neuroscientists.
+
+  Args:
+    tau_decay: float, ArrayArray, Callable. The time constant of the synaptic decay phase. [ms]
+    tau_rise: float, ArrayArray, Callable. The time constant of the synaptic rise phase. [ms]
+    %s
+  """
+
+  def __init__(
+      self,
+      size: Union[int, Sequence[int]],
+      keep_size: bool = False,
+      sharding: Optional[Sequence[str]] = None,
+      method: str = 'exp_auto',
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
+
+      # synapse parameters
+      tau_decay: Union[float, ArrayType, Callable] = 10.0,
+      tau_rise: Union[float, ArrayType, Callable] = 1.,
+  ):
+    super().__init__(name=name,
+                     mode=mode,
+                     size=size,
+                     keep_size=keep_size,
+                     sharding=sharding)
+
+    # parameters
+    self.tau_rise = self.init_param(tau_rise)
+    self.tau_decay = self.init_param(tau_decay)
+
+    # integrator
+    self.integral = odeint(JointEq(self.dg, self.dh), method=method)
+
+    self.reset_state(self.mode)
+
+  def reset_state(self, batch_size=None):
+    self.h = self.init_variable(bm.zeros, batch_size)
+    self.g = self.init_variable(bm.zeros, batch_size)
+
+  def dh(self, h, t):
+    return -h / self.tau_rise
+
+  def dg(self, g, t, h):
+    return -g / self.tau_decay + h
+
+  def update(self, x):
     # update synaptic variables
-    self.spike_arrival_time.value = bm.where(pre_spike, t, self.spike_arrival_time.value)
-    if isinstance(self.mode, bm.TrainingMode):
-      self.spike_arrival_time.value = stop_gradient(self.spike_arrival_time.value)
-    TT = ((t - self.spike_arrival_time) < self.T_duration) * self.T
-    self.g.value = self.integral(self.g, t, TT, dt)
-
-    # post-synaptic values
-    syn_value = self.g.value
-    if self.stp is not None: syn_value = self.stp(syn_value)
-    if isinstance(self.conn, All2All):
-      post_vs = self._syn2post_with_all2all(syn_value, self.g_max)
-    elif isinstance(self.conn, One2One):
-      post_vs = self._syn2post_with_one2one(syn_value, self.g_max)
-    else:
-      if self.comp_method == 'sparse':
-        f = lambda s: bm.sparse.csrmv(
-          self.g_max, self.conn_mask[0], self.conn_mask[1], s,
-          shape=(self.pre.num, self.post.num),
-          transpose=True
-        )
-        if isinstance(self.mode, bm.BatchingMode):
-          f = vmap(f)
-        post_vs = f(syn_value)
-      else:
-        post_vs = self._syn2post_with_dense(syn_value, self.g_max, self.conn_mask)
+    self.g.value, self.h.value = self.integral(self.g.value, self.h.value, share['t'], dt=share['dt'])
+    self.h += x
+    return self.g.value
+
+  def return_info(self):
+    return self.g
+
 
-    # output
-    return self.output(post_vs)
+DualExpon.__doc__ = DualExpon.__doc__ % (pneu_doc,)
 
 
-class GABAa(AMPA):
-  r"""GABAa synapse model.
+class DualExponV2(SynDyn, AlignPost):
+  r"""Dual exponential synapse model.
+
+  The dual exponential synapse model [1]_, also named as *difference of two exponentials* model,
+  is given by:
+
+  .. math::
+
+    g_{\mathrm{syn}}(t)=g_{\mathrm{max}} \frac{\tau_{1} \tau_{2}}{
+        \tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)
+        -\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right)
+
+  where :math:`\tau_1` is the time constant of the decay phase, :math:`\tau_2`
+  is the time constant of the rise phase, :math:`t_0` is the time of the pre-synaptic
+  spike, :math:`g_{\mathrm{max}}` is the maximal conductance.
+
+  .. [1] Sterratt, David, Bruce Graham, Andrew Gillies, and David Willshaw.
+         "The Synapse." Principles of Computational Modelling in Neuroscience.
+         Cambridge: Cambridge UP, 2011. 172-95. Print.
+  .. [2] Roth, A., & Van Rossum, M. C. W. (2009). Modeling Synapses. Computational
+         Modeling Methods for Neuroscientists.
+
+  Args:
+    tau_decay: float, ArrayArray, Callable. The time constant of the synaptic decay phase. [ms]
+    tau_rise: float, ArrayArray, Callable. The time constant of the synaptic rise phase. [ms]
+    %s
+  """
+
+  def __init__(
+      self,
+      size: Union[int, Sequence[int]],
+      keep_size: bool = False,
+      sharding: Optional[Sequence[str]] = None,
+      method: str = 'exp_auto',
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
+
+      # synapse parameters
+      tau_decay: Union[float, ArrayType, Callable] = 10.0,
+      tau_rise: Union[float, ArrayType, Callable] = 1.,
+  ):
+    super().__init__(name=name,
+                     mode=mode,
+                     size=size,
+                     keep_size=keep_size,
+                     sharding=sharding)
+
+    # parameters
+    self.tau_rise = self.init_param(tau_rise)
+    self.tau_decay = self.init_param(tau_decay)
+    self.coeff = self.tau_rise * self.tau_decay / (self.tau_decay - self.tau_rise)
+
+    # integrator
+    self.integral = odeint(lambda g, t, tau: -g / tau, method=method)
+
+    self.reset_state(self.mode)
+
+  def reset_state(self, batch_size=None):
+    self.g_rise = self.init_variable(bm.zeros, batch_size)
+    self.g_decay = self.init_variable(bm.zeros, batch_size)
+
+  def update(self, x=None):
+    self.g_rise.value = self.integral(self.g_rise.value, share['t'], self.tau_rise, share['dt'])
+    self.g_decay.value = self.integral(self.g_decay.value, share['t'], self.tau_decay, share['dt'])
+    if x is not None:
+      self.add_current(x)
+    return self.coeff * (self.g_decay - self.g_rise)
+
+  def add_current(self, inp):
+    self.g_rise += inp
+    self.g_decay += inp
+
+  def return_info(self):
+    return ReturnInfo(self.varshape, self.sharding, self.mode,
+                      lambda shape: self.coeff * (self.g_decay - self.g_rise))
+
+
+DualExponV2.__doc__ = DualExponV2.__doc__ % (pneu_doc,)
+
+
+class Alpha(DualExpon):
+  r"""Alpha synapse model.
 
   **Model Descriptions**
 
-  GABAa synapse model has the same equation with the `AMPA synapse <./brainmodels.synapses.AMPA.rst>`_,
+  The analytical expression of alpha synapse is given by:
 
   .. math::
 
-      \frac{d g}{d t}&=\alpha[T](1-g) - \beta g \\
-      I_{syn}&= - g_{max} g (V - E)
+      g_{syn}(t)= g_{max} \frac{t-t_{s}}{\tau} \exp \left(-\frac{t-t_{s}}{\tau}\right).
+
+  While, this equation is hard to implement. So, let's try to convert it into the
+  differential forms:
 
-  but with the difference of:
+  .. math::
 
-  - Reversal potential of synapse :math:`E` is usually low, typically -80. mV
-  - Activating rate constant :math:`\alpha=0.53`
-  - De-activating rate constant :math:`\beta=0.18`
-  - Transmitter concentration :math:`[T]=1\,\mu ho(\mu S)` when synapse is
-    triggered by a pre-synaptic spike, with the duration of 1. ms.
-
-  **Model Examples**
-
-  - `Gamma oscillation network model <https://brainpy-examples.readthedocs.io/en/latest/oscillation_synchronization/Wang_1996_gamma_oscillation.html>`_
-
-
-  Parameters
-  ----------
-  pre: NeuGroup
-    The pre-synaptic neuron group.
-  post: NeuGroup
-    The post-synaptic neuron group.
-  conn: optional, ArrayType, dict of (str, ndarray), TwoEndConnector
-    The synaptic connections.
-  comp_method: str
-    The connection type used for model speed optimization. It can be
-    `sparse` and `dense`. The default is `dense`.
-  delay_step: int, ArrayType, Initializer, Callable
-    The delay length. It should be the value of :math:`\mathrm{delay\_time / dt}`.
-  g_max: float, ArrayType, Initializer, Callable
-    The synaptic strength (the maximum conductance). Default is 1.
-  alpha: float, ArrayType
-    Binding constant. Default 0.062
-  beta: float, ArrayType
-    Unbinding constant. Default 3.57
-  T: float, ArrayType
-    Transmitter concentration when synapse is triggered by
-    a pre-synaptic spike.. Default 1 [mM].
-  T_duration: float, ArrayType
-    Transmitter concentration duration time after being triggered. Default 1 [ms]
-  name: str
-    The name of this synaptic projection.
-  method: str
-    The numerical integration methods.
-
-  References
-  ----------
-  .. [1] Destexhe, Alain, and Denis Par. "Impact of network activity
-         on the integrative properties of neocortical pyramidal neurons
-         in vivo." Journal of neurophysiology 81.4 (1999): 1531-1547.
+      \begin{aligned}
+      &\frac{d g}{d t}=-\frac{g}{\tau}+h \\
+      &\frac{d h}{d t}=-\frac{h}{\tau}+\delta\left(t_{0}-t\right)
+      \end{aligned}
+
+  .. [1] Sterratt, David, Bruce Graham, Andrew Gillies, and David Willshaw.
+          "The Synapse." Principles of Computational Modelling in Neuroscience.
+          Cambridge: Cambridge UP, 2011. 172-95. Print.
+
+  Args:
+    tau_decay: float, ArrayType, Callable. The time constant [ms] of the synaptic decay phase.
+       The name of this synaptic projection.
+    %s
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
-      conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
-      output: SynOut = COBA(E=-80.),
-      stp: Optional[SynSTP] = None,
-      comp_method: str = 'dense',
-      g_max: Union[float, ArrayType, Initializer, Callable] = 0.04,
-      delay_step: Union[int, ArrayType, Initializer, Callable] = None,
-      alpha: Union[float, ArrayType] = 0.53,
-      beta: Union[float, ArrayType] = 0.18,
-      T: Union[float, ArrayType] = 1.,
-      T_duration: Union[float, ArrayType] = 1.,
+      size: Union[int, Sequence[int]],
+      keep_size: bool = False,
+      sharding: Optional[Sequence[str]] = None,
       method: str = 'exp_auto',
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
 
-      # other parameters
-      name: str = None,
-      mode: bm.Mode = None,
-      stop_spike_gradient: bool = False,
+      # synapse parameters
+      tau_decay: Union[float, ArrayType, Callable] = 10.0,
   ):
-    super(GABAa, self).__init__(pre=pre,
-                                post=post,
-                                conn=conn,
-                                output=output,
-                                stp=stp,
-                                comp_method=comp_method,
-                                delay_step=delay_step,
-                                g_max=g_max,
-                                alpha=alpha,
-                                beta=beta,
-                                T=T,
-                                T_duration=T_duration,
-                                method=method,
-                                name=name,
-                                mode=mode,
-                                stop_spike_gradient=stop_spike_gradient, )
+    super().__init__(
+      tau_decay=tau_decay,
+      tau_rise=tau_decay,
+      method=method,
+      name=name,
+      mode=mode,
+      size=size,
+      keep_size=keep_size,
+      sharding=sharding
+    )
+
+
+Alpha.__doc__ = Alpha.__doc__ % (pneu_doc,)
 
 
-class BioNMDA(TwoEndConn):
-  r"""Biological NMDA synapse model.
+class NMDA(SynDyn):
+  r"""NMDA synapse model.
 
   **Model Descriptions**
 
   The NMDA receptor is a glutamate receptor and ion channel found in neurons.
   The NMDA receptor is one of three types of ionotropic glutamate receptors,
   the other two being AMPA and kainate receptors.
 
@@ -358,16 +413,16 @@
   from the outside by a positively charged magnesium ion. The channel is
   nearly completely blocked at resting potential, but the magnesium block is
   relieved if the cell is depolarized. The fraction of channels :math:`g_{\infty}`
   that are not blocked by magnesium can be fitted to
 
   .. math::
 
-      g_{\infty}(V,[{Mg}^{2+}]_{o}) = (1+{e}^{-a V}
-      \frac{[{Mg}^{2+}]_{o}} {b})^{-1}
+      g_{\infty}(V,[{Mg}^{2+}]_{o}) = (1+{e}^{-\alpha V}
+      \frac{[{Mg}^{2+}]_{o}} {\beta})^{-1}
 
   Here :math:`[{Mg}^{2+}]_{o}` is the extracellular magnesium concentration,
   usually 1 mM. Thus, the channel acts as a
   "coincidence detector" and only once both of these conditions are met, the
   channel opens and it allows positively charged ions (cations) to flow through
   the cell membrane [2]_.
 
@@ -378,210 +433,266 @@
   .. math::
 
       I_{syn} = g_\mathrm{NMDA}(t) (V(t)-E) \cdot g_{\infty}
 
   where :math:`V(t)` is the post-synaptic neuron potential, :math:`E` is the
   reversal potential.
 
-  Simultaneously, the kinetics of synaptic state :math:`g` is determined by a 2nd-order kinetics [1]_:
+  Simultaneously, the kinetics of synaptic state :math:`g` is given by
 
   .. math::
 
       & g_\mathrm{NMDA} (t) = g_{max} g \\
-      & \frac{d g}{dt} = \alpha_1 x (1 - g) - \beta_1 g \\
-      & \frac{d x}{dt} = \alpha_2 [T] (1 - x) - \beta_2 x
+      & \frac{d g}{dt} = -\frac{g} {\tau_{decay}}+a x(1-g) \\
+      & \frac{d x}{dt} = -\frac{x}{\tau_{rise}}+ \sum_{k} \delta(t-t_{j}^{k})
 
-  where :math:`\alpha_1, \beta_1` refers to the conversion rate of variable g and
-  :math:`\alpha_2, \beta_2` refers to the conversion rate of variable x.
+  where the decay time of NMDA currents is usually taken to be
+  :math:`\tau_{decay}` =100 ms, :math:`a= 0.5 ms^{-1}`, and :math:`\tau_{rise}` =2 ms.
 
   The NMDA receptor has been thought to be very important for controlling
   synaptic plasticity and mediating learning and memory functions [3]_.
 
-  .. plot::
-    :include-source: True
-
-    >>> import brainpy as bp
-    >>> from brainpy import neurons, synapses
-    >>> import matplotlib.pyplot as plt
-    >>>
-    >>> neu1 = neurons.HH(1)
-    >>> neu2 = neurons.HH(1)
-    >>> syn1 = synapses.BioNMDA(neu1, neu2, bp.connect.All2All())
-    >>> net = bp.Network(pre=neu1, syn=syn1, post=neu2)
-    >>>
-    >>> runner = bp.DSRunner(net, inputs=[('pre.input', 5.)], monitors=['pre.V', 'post.V', 'syn.g', 'syn.x'])
-    >>> runner.run(150.)
-    >>>
-    >>> fig, gs = bp.visualize.get_figure(2, 1, 3, 8)
-    >>> fig.add_subplot(gs[0, 0])
-    >>> plt.plot(runner.mon.ts, runner.mon['pre.V'], label='pre-V')
-    >>> plt.plot(runner.mon.ts, runner.mon['post.V'], label='post-V')
-    >>> plt.legend()
-    >>>
-    >>> fig.add_subplot(gs[1, 0])
-    >>> plt.plot(runner.mon.ts, runner.mon['syn.g'], label='g')
-    >>> plt.plot(runner.mon.ts, runner.mon['syn.x'], label='x')
-    >>> plt.legend()
-    >>> plt.show()
-
-  Parameters
-  ----------
-  pre: NeuGroup
-    The pre-synaptic neuron group.
-  post: NeuGroup
-    The post-synaptic neuron group.
-  conn: optional, ArrayType, dict of (str, ndarray), TwoEndConnector
-    The synaptic connections.
-  comp_method: str
-    The connection type used for model speed optimization. It can be
-    `sparse` and `dense`. The default is `dense`.
-  delay_step: int, ArrayType, Initializer, Callable
-    The delay length. It should be the value of :math:`\mathrm{delay\_time / dt}`.
-  g_max: float, ArrayType, Initializer, Callable
-    The synaptic strength (the maximum conductance). Default is 1.
-  alpha1: float, ArrayType
-    The conversion rate of g from inactive to active. Default 2 ms^-1.
-  beta1: float, ArrayType
-    The conversion rate of g from active to inactive. Default 0.01 ms^-1.
-  alpha2: float, ArrayType
-    The conversion rate of x from inactive to active. Default 1 ms^-1.
-  beta2: float, ArrayType
-    The conversion rate of x from active to inactive. Default 0.5 ms^-1.
-  name: str
-    The name of this synaptic projection.
-  method: str
-    The numerical integration methods.
-
-  References
-  ----------
 
-  .. [1] Devaney A J . Mathematical Foundations of Neuroscience[M].
-         Springer New York, 2010: 162.
+  .. [1] Brunel N, Wang X J. Effects of neuromodulation in a
+         cortical network model of object working memory dominated
+         by recurrent inhibition[J].
+         Journal of computational neuroscience, 2001, 11(1): 63-85.
   .. [2] Furukawa, Hiroyasu, Satinder K. Singh, Romina Mancusso, and
          Eric Gouaux. "Subunit arrangement and function in NMDA receptors."
          Nature 438, no. 7065 (2005): 185-192.
   .. [3] Li, F. and Tsien, J.Z., 2009. Memory and the NMDA receptors. The New
          England journal of medicine, 361(3), p.302.
   .. [4] https://en.wikipedia.org/wiki/NMDA_receptor
 
+  Args:
+    tau_decay: float, ArrayType, Callable. The time constant of the synaptic decay phase. Default 100 [ms]
+    tau_rise: float, ArrayType, Callable. The time constant of the synaptic rise phase. Default 2 [ms]
+    a: float, ArrayType, Callable. Default 0.5 ms^-1.
+    %s
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
-      conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
-      output: SynOut = MgBlock(E=0.),
-      stp: Optional[SynSTP] = None,
-      comp_method: str = 'dense',
-      g_max: Union[float, ArrayType, Initializer, Callable] = 0.15,
-      delay_step: Union[int, ArrayType, Initializer, Callable] = None,
-      alpha1: Union[float, ArrayType] = 2.,
-      beta1: Union[float, ArrayType] = 0.01,
-      alpha2: Union[float, ArrayType] = 1.,
-      beta2: Union[float, ArrayType] = 0.5,
-      T_0: Union[float, ArrayType] = 1.,
-      T_dur: Union[float, ArrayType] = 0.5,
+      size: Union[int, Sequence[int]],
+      keep_size: bool = False,
+      sharding: Optional[Sequence[str]] = None,
       method: str = 'exp_auto',
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
 
-      # other parameters
-      mode: bm.Mode = None,
-      name: str = None,
-      stop_spike_gradient: bool = False,
+      # synapse parameters
+      a: Union[float, ArrayType, Callable] = 0.5,
+      tau_decay: Union[float, ArrayType, Callable] = 100.,
+      tau_rise: Union[float, ArrayType, Callable] = 2.,
   ):
-    super(BioNMDA, self).__init__(pre=pre,
-                                  post=post,
-                                  conn=conn,
-                                  output=output,
-                                  stp=stp,
-                                  name=name,
-                                  mode=mode)
+    super().__init__(name=name,
+                     mode=mode,
+                     size=size,
+                     keep_size=keep_size,
+                     sharding=sharding)
 
     # parameters
-    self.beta1 = beta1
-    self.beta2 = beta2
-    self.alpha1 = alpha1
-    self.alpha2 = alpha2
-    self.T_0 = T_0
-    self.T_dur = T_dur
-    if bm.size(alpha1) != 1:
-      raise ValueError(f'"alpha1" must be a scalar or a tensor with size of 1. But we got {alpha1}')
-    if bm.size(beta1) != 1:
-      raise ValueError(f'"beta1" must be a scalar or a tensor with size of 1. But we got {beta1}')
-    if bm.size(alpha2) != 1:
-      raise ValueError(f'"alpha2" must be a scalar or a tensor with size of 1. But we got {alpha2}')
-    if bm.size(beta2) != 1:
-      raise ValueError(f'"beta2" must be a scalar or a tensor with size of 1. But we got {beta2}')
-    if bm.size(T_0) != 1:
-      raise ValueError(f'"T_0" must be a scalar or a tensor with size of 1. But we got {T_0}')
-    if bm.size(T_dur) != 1:
-      raise ValueError(f'"T_dur" must be a scalar or a tensor with size of 1. But we got {T_dur}')
-    self.comp_method = comp_method
-    self.stop_spike_gradient = stop_spike_gradient
-
-    # connections and weights
-    self.g_max, self.conn_mask = self._init_weights(g_max, comp_method, sparse_data='ij')
-
-    # variables
-    self.g = variable(bm.zeros, self.mode, self.pre.num)
-    self.x = variable(bm.zeros, self.mode, self.pre.num)
-    self.spike_arrival_time = variable(lambda s: bm.ones(s) * -1e7, self.mode, self.pre.num)
-    self.delay_step = self.register_delay(f"{self.pre.name}.spike", delay_step, self.pre.spike)
+    self.tau_decay = self.init_param(tau_decay)
+    self.tau_rise = self.init_param(tau_rise)
+    self.a = self.init_param(a)
 
     # integral
-    self.integral = odeint(method=method, f=JointEq([self.dg, self.dx]))
+    self.integral = odeint(method=method, f=JointEq(self.dg, self.dx))
 
-  def reset_state(self, batch_size=None):
-    self.g = variable(bm.zeros, batch_size, self.pre.num)
-    self.x = variable(bm.zeros, batch_size, self.pre.num)
-    self.spike_arrival_time = variable(lambda s: bm.ones(s) * -1e7, batch_size, self.pre.num)
-    self.output.reset_state(batch_size)
-    if self.stp is not None: self.stp.reset_state(batch_size)
+    self.reset_state(self.mode)
 
   def dg(self, g, t, x):
-    return self.alpha1 * x * (1 - g) - self.beta1 * g
+    return -g / self.tau_decay + self.a * x * (1 - g)
+
+  def dx(self, x, t):
+    return -x / self.tau_rise
+
+  def reset_state(self, batch_size=None):
+    self.g = self.init_variable(bm.zeros, batch_size)
+    self.x = self.init_variable(bm.zeros, batch_size)
+
+  def update(self, pre_spike):
+    t = share.load('t')
+    dt = share.load('dt')
+    self.g.value, self.x.value = self.integral(self.g, self.x, t, dt=dt)
+    self.x += pre_spike
+    return self.g.value
+
+  def return_info(self):
+    return self.g
+
+
+NMDA.__doc__ = NMDA.__doc__ % (pneu_doc,)
+
+
+class STD(SynDyn):
+  r"""Synaptic output with short-term depression.
+
+  This model filters the synaptic current by the following equation:
+
+  .. math::
+
+     I_{syn}^+(t) = I_{syn}^-(t) * x
+
+  where :math:`x` is the normalized variable between 0 and 1, and
+  :math:`I_{syn}^-(t)` and :math:`I_{syn}^+(t)` are the synaptic currents before
+  and after STD filtering.
+
+  Moreover, :math:`x` is updated according to the dynamics of:
+
+  .. math::
+
+     \frac{dx}{dt} = \frac{1-x}{\tau} - U * x * \delta(t-t_{spike})
+
+  where :math:`U` is the fraction of resources used per action potential,
+  :math:`\tau` is the time constant of recovery of the synaptic vesicles.
 
-  def dx(self, x, t, T):
-    return self.alpha2 * T * (1 - x) - self.beta2 * x
+  Args:
+    tau: float, ArrayType, Callable. The time constant of recovery of the synaptic vesicles.
+    U: float, ArrayType, Callable. The fraction of resources used per action potential.
+    %s
+  """
+
+  def __init__(
+      self,
+      size: Union[int, Sequence[int]],
+      keep_size: bool = False,
+      sharding: Optional[Sequence[str]] = None,
+      method: str = 'exp_auto',
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
+
+      # synapse parameters
+      tau: Union[float, ArrayType, Callable] = 200.,
+      U: Union[float, ArrayType, Callable] = 0.07,
+  ):
+    super().__init__(name=name,
+                     mode=mode,
+                     size=size,
+                     keep_size=keep_size,
+                     sharding=sharding)
+
+    # parameters
+    self.tau = self.init_param(tau)
+    self.U = self.init_param(U)
+
+    # integral function
+    self.integral = odeint(lambda x, t: (1 - x) / self.tau, method=method)
+
+    self.reset_state(self.mode)
+
+  def reset_state(self, batch_size=None):
+    self.x = self.init_variable(bm.ones, batch_size)
+
+  def update(self, pre_spike):
+    t = share.load('t')
+    dt = share.load('dt')
+    x = self.integral(self.x.value, t, dt)
+    self.x.value = bm.where(pre_spike, x - self.U * self.x, x)
+    return self.x.value
+
+  def return_info(self):
+    return self.x
+
+
+STD.__doc__ = STD.__doc__ % (pneu_doc,)
+
+
+class STP(SynDyn):
+  r"""Synaptic output with short-term plasticity.
+
+  This model filters the synaptic currents according to two variables: :math:`u` and :math:`x`.
 
-  def update(self, tdi, pre_spike=None):
-    t, dt = tdi['t'], tdi['dt']
+  .. math::
+
+     I_{syn}^+(t) = I_{syn}^-(t) * x * u
+
+  where :math:`I_{syn}^-(t)` and :math:`I_{syn}^+(t)` are the synaptic currents before
+  and after STP filtering, :math:`x` denotes the fraction of resources that remain available
+  after neurotransmitter depletion, and :math:`u` represents the fraction of available
+  resources ready for use (release probability).
+
+  The dynamics of :math:`u` and :math:`x` are governed by
+
+  .. math::
+
+     \begin{aligned}
+    \frac{du}{dt} & = & -\frac{u}{\tau_f}+U(1-u^-)\delta(t-t_{sp}), \\
+    \frac{dx}{dt} & = & \frac{1-x}{\tau_d}-u^+x^-\delta(t-t_{sp}), \\
+    \tag{1}\end{aligned}
+
+  where :math:`t_{sp}` denotes the spike time and :math:`U` is the increment
+  of :math:`u` produced by a spike. :math:`u^-, x^-` are the corresponding
+  variables just before the arrival of the spike, and :math:`u^+`
+  refers to the moment just after the spike.
+
+  Args:
+    tau_f: float, ArrayType, Callable. The time constant of short-term facilitation.
+    tau_d: float, ArrayType, Callable. The time constant of short-term depression.
+    U: float, ArrayType, Callable. The fraction of resources used per action potential.
+    %s
+  """
+
+  def __init__(
+      self,
+      size: Union[int, Sequence[int]],
+      keep_size: bool = False,
+      sharding: Optional[Sequence[str]] = None,
+      method: str = 'exp_auto',
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
+
+      # synapse parameters
+      U: Union[float, ArrayType, Callable] = 0.15,
+      tau_f: Union[float, ArrayType, Callable] = 1500.,
+      tau_d: Union[float, ArrayType, Callable] = 200.,
+  ):
+    super().__init__(name=name,
+                     mode=mode,
+                     size=size,
+                     keep_size=keep_size,
+                     sharding=sharding)
+
+    # parameters
+    self.tau_f = self.init_param(tau_f)
+    self.tau_d = self.init_param(tau_d)
+    self.U = self.init_param(U)
+    self.method = method
+
+    # integral function
+    self.integral = odeint(self.derivative, method=self.method)
+
+    self.reset_state(self.mode)
+
+  def reset_state(self, batch_size=None):
+    self.x = self.init_variable(bm.ones, batch_size)
+    self.u = self.init_variable(bm.ones, batch_size)
+    self.u.fill_(self.U)
+
+  @property
+  def derivative(self):
+    du = lambda u, t: self.U - u / self.tau_f
+    dx = lambda x, t: (1 - x) / self.tau_d
+    return JointEq(du, dx)
+
+  def update(self, pre_spike):
+    t = share.load('t')
+    dt = share.load('dt')
+    u, x = self.integral(self.u.value, self.x.value, t, dt)
+    # if pre_spike.dtype == jax.numpy.bool_:
+    #   u = bm.where(pre_spike, u + self.U * (1 - self.u), u)
+    #   x = bm.where(pre_spike, x - u * self.x, x)
+    # else:
+    # u = pre_spike * (u + self.U * (1 - self.u)) + (1 - pre_spike) * u
+    # x = pre_spike * (x - u * self.x) + (1 - pre_spike) * x
+    u = pre_spike * self.U * (1 - self.u) + u
+    x = pre_spike * -u * self.x + x
+    self.x.value = x
+    self.u.value = u
+    return u * x
+
+  def return_info(self):
+    return ReturnInfo(self.varshape, self.sharding, self.mode,
+                      lambda shape: self.u * self.x)
 
-    # pre-synaptic spikes
-    if pre_spike is None:
-      pre_spike = self.get_delay_data(f"{self.pre.name}.spike", self.delay_step)
-    pre_spike = bm.as_jax(pre_spike)
-    if self.stop_spike_gradient:
-      pre_spike = stop_gradient(pre_spike)
-
-    # update sub-components
-    self.output.update(tdi)
-    if self.stp is not None: self.stp.update(tdi, pre_spike)
-
-    # update synapse variables
-    self.spike_arrival_time.value = bm.where(pre_spike, t, self.spike_arrival_time.value)
-    if isinstance(self.mode, bm.TrainingMode):
-      self.spike_arrival_time.value = stop_gradient(self.spike_arrival_time.value)
-    T = ((t - self.spike_arrival_time) < self.T_dur) * self.T_0
-    self.g.value, self.x.value = self.integral(self.g, self.x, t, T, dt)
-
-    # post-synaptic value
-    syn_value = self.g.value
-    if self.stp is not None: syn_value = self.stp(syn_value)
-    if isinstance(self.conn, All2All):
-      post_vs = self._syn2post_with_all2all(syn_value, self.g_max)
-    elif isinstance(self.conn, One2One):
-      post_vs = self._syn2post_with_one2one(syn_value, self.g_max)
-    else:
-      if self.comp_method == 'sparse':
-        f = lambda s: bm.sparse.csrmv(
-          self.g_max,self.conn_mask[0], self.conn_mask[1], s,
-          shape=(self.pre.num, self.post.num),
-          transpose=True
-        )
-        if isinstance(self.mode, bm.BatchingMode): f = vmap(f)
-        post_vs = f(syn_value)
-      else:
-        post_vs = self._syn2post_with_dense(syn_value, self.g_max, self.conn_mask)
 
-    # output
-    return self.output(post_vs)
+STP.__doc__ = STP.__doc__ % (pneu_doc,)
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## Comparing `brainpy/_src/synapses/compat.py` & `brainpy/_src/dynold/synapses/compat.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,300 +1,257 @@
 # -*- coding: utf-8 -*-
 
 import warnings
-from typing import Union, Dict, Callable, Optional
+from typing import Union, Dict, Callable
 
-import brainpy._src.math as bm
 from brainpy._src.connect import TwoEndConnector
-from brainpy._src.dynsys import NeuGroup, SynSTP
-from brainpy._src.synouts import COBA, CUBA, MgBlock
+from brainpy._src.dynold.synouts import COBA, CUBA
+from brainpy._src.dyn.base import NeuDyn
 from brainpy._src.initialize import Initializer
 from brainpy.types import ArrayType
-from .abstract_models import Delta, Exponential, DualExponential, NMDA as NewNMDA
+from .abstract_models import Delta, Exponential, DualExponential
 
 __all__ = [
   'DeltaSynapse',
   'ExpCUBA',
   'ExpCOBA',
   'DualExpCUBA',
   'DualExpCOBA',
   'AlphaCUBA',
   'AlphaCOBA',
-  'NMDA',
 ]
 
 
 class DeltaSynapse(Delta):
   """Delta synapse.
 
   .. deprecated:: 2.1.13
      Please use "brainpy.synapses.Delta" instead.
 
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
       conn_type: str = 'sparse',
       weights: Union[float, ArrayType, Initializer, Callable] = 1.,
       delay_step: Union[float, ArrayType, Initializer, Callable] = None,
       post_input_key: str = 'V',
       post_has_ref: bool = False,
       name: str = None,
   ):
     warnings.warn('Please use "brainpy.synapses.Delta" instead.', DeprecationWarning)
-    super(DeltaSynapse, self).__init__(pre=pre,
-                                       post=post,
-                                       conn=conn,
-                                       output=CUBA(post_input_key),
-                                       name=name,
-                                       comp_method=conn_type,
-                                       g_max=weights,
-                                       delay_step=delay_step,
-                                       post_ref_key='refractory' if post_has_ref else None)
+    super().__init__(pre=pre,
+                     post=post,
+                     conn=conn,
+                     output=CUBA(post_input_key),
+                     name=name,
+                     comp_method=conn_type,
+                     g_max=weights,
+                     delay_step=delay_step,
+                     post_ref_key='refractory' if post_has_ref else None)
 
 
 class ExpCUBA(Exponential):
   r"""Current-based exponential decay synapse model.
 
   .. deprecated:: 2.1.13
      Please use "brainpy.synapses.Exponential" instead.
 
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
       conn_type: str = 'sparse',
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       tau: Union[float, ArrayType] = 8.0,
       name: str = None,
       method: str = 'exp_auto',
   ):
-    super(ExpCUBA, self).__init__(pre=pre,
-                                  post=post,
-                                  conn=conn,
-                                  name=name,
-                                  comp_method=conn_type,
-                                  g_max=g_max,
-                                  delay_step=delay_step,
-                                  tau=tau,
-                                  method=method,
-                                  output=CUBA())
+    super().__init__(pre=pre,
+                     post=post,
+                     conn=conn,
+                     name=name,
+                     comp_method=conn_type,
+                     g_max=g_max,
+                     delay_step=delay_step,
+                     tau=tau,
+                     method=method,
+                     output=CUBA())
 
 
 class ExpCOBA(Exponential):
   """Conductance-based exponential decay synapse model.
 
   .. deprecated:: 2.1.13
      Please use "brainpy.synapses.Exponential" instead.
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       # connection
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
       conn_type: str = 'sparse',
       # connection strength
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       # synapse parameter
       tau: Union[float, ArrayType] = 8.0,
       E: Union[float, ArrayType] = 0.,
       # synapse delay
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       # others
       method: str = 'exp_auto',
       name: str = None
   ):
-    super(ExpCOBA, self).__init__(pre=pre,
-                                  post=post,
-                                  conn=conn,
-                                  comp_method=conn_type,
-                                  g_max=g_max,
-                                  delay_step=delay_step,
-                                  tau=tau,
-                                  method=method,
-                                  name=name,
-                                  output=COBA(E=E))
+    super().__init__(pre=pre,
+                     post=post,
+                     conn=conn,
+                     comp_method=conn_type,
+                     g_max=g_max,
+                     delay_step=delay_step,
+                     tau=tau,
+                     method=method,
+                     name=name,
+                     output=COBA(E=E))
 
 
 class DualExpCUBA(DualExponential):
   r"""Current-based dual exponential synapse model.
 
   .. deprecated:: 2.1.13
      Please use "brainpy.synapses.DualExponential" instead.
 
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
       conn_type: str = 'dense',
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       tau_decay: Union[float, ArrayType] = 10.0,
       tau_rise: Union[float, ArrayType] = 1.,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       method: str = 'exp_auto',
       name: str = None
   ):
-    super(DualExpCUBA, self).__init__(pre=pre,
-                                      post=post,
-                                      conn=conn,
-                                      comp_method=conn_type,
-                                      g_max=g_max,
-                                      tau_decay=tau_decay,
-                                      tau_rise=tau_rise,
-                                      delay_step=delay_step,
-                                      method=method,
-                                      name=name,
-                                      output=CUBA())
+    super().__init__(pre=pre,
+                     post=post,
+                     conn=conn,
+                     comp_method=conn_type,
+                     g_max=g_max,
+                     tau_decay=tau_decay,
+                     tau_rise=tau_rise,
+                     delay_step=delay_step,
+                     method=method,
+                     name=name,
+                     output=CUBA())
 
 
 class DualExpCOBA(DualExponential):
   """Conductance-based dual exponential synapse model.
 
 
   .. deprecated:: 2.1.13
      Please use "brainpy.synapses.DualExponential" instead.
 
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
       conn_type: str = 'dense',
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       tau_decay: Union[float, ArrayType] = 10.0,
       tau_rise: Union[float, ArrayType] = 1.,
       E: Union[float, ArrayType] = 0.,
       method: str = 'exp_auto',
       name: str = None
   ):
-    super(DualExpCOBA, self).__init__(pre=pre,
-                                      post=post,
-                                      conn=conn,
-                                      comp_method=conn_type,
-                                      g_max=g_max,
-                                      tau_decay=tau_decay,
-                                      tau_rise=tau_rise,
-                                      delay_step=delay_step,
-                                      method=method,
-                                      name=name,
-                                      output=COBA(E=E))
+    super().__init__(pre=pre,
+                     post=post,
+                     conn=conn,
+                     comp_method=conn_type,
+                     g_max=g_max,
+                     tau_decay=tau_decay,
+                     tau_rise=tau_rise,
+                     delay_step=delay_step,
+                     method=method,
+                     name=name,
+                     output=COBA(E=E))
 
 
 class AlphaCUBA(DualExpCUBA):
   r"""Current-based alpha synapse model.
 
   .. deprecated:: 2.1.13
      Please use "brainpy.synapses.Alpha" instead.
 
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
       conn_type: str = 'dense',
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       tau_decay: Union[float, ArrayType] = 10.0,
       method: str = 'exp_auto',
       name: str = None
   ):
-    super(AlphaCUBA, self).__init__(pre=pre,
-                                    post=post,
-                                    conn=conn,
-                                    conn_type=conn_type,
-                                    delay_step=delay_step,
-                                    g_max=g_max,
-                                    tau_decay=tau_decay,
-                                    tau_rise=tau_decay,
-                                    method=method,
-                                    name=name)
+    super().__init__(pre=pre,
+                     post=post,
+                     conn=conn,
+                     conn_type=conn_type,
+                     delay_step=delay_step,
+                     g_max=g_max,
+                     tau_decay=tau_decay,
+                     tau_rise=tau_decay,
+                     method=method,
+                     name=name)
 
 
 class AlphaCOBA(DualExpCOBA):
   """Conductance-based alpha synapse model.
 
   .. deprecated:: 2.1.13
      Please use "brainpy.synapses.Alpha" instead.
 
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
       conn_type: str = 'dense',
       g_max: Union[float, ArrayType, Callable, Initializer] = 1.,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       tau_decay: Union[float, ArrayType] = 10.0,
       E: Union[float, ArrayType] = 0.,
       method: str = 'exp_auto',
       name: str = None
   ):
-    super(AlphaCOBA, self).__init__(pre=pre,
-                                    post=post,
-                                    conn=conn,
-                                    conn_type=conn_type,
-                                    delay_step=delay_step,
-                                    g_max=g_max, E=E,
-                                    tau_decay=tau_decay,
-                                    tau_rise=tau_decay,
-                                    method=method,
-                                    name=name)
-
-
-class NMDA(NewNMDA):
-  def __init__(
-      self,
-      pre: NeuGroup,
-      post: NeuGroup,
-      conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
-      E=0.,
-      alpha=0.062,
-      beta=3.57,
-      cc_Mg=1.2,
-      stp: Optional[SynSTP] = None,
-      comp_method: str = 'dense',
-      g_max: Union[float, ArrayType, Initializer, Callable] = 0.15,
-      delay_step: Union[int, ArrayType, Initializer, Callable] = None,
-      tau_decay: Union[float, ArrayType] = 100.,
-      a: Union[float, ArrayType] = 0.5,
-      tau_rise: Union[float, ArrayType] = 2.,
-      method: str = 'exp_auto',
-
-      # other parameters
-      name: str = None,
-      mode: bm.Mode = None,
-      stop_spike_gradient: bool = False,
-  ):
-    super(NMDA, self).__init__(pre=pre,
-                               post=post,
-                               conn=conn,
-                               output=MgBlock(E=E, alpha=alpha, beta=beta, cc_Mg=cc_Mg),
-                               stp=stp,
-                               name=name,
-                               mode=mode,
-                               comp_method=comp_method,
-                               g_max=g_max,
-                               delay_step=delay_step,
-                               tau_decay=tau_decay,
-                               a=a,
-                               tau_rise=tau_rise,
-                               method=method,
-                               stop_spike_gradient=stop_spike_gradient)
+    super().__init__(pre=pre,
+                     post=post,
+                     conn=conn,
+                     conn_type=conn_type,
+                     delay_step=delay_step,
+                     g_max=g_max, E=E,
+                     tau_decay=tau_decay,
+                     tau_rise=tau_decay,
+                     method=method,
+                     name=name)
```

## Comparing `brainpy/_src/synapses/delay_couplings.py` & `brainpy/_src/dyn/synapses/delay_couplings.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 # -*- coding: utf-8 -*-
 
+import numbers
 from typing import Optional, Union, Sequence, Tuple, Callable
 
 import jax.numpy as jnp
 from jax import vmap
 
 import brainpy.math as bm
-from brainpy._src.dynsys import SynConn
-from brainpy._src.neurons.input_groups import InputGroup, OutputGroup
+from brainpy._src.dynsys import Projection
 from brainpy._src.initialize import Initializer
 from brainpy.check import is_sequence
 from brainpy.types import ArrayType
 
 __all__ = [
   'DelayCoupling',
   'DiffusiveCoupling',
   'AdditiveCoupling',
 ]
 
 
-class DelayCoupling(SynConn):
+class DelayCoupling(Projection):
   """Delay coupling.
 
   Parameters
   ----------
   delay_var: Variable
     The delay variable.
   var_to_output: Variable, sequence of Variable
@@ -40,23 +40,20 @@
 
   def __init__(
       self,
       delay_var: bm.Variable,
       var_to_output: Union[bm.Variable, Sequence[bm.Variable]],
       conn_mat: ArrayType,
       required_shape: Tuple[int, ...],
-      delay_steps: Optional[Union[int, ArrayType, Initializer, Callable]] = None,
-      initial_delay_data: Union[Initializer, Callable, ArrayType, float, int, bool] = None,
-      name: str = None,
-      mode: bm.Mode = None,
+      delay_steps: Optional[Union[int, ArrayType, Callable]] = None,
+      initial_delay_data: Union[Callable, ArrayType, numbers.Number] = None,
+      name: Optional[str] = None,
+      mode: Optional[bm.Mode] = None,
   ):
-    super(DelayCoupling, self).__init__(name=name,
-                                        mode=mode,
-                                        pre=InputGroup(1),
-                                        post=OutputGroup(1))
+    super().__init__(name=name, mode=mode)
 
     # delay variable
     if not isinstance(delay_var, bm.Variable):
       raise ValueError(f'"delay_var" must be an instance of brainpy.math.Variable. '
                        f'But we got {type(delay_var)}')
     self.delay_var = delay_var
 
@@ -173,32 +170,32 @@
     if jnp.ndim(coupling_var1) != 1:
       raise ValueError(f'Only support 1d vector of coupling variable. '
                        f'But we got {jnp.ndim(coupling_var1)}')
     if jnp.ndim(coupling_var2) != 1:
       raise ValueError(f'Only support 1d vector of coupling variable. '
                        f'But we got {jnp.ndim(coupling_var2)}')
 
-    super(DiffusiveCoupling, self).__init__(
+    super().__init__(
       delay_var=coupling_var1,
       var_to_output=var_to_output,
       conn_mat=conn_mat,
       required_shape=(coupling_var1.size, coupling_var2.size),
       delay_steps=delay_steps,
       initial_delay_data=initial_delay_data,
       name=name,
       mode=mode,
     )
 
     self.coupling_var1 = coupling_var1
     self.coupling_var2 = coupling_var2
 
-  def update(self, tdi):
+  def update(self):
     # delays
     axis = self.coupling_var1.ndim
-    delay_var: bm.LengthDelay = self.global_delay_data[f'delay_{id(self.delay_var)}'][0]
+    delay_var: bm.LengthDelay = self.get_delay_var(f'delay_{id(self.delay_var)}')[0]
     if self.delay_steps is None:
       diffusive = (jnp.expand_dims(self.coupling_var1.value, axis=axis) -
                    jnp.expand_dims(self.coupling_var2.value, axis=axis - 1))
       diffusive = (self.conn_mat * diffusive).sum(axis=axis - 1)
     elif self.delay_type == 'array':
       if isinstance(self.mode, bm.TrainingMode):
         indices = (slice(None, None, None), jnp.arange(self.coupling_var1.size),)
@@ -259,31 +256,31 @@
     if not isinstance(coupling_var, bm.Variable):
       raise ValueError(f'"coupling_var" must be an instance of brainpy.math.Variable. '
                        f'But we got {type(coupling_var)}')
     if jnp.ndim(coupling_var) != 1:
       raise ValueError(f'Only support 1d vector of coupling variable. '
                        f'But we got {jnp.ndim(coupling_var)}')
 
-    super(AdditiveCoupling, self).__init__(
+    super().__init__(
       delay_var=coupling_var,
       var_to_output=var_to_output,
       conn_mat=conn_mat,
       required_shape=(coupling_var.size, coupling_var.size),
       delay_steps=delay_steps,
       initial_delay_data=initial_delay_data,
       name=name,
       mode=mode,
     )
 
     self.coupling_var = coupling_var
 
-  def update(self, tdi):
+  def update(self):
     # delay function
     axis = self.coupling_var.ndim
-    delay_var: bm.LengthDelay = self.global_delay_data[f'delay_{id(self.delay_var)}'][0]
+    delay_var: bm.LengthDelay = self.get_delay_var(f'delay_{id(self.delay_var)}')[0]
     if self.delay_steps is None:
       additive = self.coupling_var @ self.conn_mat
     elif self.delay_type == 'array':
       if isinstance(self.mode, bm.TrainingMode):
         indices = (slice(None, None, None), jnp.arange(self.coupling_var.size),)
       else:
         indices = (jnp.arange(self.coupling_var.size),)
```

## Comparing `brainpy/_src/synapses/gap_junction.py` & `brainpy/_src/dynold/synapses/gap_junction.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 # -*- coding: utf-8 -*-
 
 from typing import Union, Dict, Callable
 
 import brainpy.math as bm
+from brainpy._src.dyn.base import NeuDyn
 from brainpy._src.connect import TwoEndConnector
-from brainpy._src.dynsys import NeuGroup, TwoEndConn
+from brainpy._src.dynold.synapses import TwoEndConn
 from brainpy._src.initialize import Initializer, parameter
 from brainpy.types import ArrayType
 
 __all__ = [
   'GapJunction',
 ]
 
 
 class GapJunction(TwoEndConn):
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
       comp_method: str = 'dense',
       g_max: Union[float, ArrayType, Initializer, Callable] = 1.,
       name: str = None,
   ):
     super(GapJunction, self).__init__(pre=pre,
                                       post=post,
@@ -41,15 +42,15 @@
       self.weights = parameter(g_max, (pre.num, post.num), allow_none=False)
     elif comp_method == 'sparse':
       self.pre_ids, self.post_ids = self.conn.require('pre_ids', 'post_ids')
       self.weights = parameter(g_max, self.pre_ids.shape, allow_none=False)
     else:
       raise ValueError
 
-  def update(self, tdi):
+  def update(self):
     if self.comp_method == 'dense':
       # pre -> post
       diff = (self.pre.V.reshape((-1, 1)) - self.post.V) * self.conn_mat * self.weights
       self.post.input += bm.einsum('ij->j', diff)
       # post -> pre
       self.pre.input += bm.einsum('ij->i', -diff)
     else:
```

## Comparing `brainpy/_src/synapses/learning_rules.py` & `brainpy/_src/dynold/synapses/learning_rules.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,26 +1,34 @@
 # -*- coding: utf-8 -*-
 
-from typing import Union, Dict, Callable
+from typing import Union, Dict, Callable, Optional
 
-import jax.numpy as jnp
-
-import brainpy.math as bm
-from brainpy._src.dynsys import NeuGroup, TwoEndConn
-from brainpy._src.initialize import Initializer, delay as init_delay
-from brainpy._src.integrators import odeint, JointEq
 from brainpy._src.connect import TwoEndConnector
+from brainpy._src.dyn import synapses
+from brainpy._src.dynold.synouts import CUBA
+from brainpy._src.dynold.synapses import _TwoEndConnAlignPre
+from brainpy._src.dynsys import Sequential
+from brainpy._src.dyn.base import NeuDyn
+from brainpy._src.initialize import Initializer
+from brainpy._src.mixin import ParamDesc
 from brainpy.types import ArrayType
 
 __all__ = [
   'STP'
 ]
 
 
-class STP(TwoEndConn):
+class _STPModel(Sequential, ParamDesc):
+  def __init__(self, size, keep_size, tau, U, tau_f, tau_d, mode=None, method='exp_euler'):
+    stp = synapses.STP(size, keep_size, U=U, tau_f=tau_f, tau_d=tau_d, method=method, mode=mode)
+    exp = synapses.Expon(size, keep_size, tau=tau, method=method, mode=mode)
+    super().__init__(stp, exp)
+
+
+class STP(_TwoEndConnAlignPre):
   r"""Short-term plasticity model.
 
   **Model Descriptions**
 
   Short-term plasticity (STP) [1]_ [2]_ [3]_, also called dynamical synapses,
   refers to the changes of synaptic strengths over time in a way that reflects
   the history of presynaptic activity. Two types of STP, with opposite effects
@@ -172,73 +180,48 @@
   .. [5] Tsodyks, Misha, and Si Wu. "Short-term synaptic plasticity."
          Scholarpedia 8, no. 10 (2013): 3153.
 
   """
 
   def __init__(
       self,
-      pre: NeuGroup,
-      post: NeuGroup,
+      pre: NeuDyn,
+      post: NeuDyn,
       conn: Union[TwoEndConnector, ArrayType, Dict[str, ArrayType]],
       U: Union[float, ArrayType] = 0.15,
       tau_f: Union[float, ArrayType] = 1500.,
       tau_d: Union[float, ArrayType] = 200.,
       tau: Union[float, ArrayType] = 8.,
       A: Union[float, ArrayType] = 1.,
       delay_step: Union[int, ArrayType, Initializer, Callable] = None,
       method: str = 'exp_auto',
-      name: str = None
+      name: Optional[str] = None
   ):
-    super(STP, self).__init__(pre=pre, post=post, conn=conn, name=name)
-    self.check_post_attrs('input')
-
     # parameters
     self.tau_d = tau_d
     self.tau_f = tau_f
     self.tau = tau
     self.U = U
     self.A = A
 
-    # connections
-    self.pre_ids, self.post_ids = self.conn.require('pre_ids', 'post_ids')
+    syn = _STPModel(pre.size,
+                    pre.keep_size,
+                    tau,
+                    U,
+                    tau_f,
+                    tau_d,
+                    method=method)
+
+    super().__init__(pre=pre,
+                     post=post,
+                     syn=syn,
+                     conn=conn,
+                     g_max=A,
+                     output=CUBA(),
+                     comp_method='sparse',
+                     delay_step=delay_step,
+                     name=name)
 
     # variables
-    self.num = len(self.pre_ids)
-    self.x = bm.Variable(jnp.ones(self.num))
-    self.u = bm.Variable(jnp.zeros(self.num))
-    self.I = bm.Variable(jnp.zeros(self.num))
-    self.delay_type, self.delay_step, self.delay_I = init_delay(delay_step, self.I)
-
-    # integral
-    self.integral = odeint(method=method, f=self.derivative)
-
-  def reset(self):
-    self.x.value = jnp.zeros(self.num)
-    self.u.value = jnp.zeros(self.num)
-    self.I.value = jnp.zeros(self.num)
-    self.delay_I.reset(self.I)
-
-  @property
-  def derivative(self):
-    dI = lambda I, t: -I / self.tau
-    du = lambda u, t: - u / self.tau_f
-    dx = lambda x, t: (1 - x) / self.tau_d
-    return JointEq([dI, du, dx])
-
-  def update(self, tdi):
-    # delayed pre-synaptic spikes
-    if self.delay_type == 'homo':
-      delayed_I = self.delay_I(self.delay_step)
-    elif self.delay_type == 'heter':
-      delayed_I = self.delay_I(self.delay_step, jnp.arange(self.pre.num))
-    else:
-      delayed_I = self.I
-    self.post.input += bm.syn2post(delayed_I, self.post_ids, self.post.num)
-    self.I.value, u, x = self.integral(self.I, self.u, self.x, tdi.t, tdi.dt)
-    syn_sps = bm.pre2syn(self.pre.spike, self.pre_ids)
-    u = jnp.where(syn_sps, u + self.U * (1 - self.u), u)
-    x = jnp.where(syn_sps, x - u * self.x, x)
-    self.I.value = jnp.where(syn_sps, self.I + self.A * u * self.x, self.I.value)
-    self.u.value = u
-    self.x.value = x
-    if self.delay_type in ['homo', 'heter']:
-      self.delay_I.update(self.I)
+    self.x = self.syn[0].x
+    self.u = self.syn[0].u
+    self.I = self.syn[1].g
```

## Comparing `brainpy/_src/synapses_v2/abstract_synapses.py` & `brainpy/_src/dynold/experimental/abstract_synapses.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from typing import Union, Dict, Callable, Optional
 
 from jax import vmap
 
 import brainpy.math as bm
 from brainpy._src.connect import TwoEndConnector, All2All, One2One
 from brainpy._src.context import share
-from brainpy._src.synapses_v2.base import SynConnNS, SynOutNS, SynSTPNS
+from brainpy._src.dynold.experimental.base import SynConnNS, SynOutNS, SynSTPNS
 from brainpy._src.initialize import Initializer, variable_
 from brainpy._src.integrators import odeint, JointEq
 from brainpy.check import is_float
 from brainpy.types import ArrayType
 
 
 class Exponential(SynConnNS):
```

## Comparing `brainpy/_src/synapses_v2/base.py` & `brainpy/_src/dynold/experimental/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 from typing import Union, Callable, Optional, Tuple
 
 import jax
 import numpy as np
 
 import brainpy.math as bm
 from brainpy._src.connect import TwoEndConnector, All2All, One2One, MatConn, IJConn
-from brainpy._src.dynsys import DynamicalSystemNS
+from brainpy._src.dynsys import DynamicalSystem
 from brainpy._src.initialize import Initializer, parameter
 from brainpy.types import ArrayType
 
 
-class SynConnNS(DynamicalSystemNS):
+class SynConnNS(DynamicalSystem):
   def __init__(
       self,
       conn: TwoEndConnector,
       out: Optional['SynOutNS'] = None,
       stp: Optional['SynSTPNS'] = None,
       name: str = None,
       mode: bm.Mode = None,
@@ -114,20 +114,20 @@
     if bm.ndim(syn_weight) == 0:
       post_vs = (syn_weight * syn_value) @ conn_mat
     else:
       post_vs = syn_value @ (syn_weight * conn_mat)
     return post_vs
 
 
-class SynOutNS(DynamicalSystemNS):
+class SynOutNS(DynamicalSystem):
   def update(self, post_g, post_v):
     raise NotImplementedError
 
   def reset_state(self, batch_size: Optional[int] = None):
     pass
 
 
-class SynSTPNS(DynamicalSystemNS):
+class SynSTPNS(DynamicalSystem):
   """Base class for synaptic short-term plasticity."""
 
   def update(self, pre_spike):
     raise NotImplementedError
```

## Comparing `brainpy/_src/synapses_v2/others.py` & `brainpy/_src/dynold/experimental/others.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 
 from typing import Union, Optional
 
 import brainpy.math as bm
-from brainpy._src.dynsys import DynamicalSystemNS
+from brainpy._src.dynsys import DynamicalSystem
 from brainpy._src.context import share
 from brainpy.check import is_float, is_integer
 
 
-class PoissonInput(DynamicalSystemNS):
+class PoissonInput(DynamicalSystem):
   """Poisson Input.
 
   Adds independent Poisson input to a target variable. For large
   numbers of inputs, this is much more efficient than creating a
   `PoissonGroup`. The synaptic events are generated randomly during the
   simulation and are not preloaded and stored in memory. All the inputs must
   target the same variable, have the same frequency and same synaptic weight.
```

## Comparing `brainpy/_src/synapses_v2/syn_outs.py` & `brainpy/_src/dynold/experimental/syn_outs.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # -*- coding: utf-8 -*-
 
 from typing import Union
 
-from brainpy._src.synapses_v2.base import SynOutNS
+from brainpy._src.dynold.experimental.base import SynOutNS
 from brainpy.math import exp
 from brainpy.types import ArrayType
 
 __all__ = [
   'COBA',
   'CUBA',
   'MgBlock',
```

## Comparing `brainpy/_src/synapses_v2/syn_plasticity.py` & `brainpy/_src/dynold/experimental/syn_plasticity.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 # -*- coding: utf-8 -*-
 
 from typing import Union
 
 import jax.numpy as jnp
 
-from brainpy._src.context import share
 from brainpy import math as bm, tools
-from brainpy._src.synapses_v2.base import SynSTPNS
+from brainpy._src.context import share
+from brainpy._src.dynold.experimental.base import SynSTPNS
 from brainpy._src.initialize import variable_, OneInit, parameter
 from brainpy._src.integrators import odeint, JointEq
 from brainpy.types import ArrayType, Shape
 
 __all__ = [
   'STD',
   'STP',
```

## Comparing `brainpy/_src/synouts/conductances.py` & `brainpy/_src/dynold/synouts/conductances.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,23 @@
 # -*- coding: utf-8 -*-
 
 from typing import Union, Callable, Optional
 
-from brainpy.math import Variable
-from brainpy._src.dynsys import SynOut
+from brainpy._src.dynold.synapses.base import _SynOut
 from brainpy._src.initialize import parameter, Initializer
+from brainpy.math import Variable
 from brainpy.types import ArrayType
 
-
 __all__ = [
   'COBA',
   'CUBA',
 ]
 
 
-class CUBA(SynOut):
+class CUBA(_SynOut):
   r"""Current-based synaptic output.
 
   Given the conductance, this model outputs the post-synaptic current with a identity function:
 
   .. math::
 
      I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t)
@@ -36,21 +35,21 @@
 
   def __init__(
       self,
       target_var: Optional[Union[str, Variable]] = 'input',
       name: str = None,
   ):
     self._target_var = target_var
-    super(CUBA, self).__init__(name=name, target_var=target_var)
+    super().__init__(name=name, target_var=target_var)
 
   def clone(self):
     return CUBA(target_var=self._target_var)
 
 
-class COBA(SynOut):
+class COBA(_SynOut):
   r"""Conductance-based synaptic output.
 
   Given the synaptic conductance, the model output the post-synaptic current with
 
   .. math::
 
      I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t))
@@ -70,26 +69,26 @@
   def __init__(
       self,
       E: Union[float, ArrayType, Callable, Initializer] = 0.,
       target_var: Optional[Union[str, Variable]] = 'input',
       membrane_var: Union[str, Variable] = 'V',
       name: str = None,
   ):
-    super(COBA, self).__init__(name=name, target_var=target_var)
+    super().__init__(name=name, target_var=target_var)
     self._E = E
     self._target_var = target_var
     self._membrane_var = membrane_var
 
   def clone(self):
     return COBA(E=self._E,
                 target_var=self._target_var,
                 membrane_var=self._membrane_var)
 
   def register_master(self, master):
-    super(COBA, self).register_master(master)
+    super().register_master(master)
 
     # reversal potential
     self.E = parameter(self._E, self.master.post.num, allow_none=False)
 
     # membrane potential
     if isinstance(self._membrane_var, str):
       if not hasattr(self.master.post, self._membrane_var):
```

## Comparing `brainpy/_src/synouts/ions.py` & `brainpy/_src/dynold/synouts/ions.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,25 +1,24 @@
 # -*- coding: utf-8 -*-
 
 from typing import Union, Callable, Optional
 
 import jax.numpy as jnp
 
 import brainpy.math as bm
-from brainpy._src.dynsys import SynOut
+from brainpy._src.dynold.synapses.base import _SynOut
 from brainpy._src.initialize import parameter, Initializer
 from brainpy.types import ArrayType
 
-
 __all__ = [
   'MgBlock',
 ]
 
 
-class MgBlock(SynOut):
+class MgBlock(_SynOut):
   r"""Synaptic output based on Magnesium blocking.
 
   Given the synaptic conductance, the model output the post-synaptic current with
 
   .. math::
 
      I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t)) g_{\infty}(V,[{Mg}^{2+}]_{o})
@@ -52,24 +51,24 @@
       cc_Mg: Union[float, ArrayType, Callable, Initializer] = 1.2,
       alpha: Union[float, ArrayType, Callable, Initializer] = 0.062,
       beta: Union[float, ArrayType, Callable, Initializer] = 3.57,
       target_var: Optional[Union[str, bm.Variable]] = 'input',
       membrane_var: Union[str, bm.Variable] = 'V',
       name: str = None,
   ):
-    super(MgBlock, self).__init__(name=name, target_var=target_var)
+    super().__init__(name=name, target_var=target_var)
     self._E = E
     self._cc_Mg = cc_Mg
     self._alpha = alpha
     self._beta = beta
     self._target_var = target_var
     self._membrane_var = membrane_var
 
   def register_master(self, master):
-    super(MgBlock, self).register_master(master)
+    super().register_master(master)
 
     self.E = parameter(self._E, self.master.post.num, allow_none=False)
     self.cc_Mg = parameter(self._cc_Mg, self.master.post.num, allow_none=False)
     self.alpha = parameter(self._alpha, self.master.post.num, allow_none=False)
     self.beta = parameter(self._beta, self.master.post.num, allow_none=False)
     if isinstance(self._membrane_var, str):
       if not hasattr(self.master.post, self._membrane_var):
```

## Comparing `brainpy/_src/synplast/short_term_plasticity.py` & `brainpy/_src/dynold/synplast/short_term_plasticity.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 # -*- coding: utf-8 -*-
 
 from typing import Union
 
 import jax.numpy as jnp
 
-from brainpy._src.dynsys import SynSTP
+from brainpy._src.context import share
+from brainpy._src.dynold.synapses.base import _SynSTP
 from brainpy._src.initialize import variable
 from brainpy._src.integrators import odeint, JointEq
 from brainpy.check import is_float
 from brainpy.types import ArrayType
 
 __all__ = [
   'STD',
   'STP',
 ]
 
 
-class STD(SynSTP):
+class STD(_SynSTP):
   r"""Synaptic output with short-term depression.
 
   This model filters the synaptic current by the following equation:
 
   .. math::
 
      I_{syn}^+(t) = I_{syn}^-(t) * x
@@ -53,46 +54,50 @@
   def __init__(
       self,
       tau: float = 200.,
       U: float = 0.07,
       method: str = 'exp_auto',
       name: str = None
   ):
-    super(STD, self).__init__(name=name)
+    super().__init__(name=name)
 
     # parameters
     is_float(tau, 'tau', min_bound=0, )
     is_float(U, 'U', min_bound=0, )
     self.tau = tau
     self.U = U
     self.method = method
 
     # integral function
     self.integral = odeint(lambda x, t: (1 - x) / self.tau, method=self.method)
 
-  def register_master(self, master):
-    super(STD, self).register_master(master)
+  def clone(self):
+    return STD(tau=self.tau, U=self.U, method=self.method)
 
-    # variables
+  def register_master(self, master):
+    super().register_master(master)
     self.x = variable(jnp.ones, self.master.mode, self.master.pre.num)
 
   def reset_state(self, batch_size=None):
     self.x.value = variable(jnp.ones, batch_size, self.master.pre.num)
 
-  def update(self, tdi, pre_spike):
-    x = self.integral(self.x.value, tdi['t'], tdi['dt'])
+  def update(self, pre_spike):
+    x = self.integral(self.x.value, share['t'], share['dt'])
     self.x.value = jnp.where(pre_spike, x - self.U * self.x, x)
 
   def filter(self, g):
     if jnp.shape(g) != self.x.shape:
       raise ValueError('Shape does not match.')
     return g * self.x
 
+  def __repr__(self):
+    return f'{self.__class__.__name__}(tau={self.tau}, U={self.U}, method={self.method})'
+
 
-class STP(SynSTP):
+class STP(_SynSTP):
   r"""Synaptic output with short-term plasticity.
 
   This model filters the synaptic currents according to two variables: :math:`u` and :math:`x`.
 
   .. math::
 
      I_{syn}^+(t) = I_{syn}^-(t) * x * u
@@ -149,36 +154,40 @@
     self.tau_d = tau_d
     self.U = U
     self.method = method
 
     # integral function
     self.integral = odeint(self.derivative, method=self.method)
 
-  def register_master(self, master):
-    super(STP, self).register_master(master)
+  def clone(self):
+    return STP(tau_f=self.tau_f, tau_d=self.tau_d, U=self.U, method=self.method)
 
-    # variables
+  def register_master(self, master):
+    super().register_master(master)
     self.x = variable(jnp.ones, self.master.mode, self.master.pre.num)
     self.u = variable(lambda s: jnp.ones(s) * self.U, self.master.mode, self.master.pre.num)
 
   def reset_state(self, batch_size=None):
     self.x.value = variable(jnp.ones, batch_size, self.master.pre.num)
     self.u.value = variable(lambda s: jnp.ones(s) * self.U, batch_size, self.master.pre.num)
 
   @property
   def derivative(self):
     du = lambda u, t: self.U - u / self.tau_f
     dx = lambda x, t: (1 - x) / self.tau_d
-    return JointEq([du, dx])
+    return JointEq(du, dx)
 
-  def update(self, tdi, pre_spike):
-    u, x = self.integral(self.u.value, self.x.value, tdi['t'], tdi['dt'])
+  def update(self, pre_spike):
+    u, x = self.integral(self.u.value, self.x.value, share['t'], share['dt'])
     u = jnp.where(pre_spike, u + self.U * (1 - self.u), u)
     x = jnp.where(pre_spike, x - u * self.x, x)
     self.x.value = x
     self.u.value = u
 
   def filter(self, g):
     if jnp.shape(g) != self.x.shape:
       raise ValueError('Shape does not match.')
     return g * self.x * self.u
 
+  def __repr__(self):
+    return f'{self.__class__.__name__}(tau_f={self.tau_f}, tau_d={self.tau_d}, U={self.U}, method={self.method})'
+
```

## Comparing `brainpy-2.4.2.dist-info/LICENSE` & `brainpy-2.4.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `brainpy-2.4.2.dist-info/METADATA` & `brainpy-2.4.3.dist-info/METADATA`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: brainpy
-Version: 2.4.2
+Version: 2.4.3
 Summary: BrainPy: Brain Dynamics Programming in Python
 Home-page: https://github.com/brainpy/BrainPy
 Author: BrainPy Team
 Author-email: chao.brain@qq.com
 License: GPL-3.0 license
 Project-URL: Bug Tracker, https://github.com/brainpy/BrainPy/issues
 Project-URL: Documentation, https://brainpy.readthedocs.io/
@@ -40,20 +40,19 @@
 
 
 <p align="center">
 	<a href="https://pypi.org/project/brainpy/"><img alt="Supported Python Version" src="https://img.shields.io/pypi/pyversions/brainpy"></a>
 	<a href="https://github.com/brainpy/BrainPy"><img alt="LICENSE" src="https://anaconda.org/brainpy/brainpy/badges/license.svg"></a>
   	<a href="https://brainpy.readthedocs.io/en/latest/?badge=latest"><img alt="Documentation" src="https://readthedocs.org/projects/brainpy/badge/?version=latest"></a>
   	<a href="https://badge.fury.io/py/brainpy"><img alt="PyPI version" src="https://badge.fury.io/py/brainpy.svg"></a>
-    <a href="https://github.com/brainpy/BrainPy"><img alt="Continuous Integration" src="https://github.com/brainpy/BrainPy/actions/workflows/CI.yml/badge.svg"></a>
+    <a href="https://github.com/brainpy/BrainPy/actions/workflows/CI.yml"><img alt="Continuous Integration" src="https://github.com/brainpy/BrainPy/actions/workflows/CI.yml/badge.svg"></a>
+    <a href="https://github.com/brainpy/BrainPy/actions/workflows/CI-models.yml"><img alt="Continuous Integration with Models" src="https://github.com/brainpy/BrainPy/actions/workflows/CI-models.yml/badge.svg"></a>
 </p>
 
 
-
-
 BrainPy is a flexible, efficient, and extensible framework for computational neuroscience and brain-inspired computation based on the Just-In-Time (JIT) compilation (built on top of [JAX](https://github.com/google/jax), [Numba](https://github.com/numba/numba), and other JIT compilers). It provides an integrative ecosystem for brain dynamics programming, including brain dynamics **building**, **simulation**, **training**, **analysis**, etc. 
 
 - **Website (documentation and APIs)**: https://brainpy.readthedocs.io/en/latest
 - **Source**: https://github.com/brainpy/BrainPy
 - **Bug reports**: https://github.com/brainpy/BrainPy/issues
 - **Source on OpenI**: https://git.openi.org.cn/OpenI/BrainPy
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: brainpy Version: 2.4.2 Summary: BrainPy: Brain
+Metadata-Version: 2.1 Name: brainpy Version: 2.4.3 Summary: BrainPy: Brain
 Dynamics Programming in Python Home-page: https://github.com/brainpy/BrainPy
 Author: BrainPy Team Author-email: chao.brain@qq.com License: GPL-3.0 license
 Project-URL: Bug Tracker, https://github.com/brainpy/BrainPy/issues Project-
 URL: Documentation, https://brainpy.readthedocs.io/ Project-URL: Source Code,
 https://github.com/brainpy/BrainPy Keywords: computational neuroscience,brain-
 inspired computation,dynamical systems,differential equations,brain
 modeling,brain dynamics modeling,brain dynamics programming Platform: UNKNOWN
@@ -17,15 +17,15 @@
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Software Development :: Libraries Requires-Python: >=3.8
 Description-Content-Type: text/markdown License-File: LICENSE Requires-Dist:
 jax (>=0.4.1) Requires-Dist: msgpack Requires-Dist: numpy (>=1.15) Requires-
 Dist: tqdm
        [Header image of BrainPy - brain dynamics programming in Python.]
 [Supported_Python_Version] [LICENSE] [Documentation] [PyPI_version] [Continuous
-                                 Integration]
+               Integration] [Continuous_Integration_with_Models]
 BrainPy is a flexible, efficient, and extensible framework for computational
 neuroscience and brain-inspired computation based on the Just-In-Time (JIT)
 compilation (built on top of [JAX](https://github.com/google/jax), [Numba]
 (https://github.com/numba/numba), and other JIT compilers). It provides an
 integrative ecosystem for brain dynamics programming, including brain dynamics
 **building**, **simulation**, **training**, **analysis**, etc. - **Website
 (documentation and APIs)**: https://brainpy.readthedocs.io/en/latest -
```

## Comparing `brainpy-2.4.2.dist-info/RECORD` & `brainpy-2.4.3.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,108 +1,153 @@
-brainpy/__init__.py,sha256=tsETtMpJicaRLBe361OUWPU1aRBSA9NatfulLqZH_l4,10155
+brainpy/__init__.py,sha256=E7-xTWs2U1d3Pza4mjuaroBk4tnyrQyuHCfoZWnVBQY,4574
+brainpy/_add_deprecations.py,sha256=5Q1SIzoMCQAN25mVyLpdfJOZud5KUsuXZfa_FTQP6GY,5966
 brainpy/analysis.py,sha256=1F9j_gVbbZeE4vbtnWm3aZQnsiRFQH5jawh8idpUI54,661
-brainpy/channels.py,sha256=_IghoZjZTtLWzb8eT6iCBY2zjZOLuRuo8FuQ51DAJgs,1308
-brainpy/check.py,sha256=MEGRzRe8yuFuQ3bLukAnUK1jkai0iqgxQpuWhPrdZ-U,19357
+brainpy/channels.py,sha256=shXqmuOESm9KlQj1wPJjyJCyFxOgoks_5kMzyBoGh7s,77
+brainpy/check.py,sha256=LnX2-_iYbgSP0PNoL8HcTo5qrAn2bdDtKguztQYcLqE,18984
 brainpy/checkpoints.py,sha256=HVQLY0VkJ8K0dkOeVSmrZr6gmfTz7turFNRkkwnfwac,407
 brainpy/connect.py,sha256=attHF6pF-uf63E95mp2IyixIXDdy1mufGEbtUiFuGIs,1275
-brainpy/dnn.py,sha256=BuSWGifJqHB3YJBjdpQZS_358GqNdOLaGMorgEvT6LQ,2690
 brainpy/encoding.py,sha256=UAr-ouj1z3kktUUZUL79YL1TxkCI6cL0LhE4DiEAW2Q,324
-brainpy/errors.py,sha256=te_PNK5GduoEDpX0QLnqaMNFMU0RDG-kxkfVEwWAUVY,7520
-brainpy/experimental.py,sha256=Q8rOmhQecwVNKdo2CKaIb0GvRcsB3zat3-qX7NOUAxU,334
+brainpy/errors.py,sha256=JHGQfvnX-52RyQkymHfMkvmJfSXlXfR2m38vQdpJJjY,7567
+brainpy/experimental.py,sha256=o4LYolZMOymLhSx-3xpz68PkEK2HjxvtmbnL5dnfqYM,366
 brainpy/initialize.py,sha256=Vh-Oao7TunLnXLNLlhTJlAfVKWQ-2tLMNThBCj_g68E,1123
 brainpy/inputs.py,sha256=EieFIsSPhoYdC8w8RSFgDPHO8gpphZZqS1z3JOP58Q0,335
 brainpy/layers.py,sha256=UZhDVzO0tSqGfHFNHZ9ntLI0qUeH8HX2WQ24aj03R9U,20
 brainpy/losses.py,sha256=buIh5f-o0QI1_H-MYaayuEOcW1cTrQno5QINY8yfX7I,1086
 brainpy/measure.py,sha256=bAKe5XqcfP50j2H96ZcTIJmVvNHwBGxGQppoEsQK_gI,487
-brainpy/mixin.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-brainpy/neurons.py,sha256=_XzpTdSdBz7lYaHNJp9r0wdYbyAHW7QERZCeUWmeTVc,1013
+brainpy/mixin.py,sha256=l1G6NDUt9VNdz5TFrCvrITKkXlzOgWKSWSFjjSxJCyM,266
+brainpy/neurons.py,sha256=4h7aj6vSRoTkL1oAGcuYU7AT34HDrplcmoLNlKWMySQ,961
 brainpy/optim.py,sha256=ShG_NXD334Ns_wqh0L7e-sINzTEoPW7hLI1B-j8Clhs,1013
-brainpy/rates.py,sha256=lfddb1qSEoacvLH9MPosTE6rzuJMo_1c5x46tLzpkmw,299
+brainpy/rates.py,sha256=0NBZrz3uIoe_BoNgHsOEWHJ8ZkmXuvqQQfQzfm14ovY,52
 brainpy/running.py,sha256=ZGjJH40qBJjNzo9WWuyd9CrMf1vNFd27cN9qwPoDnnw,466
-brainpy/testing.py,sha256=4unb3IJCji-0CO5PaZsP_9727UnKlbOEoJmTthCPFK8,51
+brainpy/synapses.py,sha256=HqhenViUoMDfATrEQMqTh6xO1pMcwPWXhmeSFqt_uP4,931
+brainpy/synouts.py,sha256=6Kci627gs10_VD5gVZoi9xSt-SwoFeAckR_1MXfCx8w,186
+brainpy/synplast.py,sha256=QDXE-9ZrD4On2eark1opj0ohBeUkUHpYVXL0I30G3Ro,120
 brainpy/tools.py,sha256=E-8tyzS5mxZ4GLxSkwREsj8D_FHAgrzddViiVAWsJnE,1046
 brainpy/types.py,sha256=ivDEcU3XoFeMVrCsHH5f_NmvtGqh59lMpi2kRbjmt98,265
 brainpy/_src/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-brainpy/_src/_delay.py,sha256=vo1GQWsI6M-T5ti6KpTzQAqjoKsQZ7RwgAKwHnVj1QU,9945
+brainpy/_src/_delay.py,sha256=xrcs_kO4_sUSApDRCF8LNH8MI-uS3TgaISBmBSAO_P8,9941
 brainpy/_src/checking.py,sha256=idG-wjzyB8kW9NJh1qqtQmSFTwfePVC28juRW__QioA,2457
-brainpy/_src/context.py,sha256=aCzixPHjFGgbkBuaMsUYfoF7OEVjw2PAekIrFeukv8Y,2631
-brainpy/_src/delay.py,sha256=xC5UM65wP2kwPAnstywQ8kWtgci0vNVXds-wI1WbS9A,22860
-brainpy/_src/deprecations.py,sha256=X79H4rN7yQxtdRMXFN7DYqOdBVrqny-Dfs3sxXW8QLE,1500
-brainpy/_src/dynsys.py,sha256=rh3xEotti_r0udflOyVnCGNnJbNqlBGfGA4isL9crRk,49179
-brainpy/_src/mixin.py,sha256=CUttHHGMbW5H1qD4y7IFTrfgmrkmE29JOTlbj72lodI,2328
+brainpy/_src/context.py,sha256=Jhnbnl5XtE9QLEZm4ikoTHwuydhq5UaWXEnL8qfmsuk,2609
+brainpy/_src/delay.py,sha256=2WhLSwj1coafH6jHO4GxqwKKYznezHrCQ4ONSBTIiAA,22973
+brainpy/_src/deprecations.py,sha256=Pa8WKz4aqZkPbg-cfjCquAgc6gDeDsDShQEolmZTWGw,2031
+brainpy/_src/dynsys.py,sha256=iX_BucKxxrwt9MSiZb8ex4hQq5492CpAp619_Xuy-RU,25214
+brainpy/_src/mixin.py,sha256=wxhP04_OpbevE5HLHVxj03ontD9lbs62uIcCa0dNmjY,19533
 brainpy/_src/modes.py,sha256=9UNgQyOgp0h8Ky7z394ZyF0XM6xQXeMqiD4erEe4Svg,1101
-brainpy/_src/runners.py,sha256=lvm5sNACiQWlwWRf0eIMPAlDWovc1fMjSWd2whnOaOI,24973
-brainpy/_src/transform.py,sha256=61dEagNWUuvcL18JVd2JslBaWbvKuRX78RIW4d8rjE4,10267
+brainpy/_src/runners.py,sha256=WPqTyE2KLhmLH8uw38bDFW9Yc8Gx44AmTFa25l6b40Q,23348
+brainpy/_src/transform.py,sha256=dQvllEjn9K5t0uARitf4OEvbtEVVOhsyMV5HCM2yyPo,10247
 brainpy/_src/types.py,sha256=39V_8jqDeFq_gEGFtz3eKh7VdzGDiKnE1lmpiu8HWLc,1065
 brainpy/_src/analysis/__init__.py,sha256=LvgnPFbrO99KqhQncgovd8qRSagRYjudppkyW3jjydk,846
 brainpy/_src/analysis/base.py,sha256=Elojhf3EVObjW0BHhXSivgrKO86lgYPDg-strF-Xefg,156
 brainpy/_src/analysis/constants.py,sha256=vh_jBcUaqVgsJWe8GxyJKMNOKa30F5ijPjj5BMjs2z0,1721
 brainpy/_src/analysis/plotstyle.py,sha256=3QPU2ZIHgEQtrXlr4-pKLy2NS4KXr8CiH9TLBGXblmY,3924
 brainpy/_src/analysis/stability.py,sha256=h6dY4j-zNI0r2nil5yHyVqkHErZcL6BYEtPUJ6s1yj0,5651
 brainpy/_src/analysis/highdim/__init__.py,sha256=VEjW7stN-EyTUIfh2bwZzMKip9N6TOrLtBtp7K5serA,52
-brainpy/_src/analysis/highdim/slow_points.py,sha256=Y-GfBaIdr-xdrTRacuEb_euMEp9Smt2NdCBTCBM3Tp4,30947
+brainpy/_src/analysis/highdim/slow_points.py,sha256=NJfkXnEWomI4mR6SlKYdLaduM-cgbiomQ25cwOVOoEk,31167
 brainpy/_src/analysis/lowdim/__init__.py,sha256=LmcgMjhR2WDLRlcGHiiVpYAKEDlHs-NF-XOT7B9Vi9M,93
-brainpy/_src/analysis/lowdim/lowdim_analyzer.py,sha256=D59pQNA_FwJ4D0eV-zJdQPIvhciw3TiYuJzAUMlUJK8,44743
-brainpy/_src/analysis/lowdim/lowdim_bifurcation.py,sha256=_YwjI6mq1I4d1RUKDiAGvBKLur_7VadSE6UMfdvlSdw,25000
-brainpy/_src/analysis/lowdim/lowdim_phase_plane.py,sha256=SUXjCKl-MyK1ugLDvprB-0dNLFWf596WfmN0NK8hZCw,20273
+brainpy/_src/analysis/lowdim/lowdim_analyzer.py,sha256=xGwL9gg8oBv7beMkY8Y_oZmHD6uoSWHj5Kq38vXWI1E,44554
+brainpy/_src/analysis/lowdim/lowdim_bifurcation.py,sha256=lPBiYymSuASLJ0_GWbSApOz_aQRmrUcuuhoDZ2V_0IA,24510
+brainpy/_src/analysis/lowdim/lowdim_phase_plane.py,sha256=5xISVEQTF-AWio47Dvl7Kt2vPLqEApBj5hdgb5EISSI,20019
 brainpy/_src/analysis/utils/__init__.py,sha256=hsynCeksFHyhL0AMX3-IvPhcnVD0jjXAFO3G7K1UckM,199
 brainpy/_src/analysis/utils/function.py,sha256=bjVGfcoygD8HIrDLLpCcL1ii2i20rv9a-G6jVRodlJM,2840
 brainpy/_src/analysis/utils/measurement.py,sha256=w_CMQq7y0CDq_xzaxsC7yEIFKVSrNLF6CA1cQedUlc4,3054
-brainpy/_src/analysis/utils/model.py,sha256=kBjkcCtLXFapYj8meQWMeI_TFK-47t0G1zxvF3yiP30,5298
+brainpy/_src/analysis/utils/model.py,sha256=sen8FxoS61t3hxWzEpxg5yTbZj71q8JVGHSkMcFfDB0,5195
 brainpy/_src/analysis/utils/optimization.py,sha256=FuDKkx3pmoXZpoGRd22nsb0XoQBkHvaCq7xJRwFNDHw,19554
 brainpy/_src/analysis/utils/others.py,sha256=f64tjVHTrxnFQi6uN8RL4-sCuCzHpjn1GnvD3YbbEws,5822
 brainpy/_src/analysis/utils/outputs.py,sha256=7IsgRNIouanEpTUWRf9YnN-NSd3TzfT6vuXr_mE_0Vw,158
 brainpy/_src/analysis/utils/visualization.py,sha256=ZBUO4Lv_LpNzAVot8Jp7vyBQrA7U5wRCUV5cLdPHwQY,967
 brainpy/_src/base/__init__.py,sha256=iAq3ty-mtEtr0_3B14Y4CDYJSMDTce5enUDFIt7Hx3Q,169
 brainpy/_src/base/collector.py,sha256=iwhKnzeBJLKxpRVjvzwiRE63_zNpIBfaKLITauVph-0,24
 brainpy/_src/base/function.py,sha256=O9CT1B2F-cVB8elT0EoCJbgkcffjvlmqteqavs4giDg,25
 brainpy/_src/base/io.py,sha256=u2EUWzj6zJIdO9F-RrAb4YsRJEjl15Hp0lEYYGq3ssI,1029
 brainpy/_src/base/naming.py,sha256=N_HOta7wY7D5JQirksRw48AB43LjMtNP1E85OR8ULJk,305
 brainpy/_src/checkpoints/__init__.py,sha256=iwhKnzeBJLKxpRVjvzwiRE63_zNpIBfaKLITauVph-0,24
 brainpy/_src/checkpoints/io.py,sha256=gQZbjzKM6o41bYjeAhf7uFTQwNG_HVZ9MTEzGTxR__I,12240
 brainpy/_src/checkpoints/serialization.py,sha256=AxDYWrB6CT1pzOIFLANsxw2T-bgr3ltaFgnyv1qdbMQ,57361
 brainpy/_src/connect/__init__.py,sha256=BQDEgFkQ2WZPnLRUlc5OfDKHDhEgvMYdoBGtIXZUfKI,268
-brainpy/_src/connect/base.py,sha256=erc5Lu2oyoPssUXrAnDumqM31zf8QTYCJI24EvalvhI,24714
-brainpy/_src/connect/custom_conn.py,sha256=a7C9F0nD8I_I5IGu5FId1KCueqJNJk1nAfXXjYa-hmQ,4210
-brainpy/_src/connect/random_conn.py,sha256=MHWm-swGqfWsF1HmXvTznvYo4sdXxMS77pZlII26HhA,40725
+brainpy/_src/connect/base.py,sha256=pXc0Do7suWE6uk-xH6vahsOEcIAcz-ACotD3TJaXzb4,24721
+brainpy/_src/connect/custom_conn.py,sha256=Dp8Rg4nXBtiDhK7orS5L9jP_h6CnUkgFgKQJbTBcjIo,4218
+brainpy/_src/connect/random_conn.py,sha256=67zmavQz8sTpIA4VlKZ-zfJ3r040R7b8tqltRo9Fmng,51213
 brainpy/_src/connect/regular_conn.py,sha256=x1EV_1KwKPTiAyy7geODNmhogRUNvCg_8Pptcp5yqho,9173
 brainpy/_src/dnn/__init__.py,sha256=9DokGkdhI5OnbcT5Js6jmoLRy4vLYVsB7qNiJ04KiIg,295
-brainpy/_src/dnn/activations.py,sha256=RygZ8dg1epkOdbkNyXyr7LFBrvaKDZACrVxVZbW66WQ,32514
-brainpy/_src/dnn/base.py,sha256=wChhfWI61jOh2LqrJZx7l6Bi5jnMSbHLEitaP-bpg6g,197
-brainpy/_src/dnn/conv.py,sha256=eQ7QGCd-3UBHgm60s4AfPuwCx-Zd22SvFsqHRd0jdTM,29730
-brainpy/_src/dnn/dropout.py,sha256=7ldgz5QtOMmII7E1F3QNfSn3n9nm3s4i9qec1idndNs,1302
-brainpy/_src/dnn/function.py,sha256=LYnB_9fq8zB4KylZGN1TxjDkSd_QkBu1JtKD_tDVtGA,1803
-brainpy/_src/dnn/interoperation_flax.py,sha256=AVG8MZFEKSYfbYa5eDnmtcDwwfJ5AHSeZeAaeXOMQpY,4106
-brainpy/_src/dnn/linear.py,sha256=Wi9ToB_30DdQBbB2qbPlppAyKuD8AhqOpzWIM5BIB1Y,35318
-brainpy/_src/dnn/normalization.py,sha256=g3JN8i-nt9NcSrcdLeJhM9EBpyjDlbhumlyj_p3qBfA,25687
-brainpy/_src/dnn/nvar.py,sha256=5x3nQtI_WtsrgasSgXesHsT0-3oW51u7uk6h-7mzV-w,6732
-brainpy/_src/dnn/pooling.py,sha256=iF6SaiTor5EMyTavSRX4QqSba_o0-VdRk-Y24yLmEZ8,34199
-brainpy/_src/dnn/reservoir.py,sha256=XvvksZG2ue2VfVKSSlT7daWcuxsZ78-CtyMg1ue5aHA,8875
-brainpy/_src/dnn/rnncells.py,sha256=jDuSIOvLhGztwkvE2LG31f4WgB5FaFjE7DDtFHxMd0Y,27026
+brainpy/_src/dnn/activations.py,sha256=BT22Uhp4iIHtW_Grv2X7VYTAhA3zek1-QuEX4OLME8Q,32584
+brainpy/_src/dnn/base.py,sha256=Vy2yRnhfwR9VH5AWqsAfHXZGnKGVBk4B41e0fGNiuX0,220
+brainpy/_src/dnn/conv.py,sha256=ai-c5TC7wDOmckwtcWVqt8iwIxBjH-G2zXE0yjllsAg,29858
+brainpy/_src/dnn/dropout.py,sha256=JJQsYiVeU-tp94bJJ-tblUkdr6__rsGsBj3iHOob-p0,1413
+brainpy/_src/dnn/function.py,sha256=ftKmhPQRx-8nXEZYpDrYF50M-sTd6wepXKiOMgYiQQo,1819
+brainpy/_src/dnn/interoperation_flax.py,sha256=bKLpqccIokYm_FXRwWHReuMwQghzqw51fmnCGqEBXmU,4115
+brainpy/_src/dnn/linear.py,sha256=tv6IKe3enKgm5119vuxvKO7HtK4-AOtr3d4WymTr_70,36039
+brainpy/_src/dnn/normalization.py,sha256=kZsPaAdq7kBs9yYB_el_-7Cxgnlse_bSA3q7AwFrKi0,25703
+brainpy/_src/dnn/nvar.py,sha256=kUSsPsXnI_-YysBZWRLILsBdoMf5nDhllxiGybmlVOA,6748
+brainpy/_src/dnn/pooling.py,sha256=TZVD8C0kGIrP--lNGqc_srhUH87fCL56CxqlM7OYqe0,34217
+brainpy/_src/dnn/reservoir.py,sha256=lAb-zbjDlieIWbB1klZWKcFQ1tcJY6w52nCbgyGh60Y,8891
+brainpy/_src/dnn/rnncells.py,sha256=scuEfwxAzQUebXH3QlQLJe1b6ry28ziM_MfZkx_etws,27042
 brainpy/_src/dyn/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-brainpy/_src/dyn/_docs.py,sha256=LwrXyrccdd2S0SWdaQ4Ou4Fb0Z4qkzWJDITfjHtz-lk,1382
-brainpy/_src/dyn/base.py,sha256=W187iqJhxuOk2tkEaM8WkNe5y55p3YsgKVXVVQPptho,4882
-brainpy/_src/dyn/projections.py,sha256=ucOH7j9OxRBdtLaJ4q1DH2GdMRgEdubilLB8hEffm00,5485
-brainpy/_src/dyn/channels/Ca.py,sha256=DMKpsH3cY2C4gfFV_TAq9EP2rW28-CnWdu4JLP9bIg8,40139
-brainpy/_src/dyn/channels/IH.py,sha256=XZGkK64wIhyPQLFsVlVadJ7aNEq7C83Ykyq7bqHjljE,9190
-brainpy/_src/dyn/channels/K.py,sha256=1HI-Fb3H4kQmEccknvTVw8CWNyez3Z174bKFJfd0Zqw,35645
-brainpy/_src/dyn/channels/KCa.py,sha256=2kNF5cUHQBJNj-wEISxe0_Bisqvv4BWNAh_Dt_Id-M4,4491
-brainpy/_src/dyn/channels/Na.py,sha256=doSFKYMaXKiUFtz0vierNF2UT3FbJOwqP8cZNTxQajU,11936
-brainpy/_src/dyn/channels/__init__.py,sha256=PDPYmDDQCato2ZvBOXj0ueUSgkiW13hm4GNdQJn0kOI,423
-brainpy/_src/dyn/channels/base.py,sha256=ISv1CWBJyJaZajNdn0aIfDxCFicIYtxW423EhD1oR_I,4039
-brainpy/_src/dyn/channels/leaky.py,sha256=3r92utKkj67P6pNIgWQ5Uo-06mx0If-F3TwFLbF8HX4,2102
+brainpy/_src/dyn/_docs.py,sha256=PjXaa1wwtmhCa8KiSu5M1FXZqW8oZxhk45PAUdIqORc,1612
+brainpy/_src/dyn/base.py,sha256=129A1n5Lk1xD_fyMI6ThuCxg3PXglvws7-Tk7ZCvZM8,396
+brainpy/_src/dyn/utils.py,sha256=jD2l9fUuudwUazOmm7mWZABnHai6Z_ovn6y9uKRcaiM,416
+brainpy/_src/dyn/channels/__init__.py,sha256=3_9xC31sRPxhvUir_nGWU_Y4RWSh1Sv7htq2kDbnV3I,212
+brainpy/_src/dyn/channels/base.py,sha256=2wcPcN9o2JC4wobOMzi9QCSFQCbp5fklcgQNIRpVyU0,774
+brainpy/_src/dyn/channels/calcium.py,sha256=OQkNP0A-yrS-ECBav7_rS4fXrFse7Gef_FK5eAmWzHA,27906
+brainpy/_src/dyn/channels/hyperpolarization_activated.py,sha256=POJO72k2cC0FZ28N8UhOASGHqp-WkqFBYBd6cVY6rqg,9192
+brainpy/_src/dyn/channels/leaky.py,sha256=vOl_ZhlSZX1UsYHFpVHlBobv6J3jjKgjWjTZk8kf88U,1502
+brainpy/_src/dyn/channels/potassium.py,sha256=861vwB-F-44AF1qpkHGZ0ITHfK8Ai-7id8BD05IqTHA,70273
+brainpy/_src/dyn/channels/potassium_calcium.py,sha256=Nj9fOnx7iXWoiU0YrHvZXO54pAzv3DYo-6rgmmVdmBw,4561
+brainpy/_src/dyn/channels/potassium_calcium_compatible.py,sha256=Z-4hfqIC2IZn3toQes3_VDhItRKCrWkNQ39T0kxc8qE,4653
+brainpy/_src/dyn/channels/potassium_compatible.py,sha256=7i5tk_EwAKaos6-abhr7fCOYl3CvM7PG48feGl1SKUc,36885
+brainpy/_src/dyn/channels/sodium.py,sha256=-t6kgL7qbKikvXLPu8HYuRob-oTC1n7lZsZR4ypd730,11637
+brainpy/_src/dyn/channels/sodium_compatible.py,sha256=uFNIJSb4Z0L_l156Xm2cL5L31nQAPrb-IYQlmwZqz_A,12054
+brainpy/_src/dyn/ions/__init__.py,sha256=VF7b0p1gFxGnTLxUb7ac733Pm0yXFEnRPhDmcGLadzQ,91
+brainpy/_src/dyn/ions/base.py,sha256=jHY3ZPCJR6NzKx1skuB-bNSnKAcWsGpJsUkQJpTXa_8,6867
+brainpy/_src/dyn/ions/calcium.py,sha256=rjEMBIsq-AVLcv-bgXg_r6BaNpbPhmm5RHI6YYMUOik,11715
+brainpy/_src/dyn/ions/potassium.py,sha256=LasBAi5sRHr29DFa_B92vLVTjLvnGsHsLYACj6lQFDY,1469
+brainpy/_src/dyn/ions/sodium.py,sha256=dd_5tl9Ra1FICq-Ihh3bAJGFZODoh7_6Llknbq8n_CI,1487
 brainpy/_src/dyn/neurons/__init__.py,sha256=PFZ1iNJNJVQT7yuYmJaVEzZ5hOYw8VB2sRPiee1Un8Y,21
-brainpy/_src/dyn/neurons/hh.py,sha256=CWGSpq1-rbnw8VVZKJsd348vh0ubCm2U_ZR_BDISrHc,44733
-brainpy/_src/dyn/neurons/input.py,sha256=2hNeVLNfqfb9FZ6lhXatyVX3Xg2WbyIEEBlvQWaKvP8,5860
-brainpy/_src/dyn/neurons/lif.py,sha256=DPDBJJDYH9JXJNmBaxM3rOfrYIa6NyixDkQ1TJYy4Fo,84993
+brainpy/_src/dyn/neurons/base.py,sha256=1dLdJVD8TGzXshwqqoHv7gDSZ9ohonExSQ6CbXh31VA,1422
+brainpy/_src/dyn/neurons/hh.py,sha256=vfXFTuZuSkvpKPncJJ-OYw8HeY56p_4zn-sNH44cldg,49806
+brainpy/_src/dyn/neurons/lif.py,sha256=UaYkxfEALeN40qdVcqqGp1TK05Yb0OsyYpX220Mru0w,87391
 brainpy/_src/dyn/others/__init__.py,sha256=A2MAv0P_hAjdLhI1sMCq3f261BPtquh4CvF55SC-Vig,28
-brainpy/_src/dyn/others/common.py,sha256=BAuUt6ZsQGp_wHYaUtdw-ujnEyjYAkNKYu9gwFObZeM,4141
-brainpy/_src/dyn/synapses/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-brainpy/_src/dyn/synapses/dynamics.py,sha256=R0P5a6ul1kyQWkKkAlV3Bjql530RrkZQUyUZUm7Oa64,28351
-brainpy/_src/dyn/synapses/outputs.py,sha256=tFx0AM4qL1G5_n4XTgqXorGwkEODory1SzvuV7FAbhU,3147
+brainpy/_src/dyn/others/common.py,sha256=sfkFYulYLZL_n44l-bV7Y4DxJHeOQJibdeNEOLCyT5E,4131
+brainpy/_src/dyn/others/input.py,sha256=ucgpKcgvMo1frXbBi5-RsaoRvpmFAlCZ4NJFKkqO0bk,6748
+brainpy/_src/dyn/others/noise.py,sha256=UXTIdj1iufKIimoaZq3d98E964_qRsKF_BsAool-Gh8,2195
+brainpy/_src/dyn/outs/__init__.py,sha256=6IrMQQ-z_jCdAiMs5JYTx1c-chtbDBmGTAsuz9CB4yQ,43
+brainpy/_src/dyn/outs/base.py,sha256=GvWugusPsiuxAInMWHhYF5eORnKMEgBm7E4iamDGP08,755
+brainpy/_src/dyn/outs/outputs.py,sha256=-syYlur7Ar-o5dyw6awGY4g21KpZKul7VLzbFbz2yUY,3185
+brainpy/_src/dyn/projections/__init__.py,sha256=LqtSFR6N36EAmtZY_0whlsiYMlzj1Fm7MfJC-LGBlYc,65
+brainpy/_src/dyn/projections/aligns.py,sha256=5gu_07Gvwn3G_itcumlP0H0UDEdjsxo2g89qS2pj9Kc,29373
+brainpy/_src/dyn/projections/conn.py,sha256=yd00-I7cWPNSk0l-asu3XIrIwmmIeaOsoC_dsRgq6QQ,3766
+brainpy/_src/dyn/projections/others.py,sha256=eYfuruzTSlHrYLJ04WBreKbIXh1FhmRpLEPgMyyVRpY,2694
+brainpy/_src/dyn/rates/__init__.py,sha256=biF71W-hlm9xRYQpbOWuA3jpDZ3WWGXoaOG7emElYSc,52
+brainpy/_src/dyn/rates/populations.py,sha256=_uJ6AUJC2tjNv5VqkOdygsO4qPXUw3igLN-kcUV4mjk,41198
+brainpy/_src/dyn/synapses/__init__.py,sha256=YRbfuvuInfxrc1PsxBK0PKdwOP64mZJshLf9oEQXdm0,58
+brainpy/_src/dyn/synapses/abstract_models.py,sha256=ueAT7FovJ-yuck9OkfXMgGJKVzr8RDZXvY3bBllHkXY,21874
+brainpy/_src/dyn/synapses/bio_models.py,sha256=8rvyrW329LajJMfca4ELwsfFX7y_4sqigEQYAyaIL0Q,11826
+brainpy/_src/dyn/synapses/delay_couplings.py,sha256=jXCl7g6OSVqQ8opyEF9RwhlX5n_Y7Vtxin6IHcFgwEM,10834
+brainpy/_src/dynold/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+brainpy/_src/dynold/experimental/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+brainpy/_src/dynold/experimental/abstract_synapses.py,sha256=NVi449DDGFzxd9C6kMlP5AYR3xqegsXC2yTd9foaI8c,13565
+brainpy/_src/dynold/experimental/base.py,sha256=V4wLZqaqoCIjy0gy3EbnDGMs7z4sXgu-QjElXO_MQs8,4641
+brainpy/_src/dynold/experimental/others.py,sha256=jV33eY1QXPksvBUE0Vit8_TZEqUA-2IylyltSttF1xE,2549
+brainpy/_src/dynold/experimental/syn_outs.py,sha256=bwVzxBTGAibyNCung6hS3nzvEiZGlK_b-dP9Sw5MlMk,2656
+brainpy/_src/dynold/experimental/syn_plasticity.py,sha256=eC7zkZ9E7F-gw9qa58dafGFWXnzJssZ7kfg1FJLm2tY,4730
+brainpy/_src/dynold/neurons/__init__.py,sha256=TzFnHE1aRIT2-lpBaydBFW7E1PwghKs1fkQ_1-HhFuU,121
+brainpy/_src/dynold/neurons/biological_models.py,sha256=2_p-nZvP4FEzHiaqUXBRslKYAYBS3Df_3Uzq6S1-BoU,37030
+brainpy/_src/dynold/neurons/fractional_models.py,sha256=TToY8O8E7T4jyKOJ02YgRp31QpYd9k9bDNmPlcEH5Io,13087
+brainpy/_src/dynold/neurons/reduced_models.py,sha256=XjlWtSP_9rjCI8J_iHkFjpnOk4tWSLiZsRzSffcb3xk,60694
+brainpy/_src/dynold/synapses/__init__.py,sha256=HsKIyxAih6cZGSOPnzEhPROYKUv3AIHJV1cvLbVtqIQ,162
+brainpy/_src/dynold/synapses/abstract_models.py,sha256=mxcZZPzYwFlGy2XDHn-QK1grpjq7mRg7dgHDUBU8KME,28740
+brainpy/_src/dynold/synapses/base.py,sha256=hnD90yZB_J9VaIYLDDX8jzQ1qUcVwL_bQa1zSlKwkog,10491
+brainpy/_src/dynold/synapses/biological_models.py,sha256=A7GxLov1aqh55_VovkKVVFySNSsa7eFGDreu3dw_1eo,13419
+brainpy/_src/dynold/synapses/compat.py,sha256=aapUG29COxBDoDIIV87ngVNFMe7WJtK3oxFGcNr-eOo,7569
+brainpy/_src/dynold/synapses/gap_junction.py,sha256=bp1nrNLO1xlOhq20TQ6M18MymoXbvK8XdmAmdQ8s8Ws,2054
+brainpy/_src/dynold/synapses/learning_rules.py,sha256=kFrVsI15-eat_LueyZxIggtiExgNq08Lfzis8hZv-VA,8569
+brainpy/_src/dynold/synouts/__init__.py,sha256=0Z2hJcdyS_8FsPlHxP0oGhLj_tCS7FF5xt-rImFiIew,73
+brainpy/_src/dynold/synouts/conductances.py,sha256=L7Vc9rbeFrUp-KnYfdB6hEV8PtY_G7ILq7Oq1bdzdTo,2620
+brainpy/_src/dynold/synouts/ions.py,sha256=AJhWTeHSwy2G484PRqvUjW-tpMOvD4ia11zXYr-ZDCo,3268
+brainpy/_src/dynold/synplast/__init__.py,sha256=2So17UaLExSNKtLczYtfCo53b3agOcLjz2UCCPtB0S4,62
+brainpy/_src/dynold/synplast/short_term_plasticity.py,sha256=Tf3TlefXWkk3KEoy2lyY2YLsBxmilwpVCkttpazu_D4,5589
 brainpy/_src/encoding/__init__.py,sha256=Y4USQ3gijv8QEyE8Kcnq7hIsY3xQiS2lr3I85OCakiE,114
 brainpy/_src/encoding/base.py,sha256=2-q2O9HyMG-f1Y-t-BImU-u-1x_ySKsD50BCqrzzcDE,364
 brainpy/_src/encoding/stateful_encoding.py,sha256=lOu0_fEwyLID5NASynahFbWOrfKjswyw-N-IUt0-cBU,4769
 brainpy/_src/encoding/stateless_encoding.py,sha256=ZJWrKFGXP9TLA69efojTsmRtYaEUXsLP5D49KXHvRsg,2156
 brainpy/_src/initialize/__init__.py,sha256=BZnTMuwfTddVqmvLbwjczRYLPC_CN8yJZtu__wnZmZA,154
 brainpy/_src/initialize/base.py,sha256=xseZjikf7TWq6BL5C_WybpGHm1G9yQW-MCCvlfG0AQs,614
 brainpy/_src/initialize/decay_inits.py,sha256=NDpSLBHM5IsPQ9rCPQqQX72cJtgaUdpU52OJ16gFo7s,13428
@@ -124,15 +169,15 @@
 brainpy/_src/integrators/fde/base.py,sha256=YJLBNm7uIEA2GKf2vvLkT0z9uUYulMoISKdfOlXV0uw,2771
 brainpy/_src/integrators/fde/generic.py,sha256=zFfKmvOIyS8_CnWDKceRYK-FsHhfO_Jf-jGb2Nph6wY,2706
 brainpy/_src/integrators/ode/__init__.py,sha256=aX9ioVcUyyzTe55DJxjZMx54znbGjKpvt7y1KNJQRko,220
 brainpy/_src/integrators/ode/adaptive_rk.py,sha256=MHGcrraQG43A886TZcFD05eGFlTe6JKxURyrFmr1_5A,17892
 brainpy/_src/integrators/ode/base.py,sha256=YraM5qO9m5hsWvzmFbQ5p0pWpIaC2KsfZ4ddBD_8DZk,4845
 brainpy/_src/integrators/ode/common.py,sha256=shreGp53Qg8i602ZDQa0qurMWpAbOP7djSLsc1UZBqw,1493
 brainpy/_src/integrators/ode/explicit_rk.py,sha256=zNBYecP9JydLWxip49J-SQpP98sYHOi8OQzZVVEDsJk,25974
-brainpy/_src/integrators/ode/exponential.py,sha256=GXC4oNsIJQxammb4UP3VSAW6HSQpEZekzZfw_ljg-tk,13756
+brainpy/_src/integrators/ode/exponential.py,sha256=fsBOU4alDzf9Glz9gJyAKcJfK7OYMbOTmQ4Hn-4FW88,13760
 brainpy/_src/integrators/ode/generic.py,sha256=WnSD2LFE-Yj7nWOCN0DoY7ySG47Xc2fzR-8aMSuoVxw,4139
 brainpy/_src/integrators/pde/__init__.py,sha256=iwhKnzeBJLKxpRVjvzwiRE63_zNpIBfaKLITauVph-0,24
 brainpy/_src/integrators/pde/base.py,sha256=8lHCODxLITFBRTs1vHRNc2QpoS8VefHH3_izeyeltI4,98
 brainpy/_src/integrators/sde/__init__.py,sha256=s76hX5fgaP-zZXSf_Q03052NYzU_xkC9AUVgIreZXQA,184
 brainpy/_src/integrators/sde/base.py,sha256=eFIiRCj3ictHhE7fBZWxUN3cIQIgcHJldPH8Rgl5l-s,3321
 brainpy/_src/integrators/sde/generic.py,sha256=LyBzeLCgrDLBnbiXtIAd6y_S6rVcjdgTUspXKUNdC5Q,3877
 brainpy/_src/integrators/sde/normal.py,sha256=8JweJYKdjtQetySpjqKVyljhnCPj5S_fPQ0DbGYd-dA,24171
@@ -142,52 +187,52 @@
 brainpy/_src/losses/base.py,sha256=s-9GiRz5PEZFhM1VQ9IbOR8sR1qpiidEYKsdmdxbb80,445
 brainpy/_src/losses/comparison.py,sha256=A2bdIEDKtRLTFn2G3F0CrJ_h6TtF17b5Mb6e_4UHGaE,40982
 brainpy/_src/losses/regularization.py,sha256=PR6eHqO-dXszHk9Qj9JWWPtRf9bN76pbj03Eo0xktro,2368
 brainpy/_src/losses/utils.py,sha256=36MfIfCZV2-zD19CPANwbwfl0hak7rpHl2bz_n3qUG8,792
 brainpy/_src/math/__init__.py,sha256=O9Qw0ErmR0TYIhI4XTMeOwmtUD5R8e2TRmr-rrkRSWE,1488
 brainpy/_src/math/_utils.py,sha256=uCb4a4MqgWuWb2Q90XQm1fOd2Gpy_zZr1_M4Cp73Bp0,1792
 brainpy/_src/math/activations.py,sha256=pXldmiFbEqcxqTRpVaDFw0JbXmJmeba3YlspVGcAX20,18933
-brainpy/_src/math/compat_numpy.py,sha256=Iju_t16qGFigmvuCVU5azq8rx0_1o86Pd6S0h3ZDdkI,29884
-brainpy/_src/math/compat_pytorch.py,sha256=dGE5YU6UapyxwJFSehGACH7SF4R9oUZGuJKUUg3nL2U,6488
+brainpy/_src/math/compat_numpy.py,sha256=2zB0Kdkf_rm8K1smsuvN1nSnYDPjqOdArfGOHXGk51A,29885
+brainpy/_src/math/compat_pytorch.py,sha256=qXFORhRRv94wf8ZRZbQwhlAOtrERJV0yJ2VGgppWEPo,6487
 brainpy/_src/math/compat_tensorflow.py,sha256=86I2H11zpRoyCrVSkNLpATnshdx9juVhg0VXBrqInpE,17887
 brainpy/_src/math/datatypes.py,sha256=AgZZmDKVEqTpVTM85i6HPT9GUf_YPs_ymi4p7tuR_S4,911
-brainpy/_src/math/delayvars.py,sha256=wdMdEZcLVA4Okcnr2rpY1d8Q9LeaVAthBxJb-3vchpU,15861
+brainpy/_src/math/delayvars.py,sha256=JU25y859ONKaJwE4vVxZxAKw1C8VZLjzrUR1TxO0kVc,16139
 brainpy/_src/math/environment.py,sha256=4ChAaPrKAFeJ67bV5rGVtOdrivxkEAGu_fe7eOUkqgI,16825
 brainpy/_src/math/fft.py,sha256=NO5RqLkdor_8JXXB5cK7yQFEAVgoatk7q6T7KCvLp8k,1498
 brainpy/_src/math/index_tricks.py,sha256=tNZ63fVlO9M4hwRbztHiWwwdZ6V3dVchKoaPYC5Wb6U,8866
 brainpy/_src/math/interoperability.py,sha256=n1bWyu-QGfg9t2q6mMVitimbG-LU3TPogbigLWcdQGc,2523
 brainpy/_src/math/linalg.py,sha256=Q9Be3i9CNE8W8bK-4hKDAsz--O2zgw-U_xF0R6rGrg0,1792
 brainpy/_src/math/modes.py,sha256=KHKtSDgbzis6ebIcl_HGnjC2y_y19U5ofaWreTk5nsM,2319
-brainpy/_src/math/ndarray.py,sha256=VeHHjZWAT6ZH9STmurKMP1RwUNKWHxYgf3pfg0YNjLA,47277
+brainpy/_src/math/ndarray.py,sha256=lV2SHQoXfr0HDBiNkKUa-J36BlgiAsclHLnP8NN50cI,47487
 brainpy/_src/math/others.py,sha256=QoseJ2WRoOcVw0iCQP0OfZmEj8XK_JfOLy_7Vdz8bH4,2255
 brainpy/_src/math/pre_syn_post.py,sha256=ZuJSaDwq1yZqzsYCe86xs76SKnGbcMdOwhMcmL3wh1A,15894
 brainpy/_src/math/random.py,sha256=RHzY5wtIvRfd6_MKuzxgcMVraLhSBkdH0gbwq9NJYWw,78579
 brainpy/_src/math/remove_vmap.py,sha256=qqSlj6NcKp-7iXpQc5Pp-NTEnad6cLdwl_y-JZ5aZ_U,2088
-brainpy/_src/math/sharding.py,sha256=wTn7K_BALSn_PlZy0wL_Q5xr1LXDhSyZaXqgWs7qzac,3578
+brainpy/_src/math/sharding.py,sha256=jqCYpmM0ZEkBLtQc_2ClM8U30SU729eYB5WrkD0B2PI,3878
 brainpy/_src/math/event/__init__.py,sha256=YhAfSQkrc_0oUXQuLbhIDI2UQ43Bl_cliT_rps545MM,61
 brainpy/_src/math/event/_csr_matvec.py,sha256=dxkjj1xMjerZIMqifDSeFy55bMSAQ4kcp4BaiyNyqEk,17787
 brainpy/_src/math/event/_info_collection.py,sha256=O1kVj-CVZbXV9QTdrLd3BNFN3PnRW5wcoRqNxdNzkTw,5062
 brainpy/_src/math/jitconn/__init__.py,sha256=qFqNeEACHCxuaidBSePVvuOWUE8A0K6LRlb0XjZo7VY,53
-brainpy/_src/math/jitconn/_event_matvec.py,sha256=C9z0ZeOKoQ8030Q4NgQKdsF-s7iLzFCtUMWlynCJc_U,26368
+brainpy/_src/math/jitconn/_event_matvec.py,sha256=z6uZbu4z0mKM0t6u9vQDDyhoiish0Oa-z_KIb0juiug,25027
 brainpy/_src/math/jitconn/_matvec.py,sha256=M4uz7uXPNiT92I0xZHA9PnGC98P1dWdeRyQh_S8I5RU,28793
 brainpy/_src/math/object_transform/__init__.py,sha256=iV3d2uYiGe0r2aVIxqmvIJwrTcqpC0pXysISGyr23yo,914
 brainpy/_src/math/object_transform/_tools.py,sha256=So_GDh8Rc0hjGjY6XBQkw4wzDgBIkTgMXO-AMSFO-4E,3615
 brainpy/_src/math/object_transform/_utils.py,sha256=VMIQvLnpFkWoNdsDVigSnpgceNoDqcEEk_MMzkOwe3M,758
-brainpy/_src/math/object_transform/autograd.py,sha256=AL-e-xA-LyFs_ec7MZJD8URiaQNTKrDdNrOAXiaRwqM,40562
-brainpy/_src/math/object_transform/base.py,sha256=9ixSzP1YTqaQlLf0dMAMdlSaWpSvUS5cq_LRehABxHQ,22773
+brainpy/_src/math/object_transform/autograd.py,sha256=OlI2WCqm4jfBODdlUTDxpPSab11aupLBKirF2_1N1-o,40727
+brainpy/_src/math/object_transform/base.py,sha256=F434HoWpn82jLraT0hHnl_q84Y3S79uJwLgjTTN3J4E,22767
 brainpy/_src/math/object_transform/collectors.py,sha256=Zl4bm-naLRzh7HbMyA9sjZxR3u6VHFqRFvWmdDcGEC0,5931
-brainpy/_src/math/object_transform/controls.py,sha256=9EXRRQZ9g6gZuS2FJBufYY90RQvacbAPsX4IhXNiO_o,32211
+brainpy/_src/math/object_transform/controls.py,sha256=q8rbUjaRX0F8p8l8b2A1bODhZDdtR27REYnvf3FAjt4,32660
 brainpy/_src/math/object_transform/function.py,sha256=LqUH4_8LvbppNrYsNP1dzLXL-Tc9U3uNXrFzPXpIjhQ,2999
-brainpy/_src/math/object_transform/jit.py,sha256=-HTPesOK2yO-hK9HFW-X-ybegMk-ZyGmad3VZ4j6T9Q,16336
-brainpy/_src/math/object_transform/naming.py,sha256=hA9pSorEw5EHYGRPZdEH51qA7v1A6ZPySwFG6YWLyb4,1637
+brainpy/_src/math/object_transform/jit.py,sha256=XSQNipDcGaeVfSsRX4LTzpiY_21GlmLJJudQU6tz9AA,15803
+brainpy/_src/math/object_transform/naming.py,sha256=Lb1mduDRSIu5-a1jm1xNk4Heg1FsUqHhS6l8e3xig2k,1642
 brainpy/_src/math/object_transform/parallels.py,sha256=t84hKHGPB0l4ddV79tvaJvIi42Bex0_qaCu21CuSCFo,17774
 brainpy/_src/math/object_transform/variables.py,sha256=q4F43xMXuMWIgqSao83QznzPePcO1Fv-SYSPtuYHRBM,14573
 brainpy/_src/math/op_registers/__init__.py,sha256=asxTRD0C_nTVXs65_cn60KxMUnb6gUWZ2ATlRQZ4jRk,204
 brainpy/_src/math/op_registers/utils.py,sha256=ijkEdSi214KVyNmV2uqYMOeENNfCTHTUbkujRe0jong,1051
-brainpy/_src/math/op_registers/numba_approach/__init__.py,sha256=EjD9sDhkG0Lt9rxag274Cy4B3ZjepjfmyQ3uHWIim9s,7129
+brainpy/_src/math/op_registers/numba_approach/__init__.py,sha256=1XA4d4diUbjBKrhRiDv4agGlagdw4bSx5Nu2CUzB0hQ,7105
 brainpy/_src/math/op_registers/numba_approach/cpu_translation.py,sha256=rwNWiUZVq8zBkg2YjzYD6mw0oPnK6e4fQ9JGL-61qpU,5129
 brainpy/_src/math/sparse/__init__.py,sha256=9IQnulrX6Gj9qgC3sC_KpbGZ5DuJhu2ErKXBVAXIupo,142
 brainpy/_src/math/sparse/_bsr_mm.py,sha256=GElnwanLgw-okA6MzZ1kNoiHAW4TwJ7mOXLVByFjojI,14576
 brainpy/_src/math/sparse/_bsr_mv.py,sha256=M78yp1cDzBSvXX4VsJYukd-HizhQOSvxSn8rL-ZUCYI,8134
 brainpy/_src/math/sparse/_coo_mv.py,sha256=AKbaubeBe3AAPaw15tyYx1L4hFl6-t6oLR4qLLtT6RM,7127
 brainpy/_src/math/sparse/_csr_mv.py,sha256=UsJ01nwfyRGVhLtHw0iQTykiq-hch1414NkbWhwiGfk,16697
 brainpy/_src/math/sparse/_jax_prim.py,sha256=Tmy0fV8AapBIOtIqDirjz1Zjs1Kpy6gP9S2RZ12waK4,4439
@@ -195,84 +240,68 @@
 brainpy/_src/math/surrogate/__init__.py,sha256=9qMFRG17XcBmn6hS1JRlRCHyQOS8kB5pv5VIdSTe_hU,99
 brainpy/_src/math/surrogate/_compt.py,sha256=4PuAhWPW_fGLvEixK6gNQh0ahr_hAIupGMixv6k6ujU,7216
 brainpy/_src/math/surrogate/_one_input.py,sha256=Yig2uVAx_z24jqjxtYeZpOBn0vn-71UgbgotsUsK6I8,42623
 brainpy/_src/math/surrogate/_two_inputs.py,sha256=lWyvtFUYk3FYq53zKGdW0gIYK88u26dWPUOJFz1tG_Q,1492
 brainpy/_src/math/surrogate/_utils.py,sha256=CVhDyaqzK6ddXQNyz0YYaS-T1b8U4KOMXqveles0X68,3665
 brainpy/_src/math/surrogate/base.py,sha256=cyAd-G3Fge59G-iHCUHZH9AU22PLoJDbqeXkrTVDQhY,243
 brainpy/_src/measure/__init__.py,sha256=eOAgeUH97IPxsvg1_q8SjY-ZdD8nui3h2uiGS_F-8E0,287
-brainpy/_src/measure/correlation.py,sha256=oOwNSQO6ChaaTuzLd0erD8tU6g_DjdaexknDl2O29jw,10102
+brainpy/_src/measure/correlation.py,sha256=fHRJMZpHVBztYt8C4FVrDFymiJxsCvhnypI_zu4KgYc,9953
 brainpy/_src/measure/firings.py,sha256=ki5MhHL0vVeMYi9Yrms3BWOlZzZ9cL-NA3nvED5YB-E,1770
 brainpy/_src/measure/lfp.py,sha256=gYsAqzHcP4V9K43-d4EioFrI39iWcebKSyiv9ONx5ZQ,3896
-brainpy/_src/neurons/__init__.py,sha256=xlEq2G7WQWqMnTz7zLmRbCUPAXDl5xELJGpvEscFMQk,177
-brainpy/_src/neurons/biological_models.py,sha256=cg9cclDGIyaj6wfAJhZn8ApGx3MAXKf004MF3n_IZ0k,48772
-brainpy/_src/neurons/compat.py,sha256=YTPVRatOoivZ7C6YL4FAUf83WflrGYSX0GNyxrCzioA,595
-brainpy/_src/neurons/fractional_models.py,sha256=h88lQn1cA59HV_FIcDTyLDnwphuX_dHFBbmcEL28OEI,13124
-brainpy/_src/neurons/input_groups.py,sha256=XjMAaSEdIRATYo1Ccrd8rKpnG9iyo0m_6WyGwwCnLWc,5477
-brainpy/_src/neurons/noise_groups.py,sha256=pixoBdjbrpyRLlkiloxB0adWBU8Y3V8ebTUk826js-w,2301
-brainpy/_src/neurons/reduced_models.py,sha256=KdG5smdfafrPfG4bhHWMOsEb0xg-iLNH5u5Sn9EZ3Jw,92062
 brainpy/_src/optimizers/__init__.py,sha256=ToxZhvkgFhL9St3jQESUSY57RsPQc_5rYFDjWSzWTsY,75
 brainpy/_src/optimizers/optimizer.py,sha256=_47vxjzn34oSa603oz8jeR_ops-l7gVbNPZUhakH6X0,41331
 brainpy/_src/optimizers/scheduler.py,sha256=e_W27P4M-ZPDEW1930d3C1BK2N4szpdjL4w4N2NjTBY,13240
-brainpy/_src/rates/__init__.py,sha256=biF71W-hlm9xRYQpbOWuA3jpDZ3WWGXoaOG7emElYSc,52
-brainpy/_src/rates/populations.py,sha256=9Lj6d8Lz9oFOGKolEu8P6iMSeDASF7SyUw3XaJ-JEeA,41216
 brainpy/_src/running/__init__.py,sha256=4EORFDW-JNEpvzsyNuBsj9Sa5Ie61TeUAY4JiRM57DY,525
 brainpy/_src/running/constants.py,sha256=ekB6jpM51xZnWCscVM1hLPw86WQtoQYnBBHpRs6SII8,269
-brainpy/_src/running/jax_multiprocessing.py,sha256=rmd_voTmREb4etkOODkPAdNcfkxKOk3-YsIHrT7V1Ao,4913
+brainpy/_src/running/jax_multiprocessing.py,sha256=KSHfkF3aWWnqWieGpgc6r3Vr48rcu3Rh8Ovh285Iu8Q,4982
 brainpy/_src/running/native_multiprocessing.py,sha256=uJgl6qX1TTGOwT-OEiGuGRuRqjM3xSpu81AxJ1Qj8Ec,2976
 brainpy/_src/running/pathos_multiprocessing.py,sha256=sSnqtbzszxIHBamt5tcUTjK2yiYiO_3GoCT1QsXLdiM,6995
-brainpy/_src/running/runner.py,sha256=mIQNxKLXgDhmJgXSe964KK4X82YWBdIPMBljocfk0LA,9779
-brainpy/_src/synapses/__init__.py,sha256=vnhuo80o4KyGjetcCrEb8nVbOhe-MXnO5wOHUT9VYI8,223
-brainpy/_src/synapses/abstract_models.py,sha256=mjIHPUqF4ALwBNsKrvk2AoQfxmXXWWAEVCAq67Dj0IY,35980
-brainpy/_src/synapses/biological_models.py,sha256=HWvU_6IE2mef-0G1XF512MesDq5zDFX73OPZmvJyCfQ,21845
-brainpy/_src/synapses/compat.py,sha256=XLesmxZTtAdA26g7xgrNHMJEsb9XsfXZNvlD-HOiAXg,10246
-brainpy/_src/synapses/delay_couplings.py,sha256=SCpXVQ0QNC_HF5YacN4iJF1RCxx5ee8vaJxQQGxs9XM,11133
-brainpy/_src/synapses/gap_junction.py,sha256=57pUw6xEYfgpzWEWsbU0Cyl2C8lX2IDhYOrPwkE_zrA,2023
-brainpy/_src/synapses/learning_rules.py,sha256=jecO7rWpXLbVuYX_x20Pj6aW7MViV30GQZvwEaVVCCU,9212
-brainpy/_src/synapses_v2/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-brainpy/_src/synapses_v2/abstract_synapses.py,sha256=k4mTi2Wj3yxNGieTdfH2u62F7pexMKgSsn1pnhvF1ak,13557
-brainpy/_src/synapses_v2/base.py,sha256=zMd6CT-JEbL0uSMjCRbBQRG7M6TLYbH4hXJcrIsHcmE,4649
-brainpy/_src/synapses_v2/others.py,sha256=4560N_0GET9g3Hxe5zOVlc9ssyRBm-K_qDc0beyBlwI,2553
-brainpy/_src/synapses_v2/syn_outs.py,sha256=oAyhBD12RXthnaCOXeTy7i9nEuYygIiVfi_ozyRXh0o,2648
-brainpy/_src/synapses_v2/syn_plasticity.py,sha256=TfvsoTeEHQhk7Mn1OZAqFclKicThaAwARW6DCgxd8Ks,4722
-brainpy/_src/synouts/__init__.py,sha256=0Z2hJcdyS_8FsPlHxP0oGhLj_tCS7FF5xt-rImFiIew,73
-brainpy/_src/synouts/conductances.py,sha256=-C5BQx6zuAnKIQEvRenfnL3_Gq1Eouygk5254i5LLf0,2634
-brainpy/_src/synouts/ions.py,sha256=3hNPl7hYq_rdLV4fUis6vzFfTeMwxORgex7oO5PBMyA,3279
-brainpy/_src/synplast/__init__.py,sha256=2So17UaLExSNKtLczYtfCo53b3agOcLjz2UCCPtB0S4,62
-brainpy/_src/synplast/long_term_plasticity.py,sha256=iwhKnzeBJLKxpRVjvzwiRE63_zNpIBfaKLITauVph-0,24
-brainpy/_src/synplast/short_term_plasticity.py,sha256=f8jEVTYsXkuLEiJaaoxdMx4TH_-uwouLS5bcQ38q2Qc,5168
-brainpy/_src/testing/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-brainpy/_src/testing/base.py,sha256=FswvOzYoom9RYaL34v3YRynynAkUTnwJ_ACo_HBXI2A,422
+brainpy/_src/running/runner.py,sha256=oZ5t1zYWAMyX8LnfnnRlK-pQNfccv0KeOA51_es2kt0,9674
 brainpy/_src/tools/__init__.py,sha256=GnNGRLjUzYTbXcyMz_Y2VGybU-Gk-JpFOrbWQNGPOw4,182
 brainpy/_src/tools/codes.py,sha256=hT4Zvb3nO8Osx7GRSl_Pyfmi7eRFSvRXhewkXYHXAXk,7448
-brainpy/_src/tools/dicts.py,sha256=ctYh-jTVfsL2PSZ6Bg2g5pdj_etzHw5fq1hPLnT-7XQ,5882
+brainpy/_src/tools/dicts.py,sha256=azH7jMZKhRwHOFla6Qd_4WjDIf4PZSEkfqwvB3J1fMQ,4565
 brainpy/_src/tools/install.py,sha256=n6UbYADGwjkz4J1Y4ZCzrX66JQ2IYLu_Z3S2ceTHS2M,1015
 brainpy/_src/tools/math_util.py,sha256=afNqi1UZQmP9KXmSkaOLlwVDNDLxvWMY3dgYdUKn7w8,223
 brainpy/_src/tools/others.py,sha256=nQr93gHerHu_0DuVQ15RuPsrGxO5uP-uCx3GrQudbzE,4425
 brainpy/_src/tools/package.py,sha256=xYqde4JYnIYokzLRjiw0CFWhxg-CdFz1bYMK3UT-RKA,1407
-brainpy/_src/train/__init__.py,sha256=IOuQfImvK1BfNx4pbGq-lyeNCCqTxbgHJEppVsQXQMU,562
+brainpy/_src/train/__init__.py,sha256=7R_gAGS8HH7DrkDzZgot0q_WYdUVTXq8SibFhWOOVAo,614
 brainpy/_src/train/_utils.py,sha256=OY_jjyPdga4m0OU1ZKYPQRo0_7NOcw_fU3TndkW7EbI,1970
-brainpy/_src/train/back_propagation.py,sha256=ggkfkBtjPLMlZOIuypsc8tRHlpBvywA_KfnYgX0o64Q,24067
-brainpy/_src/train/base.py,sha256=uP-mKZqQQEciJ9Rfc5-QeabHmxubYH_Vw73P3d3fgtg,2974
-brainpy/_src/train/offline.py,sha256=ne9VTdP4X1EG2Q72R_OLdC2k73rgg8O6OApd-CYFmDg,10034
-brainpy/_src/train/online.py,sha256=-nkKGDH4OPf6HzJXslvv0sDde7VIGl6gkZkvjWGFQ1s,10356
+brainpy/_src/train/back_propagation.py,sha256=DppTe3s1Yvywf-u2Ym44bYU4cAu0D4X5btXSyIfWK50,22357
+brainpy/_src/train/base.py,sha256=vEsCo8pUmf4EnysBCf3VKamcPraVmHS8sls_OfsxSsk,2843
+brainpy/_src/train/offline.py,sha256=BobB1FEalz36GH3DEZy9p2ccyBtCsJI809sWtsu1V1w,9346
+brainpy/_src/train/online.py,sha256=Plkiy5WSzYAohV-kR5JS5DfccPGS7BTV84nX3OqbLsE,9705
 brainpy/_src/visualization/__init__.py,sha256=9jcWciqRU4d1BH4bCjuUc7u9qx_E1gyAFJFx-yDxK20,77
 brainpy/_src/visualization/base.py,sha256=A8mBH_IYD5XJYgLI8DthEjTTx-Cl6ARvNsW5w5sYohw,3544
 brainpy/_src/visualization/figures.py,sha256=_Pq10YOBDzwdYwqG2qN5B4wTW6-z_f_ti_7cxUP_BMU,841
 brainpy/_src/visualization/plots.py,sha256=FwbsaeCWijBYYylvqu0HmwOcSgX9R_7ZM22HjSbaOoM,14603
 brainpy/_src/visualization/styles.py,sha256=MkrX9e8c3bsowczpbnlVq_jhX4BjajtV2lRcvQ7mQkw,1026
 brainpy/algorithms/__init__.py,sha256=NwSgRAiSJcAHBdeuqVG98t6S6phGd6gl5cLCUSKZQPY,90
 brainpy/algorithms/offline.py,sha256=lCKCjtLlXt2CiIwMeaoowq7nAOhKHHoBg79GNoKp1Do,17344
 brainpy/algorithms/online.py,sha256=P-r3xm9xef5jnx8mw_kCz4hZ3T-rexXPMM1pHZ-ryaA,6284
 brainpy/algorithms/utils.py,sha256=-CZfyZ_7mupjU6vjRNQIwOpc4bJjsQ62xDBAhz1Dz-o,2656
-brainpy/dyn/__init__.py,sha256=3CBoNpEuBLzok0d08jEAUCsuWAR62QhpyhFsVgE4hew,121
-brainpy/dyn/channels.py,sha256=kOSx2RsHuSzDwYyDfZHolv9Og_np8ZdbLFSxrmSNId8,820
-brainpy/dyn/neurons.py,sha256=OAienl0f38_0KpfRzLa5pLPenBeXNIiJz03CQV-DfIY,737
-brainpy/dyn/others.py,sha256=ODny_ynBCK2-GWK_tSyNHWmBlBVy4ZJaRxPZBNERXGU,69
-brainpy/dyn/projections.py,sha256=qLtFL1p2JAPlJk3bIppK5qtjYp-sl3M7uGNRquArQGI,81
-brainpy/dyn/synapses.py,sha256=8_9wmEsORQ3x842Roem_0GahUu56fhHZqv63u6Bt-yM,271
+brainpy/dnn/__init__.py,sha256=A4DPvxfp4XuTcsN6GCDci45Z7l07Ae_TBpRLNjK7djY,199
+brainpy/dnn/activations.py,sha256=2vNYOjxDYAORxpYd-db4JQv7LWHMuHFkZLiQYYzenX4,471
+brainpy/dnn/conv.py,sha256=uiaC1A1OO794z249pE4MZQRtifBC4qGlQbY0-9tFjYM,273
+brainpy/dnn/interoperation.py,sha256=INMU7sKlEw-2mPyPXZ2rwrKiGBvY7M5pEFb18t0S-Dc,91
+brainpy/dnn/linear.py,sha256=7d8r5ygF6G1j2BHvPJ03d5IOWcihBMWB76gmTU6egsI,523
+brainpy/dnn/normalization.py,sha256=W4LWdcSnydViOq06Vhmdry6IaLNf3KX5z6rapgD5Iv0,313
+brainpy/dnn/others.py,sha256=RqBi-QWTJwUcYOzD59AH6ywYTfDGK3afyjhOBnY7iP4,202
+brainpy/dnn/pooling.py,sha256=KoM6iRtUZ_-qBvZHwqORu1jTGtovrvDNQzkE6-SON8o,519
+brainpy/dnn/recurrent.py,sha256=xGpWNH69IvywhT9c7GahwYXqwyxfhw8RVJJJjHSg30k,344
+brainpy/dyn/__init__.py,sha256=Brg6tiTr0q19AWy1HEt3_0W0QIR4DdPTWWPHdyqhsBc,231
+brainpy/dyn/base.py,sha256=jRDwICLzrGJIXYBh1d296C13lyvEo-X8lNrnUciDKJ0,72
+brainpy/dyn/channels.py,sha256=euIawZa4EO1loObS-zJul_Rn96E27FI3AwyIT5LsbA4,1519
+brainpy/dyn/compat.py,sha256=gdf00MHdKMPl2BtZchTpN1TxY4cc3aszLHDzAEnfgZA,92
+brainpy/dyn/ions.py,sha256=a_qMaHluFdb-6zdhat5GvKySYjoNGb2xoIC_APaxjFQ,566
+brainpy/dyn/neurons.py,sha256=xSRk4Y3WQhktaDWxiNnHEN3QRFEUvW7AqQp3hlnB3Tg,584
+brainpy/dyn/others.py,sha256=uZM9PasJnPemokw0u3_xYeNA0070c3E0VOJ8Dn-CMa4,341
+brainpy/dyn/outs.py,sha256=RCe9tI1w-yshyM3bM50rAfHo-4vvqrRoGjOgzOADROA,126
+brainpy/dyn/projections.py,sha256=7wqM1_JEkavwIKtzBebe7ytuRLhjyRyybAHYvKvkcHE,342
+brainpy/dyn/rates.py,sha256=pZPNbR-gnyPZsD7mAH1zhsRxtC7iCg8-B3hbrcnedOs,138
+brainpy/dyn/synapses.py,sha256=gb3fl8clWvw893xTAXTTSruLUX2dZ33krrZkrrxvArI,310
 brainpy/integrators/__init__.py,sha256=Cvh3lH346n1pgnra6SkLc92uBnFp-y7QXVew5TgL5lE,128
 brainpy/integrators/fde.py,sha256=l7AexMW_0mS8D6Euovn04Vs5aCFddB6QnT8QwwurTvQ,576
 brainpy/integrators/ode.py,sha256=NI80C2t_v8Fq6rTMBxtqW78Rxm5eiuR-Dho_p0wYFpI,1061
 brainpy/integrators/sde.py,sha256=6kYyp0FDyDWvVwTeqdWPZk6RK_ktXeku5c5SqaILv9M,679
 brainpy/math/__init__.py,sha256=6w3Ruoe82py-NVRsIJMjer_EQwYiyoCbfxXZSmBK-3w,4768
 brainpy/math/activations.py,sha256=T85gbICpXPrLsGzY7OJY-6Bblve7Pm-Eg_we4rxOkbg,780
 brainpy/math/compat_numpy.py,sha256=F830tznz3RM758Ay3ytCAWIB4JD2I7acBjiPNhqPE80,8074
@@ -293,16 +322,12 @@
 brainpy/math/op_register.py,sha256=-CTPZo8Sv_CzfHxHBnJm233535IEbLfLEDo_pzbs3J0,127
 brainpy/math/others.py,sha256=Q5WaFgg2u8e3iuhXSaQq__OFvca-ZOy-BeLrwHasgfI,257
 brainpy/math/pre_syn_post.py,sha256=0-vfY89KIadAF5XZyN5dznjaUzPpl8XX6myFICxipGc,331
 brainpy/math/random.py,sha256=DevtUfAjobWASIsOgEq1JUBtwHino5r6okFIH_rjtM0,1770
 brainpy/math/sharding.py,sha256=AOdBWKmfUMLZyisUZkINW6XXK4pefBqyOUHy66E_ltk,213
 brainpy/math/sparse.py,sha256=DGmy2wFfrCu3tgGsDxJFLlR-iAJTPM3_hI4Ei1JP-Tg,164
 brainpy/math/surrogate.py,sha256=bX-XigJ3R4Ctv3yhcErJqUt70mtDEGRBDU_xJQsm3WA,1249
-brainpy/synapses/__init__.py,sha256=OsHo0CQjonQzN2C2w1ls1PyXZiksaA22u6iyljV947I,73
-brainpy/synapses/dynamics.py,sha256=ADPR4uUwlgqsh6NWbmtNdh1b6cHt06LS4ibektdF4HA,596
-brainpy/synapses/synouts.py,sha256=2DFTM0LFTfX7NVslwiwd8ffP4---8fVmKQzpaYh51WM,172
-brainpy/synapses/synplast.py,sha256=98xl_N491M6BXkPISLojPnLzi-VhoVazeNBl9YVu8lo,113
-brainpy-2.4.2.dist-info/LICENSE,sha256=awdTB1OZyMqrS8QDj8fyUDxz40eWjrqByUqj7MApebM,35100
-brainpy-2.4.2.dist-info/METADATA,sha256=ybFz7mzupd7OyEeUNIiwejEbiCF2agrzuVNaeZ88AWo,4131
-brainpy-2.4.2.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-brainpy-2.4.2.dist-info/top_level.txt,sha256=5oi55xrJaqccrIi7VVShKhvGx0MjnDl0efebGKr5Omc,8
-brainpy-2.4.2.dist-info/RECORD,,
+brainpy-2.4.3.dist-info/LICENSE,sha256=awdTB1OZyMqrS8QDj8fyUDxz40eWjrqByUqj7MApebM,35100
+brainpy-2.4.3.dist-info/METADATA,sha256=7ssjxVDMlCH49wXR_K0fZBZ64FcNdpglIsdS00Ignho,4369
+brainpy-2.4.3.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+brainpy-2.4.3.dist-info/top_level.txt,sha256=5oi55xrJaqccrIi7VVShKhvGx0MjnDl0efebGKr5Omc,8
+brainpy-2.4.3.dist-info/RECORD,,
```

